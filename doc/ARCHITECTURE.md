# Stage 1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

## ğŸ—ï¸ è¨­è¨ˆæ€æƒ³

### æ ¸å¿ƒã‚³ãƒ³ã‚»ãƒ—ãƒˆ

Stage 1ã¯**è‡ªå·±æ•™å¸«ã‚ã‚Šãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å†æ§‹ç¯‰**ã¨ã„ã†é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¡ç”¨ã—ã€é‡‘èæ™‚ç³»åˆ—ã®æœ¬è³ªçš„ãªå¤šã‚¹ã‚±ãƒ¼ãƒ«æ§‹é€ ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

**ã‚­ãƒ¼ã‚¢ã‚¤ãƒ‡ã‚¢**:
1. **ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒ«ã®é‡‘èå¿œç”¨**: NLPã®BERTã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é‡‘èæ™‚ç³»åˆ—ã«é©ç”¨
2. **ã‚¯ãƒ­ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ä¸€è²«æ€§**: ç•°ãªã‚‹æ™‚é–“è¶³é–“ã®å†…åœ¨çš„é–¢ä¿‚æ€§ã‚’å¼·åˆ¶å­¦ç¿’
3. **è¡¨ç¾å­¦ç¿’**: Stage 2ã®RLå–å¼•ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®é«˜å“è³ªç‰¹å¾´é‡ã‚’äº‹å‰å­¦ç¿’

### è¨­è¨ˆåŸç†

#### 1. å¤šã‚¹ã‚±ãƒ¼ãƒ«åŒæœŸåŸç†
```
M1 (1åˆ†)  : |----|----|----|----|  è©³ç´°ãƒã‚¤ã‚º + çŸ­æœŸãƒ‘ã‚¿ãƒ¼ãƒ³
M5 (5åˆ†)  : |---------|---------|  ä¸­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ + ãƒ‘ã‚¿ãƒ¼ãƒ³  
H1 (1æ™‚é–“): |------------------|  é•·æœŸæ§‹é€  + ãƒ¬ã‚¸ãƒ¼ãƒ 
```

**å³ç«¯æ•´åˆ—**: å…¨ã¦ã®ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãŒåŒã˜çµ‚äº†æ™‚åˆ»ã§æƒã†
â†’ å®Ÿéš›ã®å–å¼•ç’°å¢ƒã§ã®æ„æ€æ±ºå®šæ™‚ç‚¹ã‚’æ¨¡æ“¬

#### 2. ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ã®è¨­è¨ˆ
```python
# é€£ç¶šãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¹ã‚­ãƒ³ã‚°ï¼ˆ5-60 M1ãƒãƒ¼ï¼‰
Input:  [O, H, L, C, O, H, L, C, [MASK], [MASK], [MASK], O, H, L, C]
Target: [O, H, L, C, O, H, L, C,    O,      H,      L,    C, O, H, L, C]
```

**TFé–“åŒæœŸ**: åŒã˜ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼æœŸé–“ãŒå…¨TFã§ãƒã‚¹ã‚¯ã•ã‚Œã‚‹
â†’ ãƒ¢ãƒ‡ãƒ«ãŒTFé–“é–¢ä¿‚æ€§ã‚’å­¦ç¿’ã›ã–ã‚‹ã‚’å¾—ãªã„

#### 3. æå¤±é–¢æ•°ã®éšå±¤è¨­è¨ˆ
```
L_total = 0.6Ã—L_recon + 0.2Ã—L_stft + 0.15Ã—L_cross + 0.05Ã—L_amp_phase
          â†‘å†æ§‹ç¯‰     â†‘å‘¨æ³¢æ•°åŸŸ   â†‘æ•´åˆæ€§    â†‘é«˜æ¬¡çµ±è¨ˆ
```

å„æå¤±ãŒç•°ãªã‚‹å´é¢ã‚’ã‚«ãƒãƒ¼:
- **Huber**: åŸºæœ¬çš„ãªä¾¡æ ¼å†æ§‹ç¯‰
- **STFT**: ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ æ§‹é€ ï¼ˆå‘¨æœŸæ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
- **Cross**: TFé–“ç‰©ç†çš„æ•´åˆæ€§
- **Amp-Phase**: é«˜æ¬¡çµ±è¨ˆç‰¹æ€§

## ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­è¨ˆ

### éšå±¤æ§‹é€ ã®è«–ç†

```
stage1/
â”œâ”€â”€ configs/           # ğŸ“‹ è¨­å®šç®¡ç†
â”‚   â””â”€â”€ base.yaml      #   ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»å®Ÿé¨“è¨­å®š
â”œâ”€â”€ scripts/           # ğŸš€ å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ  
â”‚   â”œâ”€â”€ train_stage1.py    #   è¨“ç·´ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â””â”€â”€ evaluate_stage1.py #   è©•ä¾¡ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
â”œâ”€â”€ src/               # ğŸ§  ã‚³ã‚¢å®Ÿè£…
â”‚   â”œâ”€â”€ data_loader.py     #   é«˜ãƒ¬ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿çµ±åˆ
â”‚   â”œâ”€â”€ window_sampler.py  #   å¤šTFåŒæœŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
â”‚   â”œâ”€â”€ feature_engineering.py #   OHLCâ†’ç‰¹å¾´é‡å¤‰æ›
â”‚   â”œâ”€â”€ masking.py         #   ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
â”‚   â”œâ”€â”€ normalization.py   #   TFåˆ¥æ­£è¦åŒ–
â”‚   â”œâ”€â”€ model.py           #   ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆå®šç¾©
â”‚   â””â”€â”€ losses.py          #   æå¤±é–¢æ•°å®Ÿè£…
â”œâ”€â”€ docs/              # ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
â”œâ”€â”€ checkpoints/       # ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜
â””â”€â”€ logs/              # ğŸ“Š TensorBoard ãƒ­ã‚°
```

### ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è²¬å‹™åˆ†é›¢

#### ãƒ‡ãƒ¼ã‚¿å±¤ (Data Layer)
- **window_sampler.py**: ç‰©ç†çš„ãƒ‡ãƒ¼ã‚¿åˆ‡ã‚Šå‡ºã—
- **feature_engineering.py**: ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜å¤‰æ›
- **normalization.py**: çµ±è¨ˆçš„æ­£è¦åŒ–
- **masking.py**: å­¦ç¿’æˆ¦ç•¥å®Ÿè£…

#### ãƒ¢ãƒ‡ãƒ«å±¤ (Model Layer)  
- **model.py**: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å®šç¾©ï¼ˆCNN+Transformerï¼‰
- **losses.py**: æœ€é©åŒ–ç›®çš„é–¢æ•°

#### åˆ¶å¾¡å±¤ (Control Layer)
- **train_stage1.py**: è¨“ç·´ãƒ«ãƒ¼ãƒ—ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†
- **evaluate_stage1.py**: æ€§èƒ½è©•ä¾¡ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹

## ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¨­è¨ˆ

### ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```mermaid
graph TD
    A[Stage 0 Parquet Files] --> B[MultiTFWindowSampler]
    B --> C[FeatureEngineer]
    C --> D[TFNormalizer]
    D --> E[MaskingStrategy]
    E --> F[DataLoader Batch]
    
    F --> G[TF-Specific Stems]
    G --> H[Shared Encoder]
    H --> I[Bottleneck]
    I --> J[TF-Specific Decoders]
    
    J --> K[Stage1CombinedLoss]
    K --> L[AdamW + OneCycleLR]
    L --> M[Checkpoints]
```

### é‡è¦ãªè¨­è¨ˆæ±ºå®š

#### 1. å³ç«¯æ•´åˆ— (Right-Edge Alignment)
```python
# å®Ÿå–å¼•ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
window_end = current_timestamp
window_start = current_timestamp - timedelta(hours=3.3)

# å…¨TFãŒåŒã˜çµ‚äº†æ™‚ç‚¹
m1_data = df_m1.loc[window_start:window_end]
m5_data = df_m5.loc[window_start:window_end]
# ... ä»–ã®TFã‚‚åŒæ§˜
```

**ç†ç”±**: å®Ÿéš›ã®å–å¼•ã§ã¯ã€Œç¾åœ¨æ™‚ç‚¹ã€ã§ã®æƒ…å ±ã®ã¿åˆ©ç”¨å¯èƒ½

#### 2. ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãªã—è¨­è¨ˆ
```python
# âŒ æ‚ªã„ä¾‹: ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
if len(tf_data) < seq_len:
    tf_data = pad_with_zeros(tf_data, seq_len)

# âœ… è‰¯ã„ä¾‹: å¯å¤‰é•· + ãƒã‚¹ã‚¯
valid_mask = torch.any(features != 0, dim=-1)
loss = loss * valid_mask  # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã®ã¿æå¤±è¨ˆç®—
```

**ç†ç”±**: äººå·¥çš„ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¯ãƒ¢ãƒ‡ãƒ«ã‚’æ··ä¹±ã•ã›ã‚‹

#### 3. çµ±è¨ˆåˆ†é›¢åŸå‰‡
```python
# å„TFã”ã¨ã«ç‹¬ç«‹ãªæ­£è¦åŒ–çµ±è¨ˆ
stats = {
    'm1': {'mean': [...], 'std': [...]},
    'm5': {'mean': [...], 'std': [...]},
    # ...
}
```

**ç†ç”±**: TFé–“ã®åˆ†å¸ƒç‰¹æ€§ã®é•ã„ã‚’ä¿æŒ

## ğŸ§  ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ·±æ˜ã‚Š

### éšå±¤è¨­è¨ˆã®æ„å›³

```
Input [6TF Ã— 200seq Ã— 6feat] â†’ 36 parallel streams

Level 1: TF-Specific Processing
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DepthwiseConv1D + PointwiseConv â”‚ Ã— 6
â”‚ (Domain-specific patterns)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
Level 2: Cross-Scale Integration  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Head Self-Attention      â”‚
â”‚ + Cross-TF Attention (every 2) â”‚
â”‚ (Scale relationships)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
Level 3: Global Compression
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Strided Conv (4x compression)   â”‚
â”‚ (Global context extraction)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
Level 4: TF-Specific Reconstruction
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transposed Conv + Linear Head   â”‚ Ã— 6
â”‚ (TF-adapted reconstruction)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è¨­è¨ˆã®å·¥å¤«

#### 1. Depth-wise Separable Convolution
```python
# å¾“æ¥ã®é‡ã„Conv1D (channels Ã— kernel Ã— features)
conv_standard = nn.Conv1d(6, 128, kernel_size=3)  # 6Ã—3Ã—128 = 2304 params

# è»½é‡ãªDepth-wise + Point-wise
conv_depthwise = nn.Conv1d(6, 6, kernel_size=3, groups=6)    # 6Ã—3 = 18 params  
conv_pointwise = nn.Conv1d(6, 128, kernel_size=1)           # 6Ã—128 = 768 params
# Total: 786 params (66% reduction)
```

**åˆ©ç‚¹**: è¨ˆç®—åŠ¹ç‡ + TFå›ºæœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’

#### 2. ã‚¯ãƒ­ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«æ³¨æ„ã®æ®µéšçš„é©ç”¨
```python
# å…¨å±¤ã§ã¯ãªã2å±¤ã”ã¨
if (layer_idx + 1) % self.cross_attn_every == 0:
    tf_features = cross_attention(tf_features)
```

**ç†ç”±**: è¨ˆç®—ã‚³ã‚¹ãƒˆ vs è¡¨ç¾åŠ›ã®ãƒãƒ©ãƒ³ã‚¹

#### 3. Bottleneckè¨­è¨ˆ
```python
# seq_len=200 â†’ latent_len=50 (4x compression)
bottleneck = nn.Conv1d(d_model, d_model, kernel_size=4, stride=4)
```

**Stage 2ã¸ã®æº–å‚™**: åœ§ç¸®ã•ã‚ŒãŸè¡¨ç¾ã¯RLçŠ¶æ…‹ç‰¹å¾´é‡ã¨ã—ã¦åˆ©ç”¨

## ğŸ¯ æå¤±é–¢æ•°ã®è¨­è¨ˆå“²å­¦

### å¤šç›®çš„æœ€é©åŒ–æˆ¦ç•¥

**æ€§èƒ½æ”¹å–„å®Ÿç¸¾**: æ®µéšçš„æœ€é©åŒ–ã«ã‚ˆã‚Š**ç·åˆ5-7å€é«˜é€ŸåŒ–**ã‚’é”æˆ

```
Stage 1 æ€§èƒ½æ”¹å–„å±¥æ­´:
1. ãƒ™ã‚¯ãƒˆãƒ«åŒ–: O(NÂ²) â†’ O(N log N) (22å€é«˜é€ŸåŒ–)
   - 486ç§’ â†’ 22ç§’ (ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ¢ç´¢)
   
2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜
   - 22ç§’ â†’ æ•°ç™¾ms (40-100å€é«˜é€ŸåŒ–)
   
3. I/Oãƒ»å­¦ç¿’æœ€é©åŒ–: DataLoader + Tensor Core
   - 2 it/s â†’ 7-9 it/s (5-7å€é«˜é€ŸåŒ–)
   - 36h/epoch â†’ 5-7h/epoch

ç·åˆåŠ¹æœ: å®Ÿç”¨çš„ãªå­¦ç¿’æ™‚é–“ã‚’å®Ÿç¾
```

#### 1. Huberæå¤± (åŸºæœ¬å†æ§‹ç¯‰)
```python
# L1ã¨L2ã®æŠ˜è¡· â†’ å¤–ã‚Œå€¤ã«é ‘å¥
Î´ = 1.0  # é‡‘èãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸé–¾å€¤
loss = F.huber_loss(pred, target, delta=Î´)
```

#### 2. STFTæå¤± (å‘¨æ³¢æ•°åŸŸç‰¹æ€§)
```python
# è¤‡æ•°è§£åƒåº¦ã§å‘¨æ³¢æ•°ç‰¹æ€§ä¿æŒ
scales = [256, 512, 1024]  # çŸ­æœŸãƒ»ä¸­æœŸãƒ»é•·æœŸå‘¨æœŸ
for scale in scales:
    stft_pred = torch.stft(pred, n_fft=scale)
    stft_target = torch.stft(target, n_fft=scale)
    loss += spectral_loss(stft_pred, stft_target)
```

#### 3. ã‚¯ãƒ­ã‚¹æ•´åˆæ€§æå¤± (ç‰©ç†åˆ¶ç´„)
```python
# M1é›†ç´„ vs ç›´æ¥TFäºˆæ¸¬ã®ä¸€è‡´å¼·åˆ¶
m1_aggregated = aggregate_m1_to_tf(m1_pred, interval=5)  # M1â†’M5
m5_direct = pred[:, 1]  # ç›´æ¥M5äºˆæ¸¬
loss = F.mse_loss(m5_direct, m1_aggregated)
```

## ğŸ”¬ å®Ÿé¨“ãƒ»è©•ä¾¡è¨­è¨ˆ

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹éšå±¤

#### ãƒ¬ãƒ™ãƒ«1: åŸºæœ¬å†æ§‹ç¯‰å“è³ª
- **MSE/MAE**: æ•°å€¤ç²¾åº¦
- **Correlation**: æ–¹å‘æ€§ä¸€è‡´

#### ãƒ¬ãƒ™ãƒ«2: ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹ç•°æ€§  
- **OHLCè«–ç†åˆ¶ç´„**: Highâ‰¥max(Open,Close), Lowâ‰¤min(Open,Close)
- **æ•´åˆæ€§æ¯”ç‡**: TFé–“ç‰©ç†çš„ä¸€è²«æ€§

#### ãƒ¬ãƒ™ãƒ«3: é«˜æ¬¡çµ±è¨ˆç‰¹æ€§
- **ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ å·®åˆ†**: å‘¨æ³¢æ•°åŸŸã§ã®é¡ä¼¼æ€§
- **æŒ¯å¹…ãƒ»ä½ç›¸ç›¸é–¢**: æ™‚ç³»åˆ—ã®æ·±å±¤æ§‹é€ 

### æ®µéšçš„æ¤œè¨¼æˆ¦ç•¥

```python
# Phase 1: ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¥å…¨æ€§
python3 tests/test_stage1_data.py

# Phase 2: å°è¦æ¨¡å®Ÿé¨“ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ç¢ºèªï¼‰
train_stage1.py --config configs/debug.yaml --epochs 5

# Phase 3: æœ¬æ ¼è¨“ç·´
train_stage1.py --config configs/base.yaml --epochs 40

# Phase 4: åŒ…æ‹¬çš„è©•ä¾¡
evaluate_stage1.py --ckpt best_model.ckpt
```

## ğŸš€ Stage 2ã¸ã®æ©‹æ¸¡ã—

### è¡¨ç¾å­¦ç¿’ã®ç¶™æ‰¿

```python
# Stage 1ã§å­¦ç¿’æ¸ˆã¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
stage1_encoder = load_checkpoint('stage1_best.ckpt').encoder

# Stage 2 RL ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§åˆæœŸåŒ–
rl_agent.policy_network.encoder.load_state_dict(stage1_encoder.state_dict())

# Bottleneckè¡¨ç¾ã‚’RLçŠ¶æ…‹ã¨ã—ã¦åˆ©ç”¨
state_features = stage1_encoder(multi_tf_obs)  # [batch, n_tf, latent_len, d_model]
action = rl_agent.policy(state_features.mean(dim=(1,2)))  # Global pooling
```

### ç¶™æ‰¿ã•ã‚Œã‚‹èƒ½åŠ›

1. **ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«èªè­˜**: ç•°ãªã‚‹æ™‚é–“è¶³ã®åŒæ™‚ç†è§£
2. **æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³**: OHLCå¤‰å‹•ã®æœ¬è³ªçš„æ§‹é€ 
3. **ãƒã‚¤ã‚ºé ‘å¥æ€§**: ãƒã‚¹ã‚­ãƒ³ã‚°è¨“ç·´ã«ã‚ˆã‚‹é ‘å¥ãªè¡¨ç¾
4. **ã‚¯ãƒ­ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«æ•´åˆæ€§**: ç‰©ç†çš„åˆ¶ç´„ã®å†…åœ¨åŒ–

## ğŸ’¡ è¨­è¨ˆã®é©æ–°æ€§

### å¾“æ¥æ‰‹æ³•ã¨ã®å·®åˆ¥åŒ–

#### å¾“æ¥: å˜ä¸€TF + æ‰‹ä½œã‚Šç‰¹å¾´é‡
```python
# å…¸å‹çš„ãªå¾“æ¥æ‰‹æ³•
features = [
    calculate_sma(prices, 20),
    calculate_rsi(prices, 14),
    calculate_bollinger_bands(prices, 20)
]
model = LSTM(features)
```

#### Stage 1: ãƒãƒ«ãƒTFè‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’
```python
# é©æ–°çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
multi_tf_data = sample_aligned_timeframes(6_tfs)
masked_data = apply_continuous_masking(multi_tf_data)
representations = model.encode(masked_data)
reconstructed = model.decode(representations)
loss = multi_component_loss(reconstructed, targets)
```

### å­¦è¡“çš„è²¢çŒ®

1. **é‡‘èæ™‚ç³»åˆ—ã¸ã®è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’é©ç”¨**
2. **ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«åŒæœŸãƒã‚¹ã‚­ãƒ³ã‚°æ‰‹æ³•**
3. **ã‚¯ãƒ­ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ç‰©ç†åˆ¶ç´„ã®æå¤±é–¢æ•°åŒ–**
4. **å³ç«¯æ•´åˆ—ã«ã‚ˆã‚‹å®Ÿç”¨æ€§æ‹…ä¿**

ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€Stage 1ã¯å˜ãªã‚‹å‰å‡¦ç†ã§ã¯ãªãã€é‡‘èæ™‚ç³»åˆ—ç†è§£ã®æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’æç¤ºã—ã¦ã„ã¾ã™ã€‚