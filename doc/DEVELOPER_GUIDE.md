# Stage 1 é–‹ç™ºè€…ã‚¬ã‚¤ãƒ‰

## ğŸ› ï¸ é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### å¿…è¦ãªä¾å­˜é–¢ä¿‚

```bash
# åŸºæœ¬çš„ãªæ©Ÿæ¢°å­¦ç¿’ã‚¹ã‚¿ãƒƒã‚¯
pip install torch>=2.0.0
pip install pytorch-lightning>=2.0.0
pip install pandas>=2.0.0
pip install numpy>=1.24.0
pip install pyarrow>=12.0.0
pip install scipy>=1.10.0

# è¿½åŠ ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
pip install tensorboard
pip install matplotlib
pip install seaborn
pip install tqdm
pip install omegaconf
pip install torchmetrics
pip install psutil
```

### é–‹ç™ºãƒ„ãƒ¼ãƒ«

```bash
# ã‚³ãƒ¼ãƒ‰å“è³ª
pip install black isort flake8
pip install pytest pytest-cov

# å‹ãƒã‚§ãƒƒã‚¯  
pip install mypy
```

## ğŸ”§ ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ç†è§£

### ä¸»è¦ã‚¯ãƒ©ã‚¹é–¢ä¿‚å›³

```
Stage1Dataset
â”œâ”€â”€ MultiTFWindowSampler    # ãƒ‡ãƒ¼ã‚¿åˆ‡ã‚Šå‡ºã—
â”œâ”€â”€ FeatureEngineer         # ç‰¹å¾´é‡å¤‰æ›
â”œâ”€â”€ TFNormalizer           # æ­£è¦åŒ–
â””â”€â”€ MaskingStrategy        # ãƒã‚¹ã‚­ãƒ³ã‚°

Stage1Model
â”œâ”€â”€ TFSpecificStem Ã— 6     # TFå›ºæœ‰å‡¦ç†
â”œâ”€â”€ SharedEncoder          # å…±æœ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
â”œâ”€â”€ Bottleneck            # åœ§ç¸®
â””â”€â”€ TFDecoder Ã— 6         # TFåˆ¥å†æ§‹ç¯‰

Stage1CombinedLoss
â”œâ”€â”€ HuberLoss             # åŸºæœ¬å†æ§‹ç¯‰
â”œâ”€â”€ STFTLoss              # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ 
â”œâ”€â”€ CrossTFConsistencyLoss # æ•´åˆæ€§
â””â”€â”€ AmplitudePhaseCorrelationLoss # é«˜æ¬¡çµ±è¨ˆ
```

### é‡è¦ãªè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³

#### 1. è¨­å®šé§†å‹•é–‹ç™º
```python
# å…¨ã¦ã®è¨­å®šã¯YAMLã§ç®¡ç†
config = yaml.safe_load(open('configs/base.yaml'))

# å®Ÿè¡Œæ™‚è¨­å®šå¤‰æ›´
config['training']['batch_size'] = 32
config['model']['encoder']['n_layers'] = 12
```

#### 2. PyTorch Lightningæ¡ç”¨ç†ç”±
```python
# å®šå‹çš„ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’æŠ½è±¡åŒ–
class Stage1LightningModule(pl.LightningModule):
    def training_step(self, batch, batch_idx):
        # æå¤±è¨ˆç®—ã®ã¿ã«é›†ä¸­
        return self.criterion(self.model(batch['features']), batch['targets'])
    
    def configure_optimizers(self):
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼è¨­å®šã‚’åˆ†é›¢
        return self._create_optimizer_and_scheduler()
```

#### 3. ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆ
```python
# å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ç‹¬ç«‹ã—ã¦ãƒ†ã‚¹ãƒˆå¯èƒ½
def test_window_sampler():
    sampler = MultiTFWindowSampler(dummy_data, seq_len=50)
    assert len(sampler) > 0
    
def test_feature_engineer():
    engineer = FeatureEngineer(config)
    features, targets = engineer.process_window(window_data)
    assert features.shape[-1] == 6  # 6ç‰¹å¾´é‡
```

## ğŸš€ é–‹ç™ºãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

### 1. æ–°æ©Ÿèƒ½é–‹ç™ºæ‰‹é †

```bash
# 1. ãƒ–ãƒ©ãƒ³ãƒä½œæˆ
git checkout -b feature/new-masking-strategy

# 2. å®Ÿè£…
# src/ é…ä¸‹ã§ã‚³ãƒ¼ãƒ‰å¤‰æ›´

# 3. ãƒ†ã‚¹ãƒˆè¿½åŠ /æ›´æ–°
# tests/ é…ä¸‹ã§ãƒ†ã‚¹ãƒˆæ‹¡å¼µ

# 4. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆ
cd tests && python3 test_stage1_data.py

# 5. å°è¦æ¨¡å®Ÿé¨“ã§å‹•ä½œç¢ºèªï¼ˆfast_dev_runå®Œäº†æ¸ˆã¿ï¼‰
cd stage1 && python3 scripts/train_stage1.py \
  --config configs/test.yaml \
  --data_dir ../data/derived \
  --max_epochs 3 \
  --devices 1

# 6. ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯
black src/ scripts/
isort src/ scripts/
flake8 src/ scripts/

# 7. ã‚³ãƒŸãƒƒãƒˆãƒ»ãƒ—ãƒƒã‚·ãƒ¥
git add . && git commit -m "Add new masking strategy"
git push origin feature/new-masking-strategy
```

### 2. ãƒ‡ãƒãƒƒã‚°æ‰‹é †

#### ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ‡ãƒãƒƒã‚°
```python
# 1. å°ã•ãªãƒ‡ãƒ¼ã‚¿ã§ç¢ºèª
config['data']['seq_len'] = 10  # çŸ­ç¸®
dataset = Stage1Dataset('../data/derived', config, split='train')
sample = dataset[0]
print(f"Features shape: {sample['features'].shape}")

# 2. å¯è¦–åŒ–ã§ãƒ‡ãƒãƒƒã‚°
import matplotlib.pyplot as plt
m1_features = sample['features'][0]  # M1
plt.plot(m1_features[:, 3])  # closeä¾¡æ ¼
plt.title("M1 Close Prices")
plt.show()

# 3. ãƒã‚¹ã‚¯ç¢ºèª
masks = sample['masks']
print(f"Mask ratio per TF: {masks.mean(dim=1)}")
```

#### ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒãƒƒã‚°
```python
# 1. Forward passç¢ºèª
model = Stage1Model(config)
batch_size = 2
dummy_input = torch.randn(batch_size, 6, 50, 6)
outputs = model(dummy_input)
print(f"Output shape: {outputs['reconstructed'].shape}")

# 2. å‹¾é…æµã‚Œç¢ºèª
loss = torch.sum(outputs['reconstructed'])
loss.backward()
for name, param in model.named_parameters():
    if param.grad is None:
        print(f"No gradient: {name}")

# 3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ç¢ºèª
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params:,}")
```

### 3. å®Ÿé¨“ç®¡ç†

#### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†
```yaml
# configs/experiment_name.yaml
# åŸºæœ¬è¨­å®šã‚’ç¶™æ‰¿ã—ã¦éƒ¨åˆ†å¤‰æ›´
defaults:
  - base

# å®Ÿé¨“å›ºæœ‰ã®å¤‰æ›´
model:
  encoder:
    n_layers: 16  # æ·±ã„ãƒ¢ãƒ‡ãƒ«å®Ÿé¨“

training:
  batch_size: 16  # å¤§ããªãƒ¢ãƒ‡ãƒ«ç”¨
  
experiment_name: "deep_encoder_v1"
```

#### å®Ÿé¨“å®Ÿè¡Œ
```bash
# ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ†ã‚¹ãƒˆï¼ˆæ¨å¥¨ï¼šæœ€åˆã«å®Ÿè¡Œï¼‰
python3 test_components.py

# fast_dev_runï¼ˆå‹•ä½œç¢ºèªï¼‰
python3 scripts/train_stage1.py \
  --config configs/test.yaml \
  --data_dir ../data/derived \
  --fast_dev_run

# æ®µéšçš„å­¦ç¿’å®Ÿè¡Œï¼ˆæ¨å¥¨é †åºï¼‰

# 1. é–‹ç™ºç¢ºèª
python3 scripts/train_stage1.py \
  --config configs/test.yaml \
  --data_dir ../data/derived \
  --fast_dev_run

# 2. ä¸­è¦æ¨¡å­¦ç¿’ï¼ˆ15ã‚¨ãƒãƒƒã‚¯ï¼‰
python3 scripts/train_stage1.py \
  --config configs/medium.yaml \
  --data_dir ../data/derived

# 3. æœ¬ç•ªå­¦ç¿’ï¼ˆ40ã‚¨ãƒãƒƒã‚¯ï¼‰
python3 scripts/train_stage1.py \
  --config configs/production.yaml \
  --data_dir ../data/derived

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ã®å®Ÿè¡Œ
python3 scripts/train_stage1.py \
  --config configs/production.yaml \
  --data_dir ../data/derived \
  --max_epochs 20 \
  --batch_size 24

# TensorBoardç›£è¦–
tensorboard --logdir logs/

# çµæœè©•ä¾¡
python3 scripts/evaluate_stage1.py \
  --config configs/test.yaml \
  --ckpt checkpoints/stage1_best.ckpt \
  --data_dir ../data/derived
```

## ğŸ§ª ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### ãƒ†ã‚¹ãƒˆéšå±¤

```
tests/
â”œâ”€â”€ test_stage1_data.py           # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_window_sampler.py    # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å˜ä½“
â”‚   â”œâ”€â”€ test_feature_engineering.py # ç‰¹å¾´é‡è¨ˆç®—å˜ä½“  
â”‚   â”œâ”€â”€ test_masking.py           # ãƒã‚¹ã‚­ãƒ³ã‚°å˜ä½“
â”‚   â”œâ”€â”€ test_normalization.py     # æ­£è¦åŒ–å˜ä½“
â”‚   â”œâ”€â”€ test_model.py             # ãƒ¢ãƒ‡ãƒ«å˜ä½“
â”‚   â””â”€â”€ test_losses.py            # æå¤±é–¢æ•°å˜ä½“
â””â”€â”€ integration/
    â”œâ”€â”€ test_training_loop.py     # è¨“ç·´ãƒ«ãƒ¼ãƒ—çµ±åˆ
    â””â”€â”€ test_evaluation.py        # è©•ä¾¡çµ±åˆ
```

### æ–°æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆä¾‹

```python
# tests/unit/test_new_feature.py
import unittest
import torch
from stage1.src.new_feature import NewFeatureClass

class TestNewFeature(unittest.TestCase):
    
    def setUp(self):
        self.config = {...}  # ãƒ†ã‚¹ãƒˆç”¨è¨­å®š
        self.feature = NewFeatureClass(self.config)
    
    def test_basic_functionality(self):
        """åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        input_data = torch.randn(4, 6, 50, 6)
        output = self.feature(input_data)
        
        # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
        self.assertEqual(output.shape, expected_shape)
        
        # å€¤åŸŸãƒã‚§ãƒƒã‚¯
        self.assertTrue(torch.all(output >= 0))
        
        # NaN/Inf ãƒã‚§ãƒƒã‚¯
        self.assertFalse(torch.isnan(output).any())
        self.assertFalse(torch.isinf(output).any())
    
    def test_edge_cases(self):
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ"""
        # ç©ºå…¥åŠ›
        empty_input = torch.zeros(1, 6, 0, 6)
        with self.assertRaises(ValueError):
            self.feature(empty_input)
        
        # å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«
        single_input = torch.randn(1, 6, 1, 6)
        output = self.feature(single_input)
        self.assertIsNotNone(output)

if __name__ == '__main__':
    unittest.main()
```

## ğŸ“Š æ€§èƒ½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

### ãƒ™ã‚¯ãƒˆãƒ«åŒ–WindowSampleræ€§èƒ½

Stage 1ã§ã¯**O(NÂ²) â†’ O(N log N)**ã¸ã®å¤§å¹…ãªæ€§èƒ½æ”¹å–„ã‚’é”æˆã—ã¾ã—ãŸï¼š

```
å®Ÿãƒ‡ãƒ¼ã‚¿æ€§èƒ½çµæœï¼ˆ3.1Mè¡Œï¼‰:
- å¾“æ¥å®Ÿè£…: 486ç§’ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
- ãƒ™ã‚¯ãƒˆãƒ«åŒ–: 22ç§’ï¼ˆ22å€é«˜é€ŸåŒ–ï¼ï¼‰
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: ãƒãƒƒãƒå‡¦ç†ã§å®‰å®š
- å­¦ç¿’ãƒ«ãƒ¼ãƒ—: fast_dev_runå®Œå…¨æˆåŠŸ
```

**æŠ€è¡“è¦å› :**
- NumPy searchsorted ã«ã‚ˆã‚‹é«˜é€Ÿæ™‚é–“æ¤œç´¢
- ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€å‡¦ç†ï¼ˆtz-aware/tz-naiveå¯¾å¿œï¼‰
- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã«ã‚ˆã‚‹å®‰å…¨æ€§
- å‹•çš„latent_lenè¨ˆç®—ï¼ˆseq_len=50 â†’ latent_len=12ï¼‰
- ãƒ‡ãƒã‚¤ã‚¹æ•´åˆæ€§ä¿è¨¼ï¼ˆGPU/CPUçµ±åˆï¼‰

### å­¦ç¿’ãƒ«ãƒ¼ãƒ—æœ€é©åŒ–

```python
# å®Œäº†æ¸ˆã¿æœ€é©åŒ–é …ç›®
- å‹•çš„å½¢çŠ¶å‡¦ç†: compressed.size(2)ã§å®Ÿæ¸¬å€¤ä½¿ç”¨
- ãƒ‡ãƒã‚¤ã‚¹çµ±ä¸€: torch.tensor(0.0, device=pred.device)
- Tensor Coreæœ€é©åŒ–: torch.set_float32_matmul_precision('high')
- MPIå®Œå…¨å›é¿: ç’°å¢ƒå¤‰æ•°ã‚’importå‰ã«è¨­å®š
```

### I/Oãƒ»DataLoaderæœ€é©åŒ–

```yaml
# é«˜é€ŸåŒ–è¨­å®šï¼ˆconfigs/test.yamlå®Ÿè£…æ¸ˆã¿ï¼‰
dataloader:
  num_workers: 8        # I/Oä¸¦åˆ—åŒ–ï¼ˆå…ƒ: 2ï¼‰
  prefetch_factor: 4    # ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒè¿½åŠ 
  persistent_workers: true
  pin_memory: true
  
training:
  batch_size: 64        # å¤§å¹…å¢—åŠ ï¼ˆå…ƒ: 8ï¼‰
  precision: "16-mixed" # Tensor Coreæ´»ç”¨
  accumulate_grad_batches: 2  # è¦‹ã‹ã‘batch_size=128

data:
  seq_len: 48           # 4ã®å€æ•°æœ€é©åŒ–ï¼ˆå…ƒ: 50ï¼‰
  
scheduler:
  pct_start: 0.05       # çŸ­ç¸®warm-upï¼ˆå…ƒ: 0.3ï¼‰
  interval: "step"      # stepå˜ä½èª¿æ•´
```

### ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–

```python
# è‡ªå‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…æ¸ˆã¿
- ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥: MD5ã§ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã‚’è­˜åˆ¥
- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜: np.save/loadã§é«˜é€ŸåŒ–
- 20ç§’ â†’ æ•°ç™¾ms: 40-100å€ã®é«˜é€ŸåŒ–
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: ../data/derived/cache/
```

### æœŸå¾…ã•ã‚Œã‚‹ç·åˆæ€§èƒ½

```
å¾“æ¥ç‰ˆ: 2 it/s â†’ 36h/epoch
æœ€é©åŒ–ç‰ˆ: 7-9 it/s â†’ 5-7h/epoch
ç·åˆåŠ¹æœ: 5-7å€é«˜é€ŸåŒ–
```

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–

```python
import psutil
import torch

def profile_memory_usage():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª
    process = psutil.Process()
    print(f"System memory: {process.memory_info().rss / 1024**2:.1f} MB")
    
    # GPU ãƒ¡ãƒ¢ãƒª
    if torch.cuda.is_available():
        print(f"GPU memory: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
        print(f"GPU cached: {torch.cuda.memory_reserved() / 1024**2:.1f} MB")

# è¨“ç·´ä¸­ã®ç›£è¦–
def training_step_with_profiling(self, batch, batch_idx):
    if batch_idx % 100 == 0:
        profile_memory_usage()
    
    return self.criterion(self.model(batch['features']), batch['targets'])
```

### è¨ˆç®—æ™‚é–“ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```python
import time
from contextlib import contextmanager

@contextmanager
def timer(description: str):
    """è¨ˆç®—æ™‚é–“æ¸¬å®š"""
    start = time.time()
    try:
        yield
    finally:
        elapsed = time.time() - start
        print(f"{description}: {elapsed:.4f}ç§’")

# ä½¿ç”¨ä¾‹
with timer("Data loading"):
    batch = next(iter(dataloader))

with timer("Forward pass"):
    outputs = model(batch['features'])

with timer("Loss calculation"):
    losses = criterion(outputs['reconstructed'], batch['targets'])
```

## ğŸ› ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ³•

### 1. CUDA Out of Memory
```python
# å•é¡Œ: GPU ãƒ¡ãƒ¢ãƒªä¸è¶³
RuntimeError: CUDA out of memory

# è§£æ±ºç­–
# 1. ãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›
config['training']['batch_size'] = 8  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ24ã‹ã‚‰å‰Šæ¸›

# 2. å‹¾é…ç´¯ç©ä½¿ç”¨
config['training']['accumulate_grad_batches'] = 4

# 3. æ··åˆç²¾åº¦ä½¿ç”¨
config['training']['precision'] = 'bf16'

# 4. ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºå‰Šæ¸›
config['model']['encoder']['n_layers'] = 4  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ8ã‹ã‚‰å‰Šæ¸›
```

### 2. åæŸã—ãªã„
```python
# å•é¡Œ: æå¤±ãŒä¸‹ãŒã‚‰ãªã„

# è¨ºæ–­æ‰‹é †
# 1. å­¦ç¿’ç‡ç¢ºèª
config['training']['scheduler']['max_lr'] = 1e-4  # ä¸‹ã’ã‚‹

# 2. å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¢ºèª
config['training']['gradient_clip'] = 0.5  # å¼·åŒ–

# 3. ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
python3 scripts/train_stage1.py --config configs/overfit_test.yaml

# 4. ãƒ‡ãƒ¼ã‚¿ç¢ºèª
python3 tests/test_stage1_data.py
```

### 3. MPIç’°å¢ƒå•é¡Œï¼ˆWSLï¼‰
```bash
# å•é¡Œ: RuntimeError: cannot load MPI library

# è§£æ±ºç­–1: ç’°å¢ƒå¤‰æ•°è¨­å®š
export PYTORCH_LIGHTNING_DISABLE_MPI=1
export OMPI_DISABLED=1
export I_MPI_DISABLED=1

# è§£æ±ºç­–2: å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½¿ç”¨
./scripts/run_stage1.sh

# è§£æ±ºç­–3: importã‚ˆã‚Šå‰ã«è¨­å®šï¼ˆæ¨å¥¨ï¼‰
# scripts/train_stage1.py ã®å†’é ­ã§è¨­å®šæ¸ˆã¿
```

### 4. ãƒã‚¹ã‚­ãƒ³ã‚°ç‡ç•°å¸¸
```python
# å•é¡Œ: ãƒã‚¹ã‚¯ãŒåŠ¹ã„ã¦ã„ãªã„

# ãƒ‡ãƒãƒƒã‚°
sample = dataset[0]
masks = sample['masks']
print(f"Actual mask ratios: {masks.mean(dim=-1)}")
print(f"Expected: {config['masking']['mask_ratio']}")

# ãƒã‚¹ã‚¯å¯è¦–åŒ–
import matplotlib.pyplot as plt
plt.imshow(masks[0].numpy(), aspect='auto')  # TF=0ã®ãƒã‚¹ã‚¯
plt.colorbar()
plt.title("Masking Pattern")
plt.show()
```

### 5. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å•é¡Œï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°0å€‹ï¼‰
```python
# å•é¡Œ: 0å€‹ã®æœ‰åŠ¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆM1ä»¥å¤–ã®TFã§1970å¹´ãƒ‡ãƒ¼ã‚¿ï¼‰
0 æœ‰åŠ¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦

# åŸå› : timestampåˆ—ãŒæ­£ã—ãã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„
# M1: æ­£ã—ã„æ—¥æ™‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
# ä»–TF: 1970å¹´ã‹ã‚‰ã®ãƒã‚¤ã‚¯ãƒ­ç§’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå½ã®æ—¥æ™‚ï¼‰

# è§£æ±ºç­–: data_loader.py ã®ä¿®æ­£æ¸ˆã¿
# M1ä»¥å¤–ã¯timestampåˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ä½¿ç”¨
if tf == 'm1':
    df.index = pd.to_datetime(df.index)
else:
    if 'timestamp' in df.columns:
        df.index = pd.to_datetime(df['timestamp'])
        df = df.drop('timestamp', axis=1)
```

### 6. cuFFTæ··åˆç²¾åº¦å•é¡Œ
```python
# å•é¡Œ: 16bitæ··åˆç²¾åº¦ã§FFTå®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼
RuntimeError: cuFFT only supports dimensions whose sizes are powers of two 
when computing in half precision, but got a signal size of[48]

# è§£æ±ºç­–1: FFTè¨ˆç®—ã‚’32bitã«å¼·åˆ¶ï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰
pred_signal_32 = pred_signal.float()
target_signal_32 = target_signal.float()
pred_fft = torch.fft.fft(pred_signal_32, dim=-1)

# è§£æ±ºç­–2: seq_lenã‚’2ã®ç´¯ä¹—ã«å¤‰æ›´
seq_len: 64  # 48 â†’ 64
```

### 7. ãƒ¡ã‚½ãƒƒãƒ‰åä¸ä¸€è‡´å•é¡Œ
```python
# å•é¡Œ: AttributeError: 'FeatureEngineer' object has no attribute 'process_multi_tf'

# åŸå› : å®Ÿè£…ã¨å‘¼ã³å‡ºã—ã®ãƒ¡ã‚½ãƒƒãƒ‰åä¸ä¸€è‡´
# ä¿®æ­£å‰: self.feature_engineer.process_multi_tf(window_data)
# ä¿®æ­£å¾Œ: self.feature_engineer.process_window(window_data)

# æˆ»ã‚Šå€¤ã‚‚ä¿®æ­£
features, targets = self.feature_engineer.process_window(window_data)
```

## ğŸ“ˆ æ€§èƒ½æœ€é©åŒ–

### 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æœ€é©åŒ–
```python
# é«˜é€ŸåŒ–è¨­å®š
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=8,           # CPUä¸¦åˆ—åŒ–
    pin_memory=True,         # GPUè»¢é€é«˜é€ŸåŒ–
    persistent_workers=True, # ãƒ¯ãƒ¼ã‚«ãƒ¼å†åˆ©ç”¨
    prefetch_factor=4        # ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ
)
```

### 2. ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–
```python
# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«é«˜é€ŸåŒ– (PyTorch 2.0+)
model = torch.compile(model, mode='max-autotune')

# åŠ¹ç‡çš„ãª attention
from torch.nn.functional import scaled_dot_product_attention
# FlashAttentionè‡ªå‹•é©ç”¨
```

### 3. æ··åˆç²¾åº¦æœ€é©åŒ–
```yaml
# configs/performance.yaml
training:
  precision: 'bf16'           # BFloat16ä½¿ç”¨
  gradient_clip: 1.0          # æ•°å€¤å®‰å®šæ€§
  
model:
  encoder:
    flash_attn: true          # FlashAttentionæœ‰åŠ¹åŒ–
```

## ğŸ”„ CI/CD çµ±åˆ

### GitHub Actions è¨­å®šä¾‹

```yaml
# .github/workflows/stage1_test.yml
name: Stage 1 Tests

on:
  push:
    paths: ['stage1/**', 'tests/**']
  pull_request:
    paths: ['stage1/**', 'tests/**']

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        
    - name: Run data pipeline tests
      run: |
        cd tests
        python3 test_stage1_data.py
        
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v
        
    - name: Check code style
      run: |
        black --check stage1/
        isort --check stage1/
        flake8 stage1/
```

## ğŸš€ æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™

### ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»ãƒ­ãƒ¼ãƒ‰

```python
# ä¿å­˜ (è¨“ç·´å¾Œ)
torch.save({
    'model_state_dict': model.state_dict(),
    'config': config,
    'epoch': epoch,
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': best_loss,
}, 'stage1_production.pth')

# ãƒ­ãƒ¼ãƒ‰ (æ¨è«–ç”¨)
checkpoint = torch.load('stage1_production.pth')
config = checkpoint['config']
model = Stage1Model(config)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
```

### æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
class Stage1InferenceEngine:
    """æœ¬ç•ªæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, checkpoint_path: str):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.config = checkpoint['config']
        
        self.model = Stage1Model(self.config)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        # æ­£è¦åŒ–çµ±è¨ˆãƒ­ãƒ¼ãƒ‰
        self.normalizer = TFNormalizer(self.config)
        self.normalizer.load_stats()
    
    def extract_features(self, multi_tf_data: Dict[str, pd.DataFrame]) -> torch.Tensor:
        """ãƒãƒ«ãƒTFãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡º"""
        
        with torch.no_grad():
            # å‰å‡¦ç†
            engineer = FeatureEngineer(self.config)
            features, _ = engineer.process_window(multi_tf_data)
            features = self.normalizer.normalize(features.unsqueeze(0))
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            features = features.to(self.device)
            outputs = self.model(features)
            encoded = outputs['encoded']  # [1, n_tf, latent_len, d_model]
            
            return encoded.cpu()

# ä½¿ç”¨ä¾‹
engine = Stage1InferenceEngine('checkpoints/stage1_best.pth')
features = engine.extract_features(current_market_data)
# features ã‚’ Stage 2 RL ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æ¸¡ã™
```

ã“ã®ã‚¬ã‚¤ãƒ‰ã«ã‚ˆã‚Šã€é–‹ç™ºè€…ã¯Stage 1ã®å®Ÿè£…ãƒ»ãƒ‡ãƒãƒƒã‚°ãƒ»æœ€é©åŒ–ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§åŒ…æ‹¬çš„ã«ç†è§£ã§ãã¾ã™ã€‚