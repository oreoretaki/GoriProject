#!/usr/bin/env python3
"""
Stage 0 ETLæœ¬ä½“: M1â†’å…¨TFç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å”¯ä¸€ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚½ãƒ¼ã‚¹ - gapè£œå®Œãƒãƒªã‚·ãƒ¼ãƒ»TZå‡¦ç†ã‚’å«ã‚€
"""

import pandas as pd
import numpy as np
import sqlite3
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

class Stage0TFGenerator:
    """Stage 0 TFç”Ÿæˆå™¨"""
    
    def __init__(self, db_path='../data/oanda_historical.db', output_dir='../data/derived'):
        self.db_path = db_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # TFè¨­å®š: label='left', closed='left' ã§ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        # ä¾‹: M5ã®08:00ãƒãƒ¼ = 08:00-08:04ã®5æœ¬M1ã‚’é›†ç´„
        self.tf_configs = {
            'm5': {'rule': '5T', 'description': '5åˆ†è¶³'},
            'm15': {'rule': '15T', 'description': '15åˆ†è¶³'},
            'm30': {'rule': '30T', 'description': '30åˆ†è¶³'},
            'h1': {'rule': '1H', 'description': '1æ™‚é–“è¶³'},
            'h4': {'rule': '4H', 'description': '4æ™‚é–“è¶³'},
            'd': {'rule': '1D', 'description': 'æ—¥è¶³'}
        }
    
    def load_m1_from_sqlite(self):
        """SQLiteã‹ã‚‰M1ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆVolumeå®Œå…¨é™¤å¤–ï¼‰"""
        print("1. M1ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆSQLite â†’ Memoryï¼‰...")
        
        # Volumeé™¤å¤–ã§OHLCé™å®šèª­ã¿è¾¼ã¿
        conn = sqlite3.connect(self.db_path)
        query = """
        SELECT timestamp, open, high, low, close 
        FROM candles_m1 
        WHERE instrument = 'USD_JPY'
        ORDER BY timestamp
        """
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å‡¦ç†: naive â†’ UTCå¤‰æ›
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.set_index('timestamp').sort_index()
        
        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€: UTCå›ºå®šï¼ˆé‡‘èãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–ï¼‰
        if df.index.tz is None:
            df.index = df.index.tz_localize('UTC')
        elif str(df.index.tz) != 'UTC':
            df.index = df.index.tz_convert('UTC')
        
        # gap_flagè¿½åŠ : ç¾åœ¨ã¯å…¨ã¦Falseï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
        # å°†æ¥çš„ãªã‚®ãƒ£ãƒƒãƒ—è£œå®Œå¯¾å¿œã®ãŸã‚ã®äºˆç´„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        df['gap_flag'] = False
        
        print(f"   ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}ä»¶")
        print(f"   æœŸé–“: {df.index.min()} ï½ {df.index.max()}")
        print(f"   ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³: {df.index.tz}")
        print(f"   gap_flag: {df['gap_flag'].sum()}ä»¶ï¼ˆå…¨ã¦å®Ÿãƒ‡ãƒ¼ã‚¿ï¼‰")
        
        return df
    
    def save_clean_m1(self, df_m1):
        """Clean M1 Parquetä¿å­˜ï¼ˆå”¯ä¸€ã®ã‚½ãƒ¼ã‚¹çœŸå®Ÿï¼‰"""
        print("\\n2. Clean M1ä¿å­˜ï¼ˆå”¯ä¸€ã®ã‚½ãƒ¼ã‚¹çœŸå®Ÿï¼‰...")
        
        output_file = self.output_dir / 'simple_gap_aware_m1.parquet'
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿æŒã§Parquetä¿å­˜ï¼ˆSnappyåœ§ç¸®ï¼‰
        df_m1.to_parquet(output_file, compression='snappy', index=True)
        
        file_size = output_file.stat().st_size
        print(f"   ä¿å­˜å®Œäº†: {output_file}")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes ({file_size/1024/1024:.1f} MB)")
        print(f"   åœ§ç¸®å½¢å¼: Parquet + Snappy")
        
        return output_file
    
    def generate_tf_from_m1(self, df_m1, tf_name, rule):
        """M1ã‹ã‚‰TFç”Ÿæˆï¼ˆGap-awareå‡¦ç†ï¼‰"""
        
        # ã‚®ãƒ£ãƒƒãƒ—è€æ€§ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        # label='left': 08:00ãƒãƒ¼ã¯08:00-08:04ã‚’è¡¨ã™
        # closed='left': é–‹å§‹æ™‚åˆ»ã‚’å«ã¿ã€çµ‚äº†æ™‚åˆ»ã‚’å«ã¾ãªã„
        agg_rules = {
            'open': 'first',   # æœŸé–“é–‹å§‹ä¾¡æ ¼
            'high': 'max',     # æœŸé–“æœ€é«˜ä¾¡æ ¼  
            'low': 'min',      # æœŸé–“æœ€å®‰ä¾¡æ ¼
            'close': 'last'    # æœŸé–“çµ‚äº†ä¾¡æ ¼
            # gap_flagã¯é™¤å¤–: TFãƒ¬ãƒ™ãƒ«ã§ã¯ã‚®ãƒ£ãƒƒãƒ—æƒ…å ±ã‚’ä¿æŒã—ãªã„
        }
        
        # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ
        df_tf = (df_m1
                .drop(columns=['gap_flag'])  # gap_flagã‚’é™¤å¤–
                .resample(rule, label='left', closed='left')
                .agg(agg_rules)
                .dropna())  # ä¸å®Œå…¨ãªãƒãƒ¼ã‚’é™¤å»
        
        # OHLCè«–ç†ãƒã‚§ãƒƒã‚¯: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼
        invalid_ohlc = (
            (df_tf['low'] > df_tf['high']) |      # Low > High
            (df_tf['high'] < df_tf['open']) |     # High < Open  
            (df_tf['high'] < df_tf['close']) |    # High < Close
            (df_tf['low'] > df_tf['open']) |      # Low > Open
            (df_tf['low'] > df_tf['close'])       # Low > Close
        )
        
        invalid_count = invalid_ohlc.sum()
        if invalid_count > 0:
            print(f"   âš ï¸ OHLCè«–ç†é•å: {invalid_count}ä»¶æ¤œå‡º")
            # é•åãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é™¤å»ï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªä¿è­·ï¼‰
            df_tf = df_tf[~invalid_ohlc]
        
        return df_tf
    
    def save_tf_parquet(self, df_tf, tf_name):
        """TFãƒ‡ãƒ¼ã‚¿ã‚’Parquetä¿å­˜"""
        output_file = self.output_dir / f'simple_gap_aware_{tf_name}.parquet'
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦Parquetä¿å­˜
        df_output = df_tf.reset_index()
        df_output.to_parquet(output_file, compression='snappy', index=False)
        
        file_size = output_file.stat().st_size
        print(f"   {tf_name.upper()}: {len(df_tf):,}ä»¶ â†’ {output_file.name} ({file_size/1024/1024:.1f}MB)")
        
        return output_file
    
    def generate_all_timeframes(self):
        """å…¨TFç”Ÿæˆãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼"""
        print("ğŸ”§ Stage 0 ETLé–‹å§‹: M1â†’å…¨TFç”Ÿæˆ")
        print("=" * 60)
        
        # 1. M1ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df_m1 = self.load_m1_from_sqlite()
        
        # 2. Clean M1ä¿å­˜
        m1_file = self.save_clean_m1(df_m1)
        
        # 3. å„TFç”Ÿæˆ
        print("\\n3. å„TFç”Ÿæˆï¼ˆM1â†’ä¸Šä½TFï¼‰...")
        
        generated_files = [str(m1_file)]
        
        for tf_name, config in self.tf_configs.items():
            rule = config['rule']
            description = config['description']
            
            # TFç”Ÿæˆ
            df_tf = self.generate_tf_from_m1(df_m1, tf_name, rule)
            
            # ä¿å­˜
            tf_file = self.save_tf_parquet(df_tf, tf_name)
            generated_files.append(str(tf_file))
        
        # 4. ç”Ÿæˆã‚µãƒãƒªãƒ¼
        print("\\n4. ç”Ÿæˆå®Œäº†ã‚µãƒãƒªãƒ¼:")
        summary = {
            'generation_timestamp': pd.Timestamp.now().isoformat(),
            'source_database': str(self.db_path),
            'output_directory': str(self.output_dir),
            'generated_files': generated_files,
            'tf_configs': self.tf_configs,
            'volume_policy': 'EXCLUDED_COMPLETELY',
            'gap_policy': 'SIMPLE_DROPNA',
            'timezone': 'UTC_FIXED',
            'resampling_params': {
                'label': 'left',
                'closed': 'left',
                'agg_functions': {
                    'open': 'first',
                    'high': 'max', 
                    'low': 'min',
                    'close': 'last'
                }
            }
        }
        
        summary_file = self.output_dir / 'tf_generation_summary.json'
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        print(f"   ğŸ“‹ ã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_file}")
        print(f"   ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print(f"   ğŸ“Š ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(generated_files)}")
        
        print("\\nâœ… Stage 0 ETLå®Œäº†")
        print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: python3 scripts/run_validate.py ã§æ•´åˆæ€§æ¤œè¨¼")
        
        return summary

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    generator = Stage0TFGenerator()
    summary = generator.generate_all_timeframes()
    return summary

if __name__ == "__main__":
    main()