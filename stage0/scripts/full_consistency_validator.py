#!/usr/bin/env python3
"""
å…¨ä»¶TFæ•´åˆæ€§æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
M1â†’å…¨TFã®å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒã‚§ãƒƒã‚¯
"""

import pandas as pd
import numpy as np
import sqlite3
from pathlib import Path
import json
import hashlib
import warnings
warnings.filterwarnings('ignore')

class FullConsistencyValidator:
    def __init__(self):
        self.tf_files = {
            'm5': 'data/derived/simple_gap_aware_m5.parquet',
            'm15': 'data/derived/simple_gap_aware_m15.parquet', 
            'm30': 'data/derived/simple_gap_aware_m30.parquet',
            'h1': 'data/derived/simple_gap_aware_h1.parquet',
            'h4': 'data/derived/simple_gap_aware_h4.parquet',
            'd': 'data/derived/simple_gap_aware_d.parquet'
        }
        
    def load_m1_source(self):
        """M1ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        print("M1ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        
        conn = sqlite3.connect('data/oanda_historical.db')
        query = """
        SELECT timestamp, open, high, low, close 
        FROM candles_m1 
        WHERE instrument = 'USD_JPY'
        ORDER BY timestamp
        """
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.set_index('timestamp').sort_index()
        
        print(f"  M1ãƒ‡ãƒ¼ã‚¿: {len(df):,}ä»¶")
        return df
        
    def vectorized_consistency_check(self, df_m1, tf_name):
        """ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸå…¨ä»¶æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        print(f"\\n{tf_name.upper()}å…¨ä»¶æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯...")
        
        tf_file = self.tf_files[tf_name]
        
        if not Path(tf_file).exists():
            print(f"  âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãªã—: {tf_file}")
            return {'error': 'file_not_found'}
            
        # TFãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df_tf = pd.read_parquet(tf_file)
        df_tf['timestamp'] = pd.to_datetime(df_tf['timestamp'])
        df_tf = df_tf.set_index('timestamp').sort_index()
        
        print(f"  TFãƒ‡ãƒ¼ã‚¿: {len(df_tf):,}ä»¶")
        
        if len(df_tf) == 0:
            return {'error': 'empty_data'}
        
        # æœŸé–“è¨­å®š
        if tf_name == 'd':
            freq_minutes = 1440
            resample_rule = '1D'
        elif tf_name == 'h4':
            freq_minutes = 240
            resample_rule = '4H'
        elif tf_name == 'h1':
            freq_minutes = 60
            resample_rule = '1H'
        else:
            freq_minutes = int(tf_name[1:])
            resample_rule = f'{freq_minutes}T'
        
        # M1ã‹ã‚‰æœŸå¾…å€¤ã‚’å†è¨ˆç®—ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
        print(f"  M1â†’{tf_name.upper()}æœŸå¾…å€¤è¨ˆç®—ä¸­...")
        
        expected_tf = (df_m1
                      .resample(resample_rule, label='left', closed='left')
                      .agg({
                          'open': 'first',
                          'high': 'max', 
                          'low': 'min',
                          'close': 'last'
                      })
                      .dropna())
        
        print(f"  æœŸå¾…å€¤: {len(expected_tf):,}ä»¶")
        
        # å…±é€šæœŸé–“ã§ã®æ¯”è¼ƒ
        common_index = df_tf.index.intersection(expected_tf.index)
        print(f"  å…±é€šæœŸé–“: {len(common_index):,}ä»¶")
        
        if len(common_index) == 0:
            return {'error': 'no_common_periods'}
        
        # å®Ÿéš›å€¤ã¨æœŸå¾…å€¤ã®æŠ½å‡º
        actual = df_tf.loc[common_index]
        expected = expected_tf.loc[common_index]
        
        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸä¸€è‡´ãƒã‚§ãƒƒã‚¯
        tolerance = 1e-6
        
        open_matches = np.abs(actual['open'] - expected['open']) < tolerance
        high_matches = np.abs(actual['high'] - expected['high']) < tolerance
        low_matches = np.abs(actual['low'] - expected['low']) < tolerance
        close_matches = np.abs(actual['close'] - expected['close']) < tolerance
        
        # å…¨OHLCä¸€è‡´
        all_ohlc_matches = open_matches & high_matches & low_matches & close_matches
        
        # çµ±è¨ˆè¨ˆç®—
        results = {
            'total_periods': len(common_index),
            'open_matches': int(open_matches.sum()),
            'high_matches': int(high_matches.sum()),
            'low_matches': int(low_matches.sum()),
            'close_matches': int(close_matches.sum()),
            'all_ohlc_matches': int(all_ohlc_matches.sum()),
            'consistency_rates': {
                'open': float(open_matches.mean()),
                'high': float(high_matches.mean()),
                'low': float(low_matches.mean()),
                'close': float(close_matches.mean()),
                'overall': float(all_ohlc_matches.mean())
            }
        }
        
        # ä¸ä¸€è‡´ã‚µãƒ³ãƒ—ãƒ«
        mismatch_mask = ~all_ohlc_matches
        if mismatch_mask.any():
            mismatch_timestamps = actual[mismatch_mask].index[:5]
            results['mismatch_sample'] = [str(ts) for ts in mismatch_timestamps]
        else:
            results['mismatch_sample'] = []
        
        print(f"  Openä¸€è‡´: {results['consistency_rates']['open']:.1%}")
        print(f"  Highä¸€è‡´: {results['consistency_rates']['high']:.1%}")
        print(f"  Lowä¸€è‡´: {results['consistency_rates']['low']:.1%}")
        print(f"  Closeä¸€è‡´: {results['consistency_rates']['close']:.1%}")
        print(f"  ç·åˆä¸€è‡´: {results['consistency_rates']['overall']:.1%}")
        
        if results['consistency_rates']['overall'] < 0.99:
            print(f"  âš ï¸ æ•´åˆæ€§åŸºæº–æœªé”æˆ (<99%)")
            
        return results
        
    def generate_data_hashes(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ"""
        print("\\nãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆä¸­...")
        
        hashes = {}
        
        for tf_name, file_path in self.tf_files.items():
            if Path(file_path).exists():
                with open(file_path, 'rb') as f:
                    file_hash = hashlib.md5(f.read()).hexdigest()
                    file_size = Path(file_path).stat().st_size
                    
                hashes[f'{tf_name}_hash'] = file_hash
                hashes[f'{tf_name}_size'] = file_size
                
                print(f"  {tf_name.upper()}: {file_hash[:8]}... ({file_size:,} bytes)")
        
        return hashes
        
    def run_full_validation(self):
        """å®Œå…¨æ¤œè¨¼å®Ÿè¡Œ"""
        print("ğŸ” Stage 0 å®Œå…¨æ•´åˆæ€§æ¤œè¨¼")
        print("=" * 50)
        
        # M1ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df_m1 = self.load_m1_source()
        
        # å„TFã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        validation_results = {}
        
        for tf_name in self.tf_files.keys():
            result = self.vectorized_consistency_check(df_m1, tf_name)
            validation_results[tf_name] = result
        
        # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
        data_hashes = self.generate_data_hashes()
        
        # ç·åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report = {
            'validation_timestamp': pd.Timestamp.now().isoformat(),
            'validation_version': '2.0.0',
            'tf_consistency_results': validation_results,
            'data_integrity_hashes': data_hashes,
            'stage0_ready': self.assess_stage0_readiness(validation_results)
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = Path('data/derived/full_consistency_report.json')
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\\nğŸ“‹ å®Œå…¨æ•´åˆæ€§ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
        
        return report
        
    def assess_stage0_readiness(self, validation_results):
        """Stage 0æº–å‚™çŠ¶æ³è©•ä¾¡"""
        print("\\nğŸ¯ Stage 0æº–å‚™çŠ¶æ³è©•ä¾¡...")
        
        ready_criteria = {
            'all_tf_generated': True,
            'consistency_above_99pct': True,
            'no_critical_errors': True
        }
        
        for tf_name, result in validation_results.items():
            if 'error' in result:
                ready_criteria['all_tf_generated'] = False
                ready_criteria['no_critical_errors'] = False
                print(f"  âŒ {tf_name.upper()}: ã‚¨ãƒ©ãƒ¼ - {result['error']}")
            elif result.get('consistency_rates', {}).get('overall', 0) < 0.99:
                ready_criteria['consistency_above_99pct'] = False
                print(f"  âš ï¸ {tf_name.upper()}: æ•´åˆæ€§ä¸è¶³ - {result['consistency_rates']['overall']:.1%}")
            else:
                print(f"  âœ… {tf_name.upper()}: OK - {result['consistency_rates']['overall']:.1%}")
        
        stage0_ready = all(ready_criteria.values())
        
        print(f"\\nğŸš€ Stage 0ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {'âœ… READY' if stage0_ready else 'âŒ NOT READY'}")
        
        return {
            'ready': stage0_ready,
            'criteria': ready_criteria,
            'assessment_time': pd.Timestamp.now().isoformat()
        }

def main():
    validator = FullConsistencyValidator()
    report = validator.run_full_validation()
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\\nğŸ“Š æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼:")
    for tf_name, result in report['tf_consistency_results'].items():
        if 'consistency_rates' in result:
            overall = result['consistency_rates']['overall']
            print(f"  {tf_name.upper()}: {overall:.1%} æ•´åˆæ€§")
        else:
            print(f"  {tf_name.upper()}: ã‚¨ãƒ©ãƒ¼")

if __name__ == "__main__":
    main()