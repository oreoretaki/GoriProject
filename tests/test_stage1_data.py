#!/usr/bin/env python3
"""
Stage 1 ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ
ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ãƒã‚¹ã‚­ãƒ³ã‚°ã€æ•´åˆ—ã®ãƒ†ã‚¹ãƒˆ
"""

import unittest
import torch
import numpy as np
import pandas as pd
import yaml
from pathlib import Path
import sys
import tempfile
import os
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’PATHã«è¿½åŠ 
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from stage1.src.window_sampler import MultiTFWindowSampler
from stage1.src.feature_engineering import FeatureEngineer
from stage1.src.masking import MaskingStrategy
from stage1.src.normalization import TFNormalizer
from stage1.src.data_loader import Stage1Dataset

class TestStage1DataPipeline(unittest.TestCase):
    """Stage 1 ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    @classmethod
    def setUpClass(cls):
        """ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹åˆæœŸåŒ–"""
        print("\\nğŸ§ª Stage 1 ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 60)
        
        # ãƒ†ã‚¹ãƒˆç”¨è¨­å®š
        cls.config = {
            'data': {
                'seq_len': 50,  # ãƒ†ã‚¹ãƒˆç”¨ã«çŸ­ç¸®
                'n_timeframes': 6,
                'n_features': 6,
                'timeframes': ['m1', 'm5', 'm15', 'm30', 'h1', 'h4'],
                'data_dir': '../data/derived',
                'stats_file': 'test_stats.json'
            },
            'masking': {
                'mask_ratio': 0.15,
                'mask_span_min': 3,
                'mask_span_max': 10,
                'sync_across_tf': True
            },
            'validation': {
                'val_split': 0.2
            },
            'normalization': {
                'method': 'zscore',
                'per_tf': True
            }
        }
        
        # ãƒ†ã‚¹ãƒˆç”¨ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        cls.dummy_tf_data = cls._create_dummy_tf_data()
        
    @classmethod
    def _create_dummy_tf_data(cls):
        """ãƒ†ã‚¹ãƒˆç”¨ãƒ€ãƒŸãƒ¼TFãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        
        # ãƒ™ãƒ¼ã‚¹æ™‚é–“ç¯„å›²
        start_time = pd.Timestamp('2024-01-01 08:00:00', tz='UTC')
        
        tf_data = {}
        
        # M1ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ™ãƒ¼ã‚¹ï¼‰
        m1_periods = 1000
        m1_index = pd.date_range(start=start_time, periods=m1_periods, freq='1T')
        
        # ãƒ€ãƒŸãƒ¼OHLCãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒªã‚¢ãƒ«ãªä¾¡æ ¼å¤‰å‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        np.random.seed(42)
        base_price = 150.0
        returns = np.random.normal(0, 0.001, m1_periods)  # 0.1%æ¨™æº–åå·®
        prices = base_price * (1 + returns).cumprod()
        
        # OHLCè¨ˆç®—
        ohlc_data = []
        for i in range(m1_periods):
            open_price = prices[i]
            high_price = open_price * (1 + abs(np.random.normal(0, 0.0005)))
            low_price = open_price * (1 - abs(np.random.normal(0, 0.0005)))
            close_price = prices[i]
            
            ohlc_data.append([open_price, high_price, low_price, close_price])
            
        m1_df = pd.DataFrame(ohlc_data, index=m1_index, columns=['open', 'high', 'low', 'close'])
        tf_data['m1'] = m1_df
        
        # ä»–ã®TFãƒ‡ãƒ¼ã‚¿ï¼ˆM1ã‹ã‚‰é›†ç´„ï¼‰
        tf_intervals = {'m5': '5T', 'm15': '15T', 'm30': '30T', 'h1': '1H', 'h4': '4H', 'd': '1D'}
        
        for tf_name, freq in tf_intervals.items():
            agg_rules = {
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last'
            }
            
            tf_df = m1_df.resample(freq, label='left', closed='left').agg(agg_rules).dropna()
            tf_data[tf_name] = tf_df
            
        print(f"   ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†:")
        for tf_name, df in tf_data.items():
            print(f"     {tf_name.upper()}: {len(df)}ãƒ¬ã‚³ãƒ¼ãƒ‰")
            
        return tf_data
        
    def test_window_sampler_basic(self):
        """ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µãƒ³ãƒ—ãƒ©ãƒ¼åŸºæœ¬ãƒ†ã‚¹ãƒˆ"""
        print("\\n1. ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µãƒ³ãƒ—ãƒ©ãƒ¼åŸºæœ¬ãƒ†ã‚¹ãƒˆ")
        
        sampler = MultiTFWindowSampler(
            self.dummy_tf_data,
            seq_len=self.config['data']['seq_len'],
            split='train'
        )
        
        # åŸºæœ¬å±æ€§ãƒã‚§ãƒƒã‚¯
        self.assertGreater(len(sampler), 0, "æœ‰åŠ¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒå­˜åœ¨ã—ãªã„")
        
        # ã‚µãƒ³ãƒ—ãƒ«å–å¾—ãƒ†ã‚¹ãƒˆ
        sample_window = sampler[0]
        
        # å…¨TFãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
        expected_tfs = set(self.config['data']['timeframes'])
        actual_tfs = set(sample_window.keys())
        self.assertTrue(expected_tfs.issubset(actual_tfs), f"TFä¸è¶³: æœŸå¾…{expected_tfs}, å®Ÿéš›{actual_tfs}")
        
        # M1ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãƒã‚§ãƒƒã‚¯
        m1_data = sample_window['m1']
        self.assertEqual(len(m1_data), self.config['data']['seq_len'], 
                        f"M1ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ä¸æ­£: æœŸå¾…{self.config['data']['seq_len']}, å®Ÿéš›{len(m1_data)}")
        
        print(f"     âœ… ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°: {len(sampler)}")
        print(f"     âœ… M1ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {len(m1_data)}")
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æƒ…å ±è¡¨ç¤º
        info = sampler.get_sample_window_info(0)
        print(f"     âœ… ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æƒ…å ±: {info['duration_hours']:.2f}æ™‚é–“")
        
    def test_feature_engineering(self):
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        print("\\n2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
        
        engineer = FeatureEngineer(self.config)
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å–å¾—
        sampler = MultiTFWindowSampler(self.dummy_tf_data, self.config['data']['seq_len'])
        window_data = sampler[0]
        
        # ç‰¹å¾´é‡ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆ
        features, targets = engineer.process_window(window_data)
        
        # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
        expected_shape = (self.config['data']['n_timeframes'], 
                         self.config['data']['seq_len'], 
                         self.config['data']['n_features'])
        self.assertEqual(features.shape, expected_shape, 
                        f"ç‰¹å¾´é‡å½¢çŠ¶ä¸æ­£: æœŸå¾…{expected_shape}, å®Ÿéš›{features.shape}")
        
        target_shape = (self.config['data']['n_timeframes'], 
                       self.config['data']['seq_len'], 4)
        self.assertEqual(targets.shape, target_shape,
                        f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå½¢çŠ¶ä¸æ­£: æœŸå¾…{target_shape}, å®Ÿéš›{targets.shape}")
        
        # ç‰¹å¾´é‡å€¤ãƒã‚§ãƒƒã‚¯
        self.assertFalse(torch.isnan(features).any(), "ç‰¹å¾´é‡ã«NaNãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        self.assertFalse(torch.isinf(features).any(), "ç‰¹å¾´é‡ã«ç„¡é™å¤§ãŒå«ã¾ã‚Œã¦ã„ã‚‹")
        
        print(f"     âœ… ç‰¹å¾´é‡å½¢çŠ¶: {features.shape}")
        print(f"     âœ… ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå½¢çŠ¶: {targets.shape}")
        
        # ç‰¹å¾´é‡çµ±è¨ˆè¡¨ç¤º
        stats = engineer.get_feature_stats(window_data)
        print(f"     âœ… M1ç‰¹å¾´é‡ç¯„å›²: [{stats['m1']['min'][0]:.6f}, {stats['m1']['max'][0]:.6f}]")
        
    def test_masking_strategy(self):
        """ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ãƒ†ã‚¹ãƒˆ"""
        print("\\n3. ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ãƒ†ã‚¹ãƒˆ")
        
        masking = MaskingStrategy(self.config)
        
        # ãƒ€ãƒŸãƒ¼ç‰¹å¾´é‡ï¼ˆ3Dã§ç”Ÿæˆï¼‰
        features = torch.randn(self.config['data']['n_timeframes'],
                              self.config['data']['seq_len'],
                              self.config['data']['n_features'])
        
        # ãƒã‚¹ã‚¯ç”Ÿæˆ
        masks = masking.generate_masks(features, seed=42)
        
        # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
        expected_mask_shape = (self.config['data']['n_timeframes'],
                              self.config['data']['seq_len'])
        self.assertEqual(masks.shape, expected_mask_shape,
                        f"ãƒã‚¹ã‚¯å½¢çŠ¶ä¸æ­£: æœŸå¾…{expected_mask_shape}, å®Ÿéš›{masks.shape}")
        
        # ãƒã‚¹ã‚¯ç‡ãƒã‚§ãƒƒã‚¯
        for tf_idx in range(self.config['data']['n_timeframes']):
            tf_mask = masks[tf_idx]
            mask_ratio = tf_mask.mean().item()
            expected_ratio = self.config['masking']['mask_ratio']
            
            self.assertLessEqual(abs(mask_ratio - expected_ratio), 0.1,
                               f"TF{tf_idx}ã®ãƒã‚¹ã‚¯ç‡ãŒç¯„å›²å¤–: {mask_ratio:.3f} (æœŸå¾…: {expected_ratio})")
        
        print(f"     âœ… ãƒã‚¹ã‚¯å½¢çŠ¶: {masks.shape}")
        
        # ãƒã‚¹ã‚¯çµ±è¨ˆè¡¨ç¤º
        stats = masking.get_mask_statistics(masks)  # ãƒã‚¹ã‚¯ãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“
        for tf_name, tf_stats in stats.items():
            print(f"     âœ… {tf_name}: ãƒã‚¹ã‚¯ç‡{tf_stats['mask_ratio']:.3f}, ãƒ–ãƒ­ãƒƒã‚¯æ•°{tf_stats['n_blocks']}")
            
    def test_normalization(self):
        """æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ"""
        print("\\n4. æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ")
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        with tempfile.TemporaryDirectory() as temp_dir:
            test_config = self.config.copy()
            test_config['data']['data_dir'] = temp_dir
            test_config['data']['stats_file'] = 'test_stats.json'
            
            normalizer = TFNormalizer(test_config, cache_stats=True)
            
            # çµ±è¨ˆè¨ˆç®—
            normalizer.fit(self.dummy_tf_data)
            
            # çµ±è¨ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯
            self.assertGreater(len(normalizer.stats), 0, "æ­£è¦åŒ–çµ±è¨ˆãŒè¨ˆç®—ã•ã‚Œã¦ã„ãªã„")
            
            # ãƒ€ãƒŸãƒ¼ç‰¹å¾´é‡ã§æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆï¼ˆTFæ¯ã«å‡¦ç†ï¼‰
            features = torch.randn(len(self.config['data']['timeframes']), 50, 6)
            normalized = normalizer.normalize(features)
            
            # å½¢çŠ¶ä¿æŒãƒã‚§ãƒƒã‚¯
            self.assertEqual(features.shape, normalized.shape, "æ­£è¦åŒ–ã§å½¢çŠ¶ãŒå¤‰åŒ–")
            
            # NaN/Inf ãƒã‚§ãƒƒã‚¯
            self.assertFalse(torch.isnan(normalized).any(), "æ­£è¦åŒ–å¾Œã«NaN")
            self.assertFalse(torch.isinf(normalized).any(), "æ­£è¦åŒ–å¾Œã«ç„¡é™å¤§")
            
            print(f"     âœ… çµ±è¨ˆè¨ˆç®—å®Œäº†: {len(normalizer.stats)}å€‹ã®TF")
            print(f"     âœ… æ­£è¦åŒ–å½¢çŠ¶: {normalized.shape}")
            
            # çµ±è¨ˆã‚µãƒãƒªãƒ¼è¡¨ç¤º
            summary = normalizer.get_stats_summary()
            if 'm1' in summary:
                m1_stats = summary['m1']['features']
                print(f"     âœ… M1 opençµ±è¨ˆ: mean={m1_stats['open']['mean']:.6f}, std={m1_stats['open']['std']:.6f}")
                
    def test_dataset_integration(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆãƒ†ã‚¹ãƒˆ"""
        print("\\n5. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆãƒ†ã‚¹ãƒˆ")
        
        # Stage 0ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        data_dir = Path("../data/derived")
        if not data_dir.exists():
            print("     âš ï¸ Stage 0ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€çµ±åˆãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return
            
        try:
            # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆãƒ†ã‚¹ãƒˆ
            dataset = Stage1Dataset(
                str(data_dir),
                self.config,
                split="train",
                cache_stats=False  # ãƒ†ã‚¹ãƒˆç”¨
            )
            
            # åŸºæœ¬å±æ€§ãƒã‚§ãƒƒã‚¯
            self.assertGreater(len(dataset), 0, "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç©º")
            
            # ã‚µãƒ³ãƒ—ãƒ«å–å¾—ãƒ†ã‚¹ãƒˆ
            sample = dataset[0]
            
            # å¿…è¦ã‚­ãƒ¼ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            required_keys = ['features', 'targets', 'masks', 'tf_ids']
            for key in required_keys:
                self.assertIn(key, sample, f"ã‚µãƒ³ãƒ—ãƒ«ã«{key}ãŒå«ã¾ã‚Œã¦ã„ãªã„")
                
            # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
            features = sample['features']
            expected_shape = (len(self.config['data']['timeframes']),
                             self.config['data']['seq_len'],
                             self.config['data']['n_features'])
            
            self.assertEqual(features.shape, expected_shape,
                           f"ã‚µãƒ³ãƒ—ãƒ«ç‰¹å¾´é‡å½¢çŠ¶ä¸æ­£: æœŸå¾…{expected_shape}, å®Ÿéš›{features.shape}")
            
            print(f"     âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}")
            print(f"     âœ… ã‚µãƒ³ãƒ—ãƒ«ç‰¹å¾´é‡å½¢çŠ¶: {features.shape}")
            print(f"     âœ… ã‚µãƒ³ãƒ—ãƒ«ã‚­ãƒ¼: {list(sample.keys())}")
            
        except Exception as e:
            print(f"     âš ï¸ çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ï¼ˆãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å¯èƒ½æ€§ï¼‰: {e}")
            
    def test_data_alignment(self):
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆ—ãƒ†ã‚¹ãƒˆ"""
        print("\\n6. ãƒ‡ãƒ¼ã‚¿æ•´åˆ—ãƒ†ã‚¹ãƒˆ")
        
        sampler = MultiTFWindowSampler(self.dummy_tf_data, self.config['data']['seq_len'])
        window_data = sampler[0]
        
        # æ™‚é–“æ•´åˆ—ãƒã‚§ãƒƒã‚¯
        m1_data = window_data['m1']
        m5_data = window_data['m5']
        
        # M1ã¨M5ã®æ™‚é–“ç¯„å›²ãŒä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        m1_start = m1_data.index[0]
        m1_end = m1_data.index[-1]
        m5_start = m5_data.index[0]
        m5_end = m5_data.index[-1]
        
        # å³ç«¯æ•´åˆ—ï¼šçµ‚äº†æ™‚åˆ»ãŒè¿‘ã„ã“ã¨ã‚’ç¢ºèª
        time_diff = abs((m1_end - m5_end).total_seconds())
        self.assertLess(time_diff, 300, f"M1ã¨M5ã®çµ‚äº†æ™‚åˆ»ãŒå¤§ããç•°ãªã‚‹: {time_diff}ç§’")
        
        print(f"     âœ… M1æœŸé–“: {m1_start} â†’ {m1_end}")
        print(f"     âœ… M5æœŸé–“: {m5_start} â†’ {m5_end}")
        print(f"     âœ… çµ‚äº†æ™‚åˆ»å·®: {time_diff:.0f}ç§’")
        
        # OHLCå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        for tf_name, tf_data in window_data.items():
            if len(tf_data) > 0:
                # High >= Open, Close; Low <= Open, Close
                highs = tf_data['high'].values
                lows = tf_data['low'].values
                opens = tf_data['open'].values
                closes = tf_data['close'].values
                
                valid_high = np.all(highs >= np.maximum(opens, closes))
                valid_low = np.all(lows <= np.minimum(opens, closes))
                
                self.assertTrue(valid_high, f"{tf_name}: High < Open/Close")
                self.assertTrue(valid_low, f"{tf_name}: Low > Open/Close")
                
        print("     âœ… OHLCå¦¥å½“æ€§: å…¨TFåˆæ ¼")

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸš€ Stage 1 ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    
    # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆä½œæˆ
    loader = unittest.TestLoader()
    suite = loader.loadTestsFromTestCase(TestStage1DataPipeline)
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\\n" + "=" * 60)
    if result.wasSuccessful():
        print("âœ… å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼")
        return 0
    else:
        print("âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—")
        print(f"   å¤±æ•—æ•°: {len(result.failures)}")
        print(f"   ã‚¨ãƒ©ãƒ¼æ•°: {len(result.errors)}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)