# T5転移学習 - 凍結なし（10エポック版）
# T5を最初から微調整、Early Stopping付き

# データ設定
data:
  seq_len: 128  # 統一シーケンス長
  n_timeframes: 6                 # TF数 (M1, M5, M15, M30, H1, H4)
  n_features: 6                   # 特徴量数 [open, high, low, close, Δclose, %body]
  total_channels: 36              # 6特徴量 × 6TF = 36チャンネル
  timeframes:
    - m1
    - m5  
    - m15
    - m30
    - h1
    - h4
  data_dir: "../data/derived"
  stats_file: "stats.json"
  # 🔥 Drop-in Sampling: データ不均衡対策（短TFの重複率を下げる）
  sampling_probs:
    m1: 1.00    # 🔥 M1は必ず残す（Cross-loss計算に必要）
    m5: 0.40    # 中頻度サンプリング
    m15: 0.55   # 中頻度サンプリング
    m30: 0.70   # 高頻度サンプリング
    h1: 0.85    # 高頻度サンプリング
    h4: 1.00    # 最高頻度サンプリング

# T5転移学習設定
transfer_learning:
  use_pretrained_lm: true
  lm_name_or_path: "t5-large"           # 正式ID（約738M parameters）
  freeze_lm_epochs: 0                   # 最初から解凍 + Layerwise LR Decay
  patch_len: 8                          # 🔥 M1改善: 128/8=16 patches (高解像度)
  lm_learning_rate_factor: 0.01         # T5学習率を10倍に調整

# モデルアーキテクチャ（T5統合）
model:
  # Model v2: 非同期マルチスケール設定
  async_sampler: true               # 🔥 非同期モード有効化でH4制約解除
  cross_pairs:                      # coarse→fine cross-attention pairs
    - ["h4", "m1"]
    - ["h1", "m1"] 
    - ["m30", "m1"]
    
  tf_stem:
    kernel_size: 3
    d_model: 1024                 # T5のd_modelに合わせる
    
  encoder:
    n_layers: 4                   # T5後の追加層数
    d_model: 1024                 # T5のd_modelに統一
    d_state: 8
    d_conv: 4
    expand: 2
    cross_attn_every: 2
    flash_attn: false
    
  bottleneck:
    latent_len: 16                # seq_len/8 = 128/8 = 16
    stride: 8
    
  decoder:
    n_layers: 2
    kernel_size: 3
    
  positional:
    intra_tf: "rotary"
    inter_tf: "learned"

# 訓練設定（T5-Large安定化）
training:
  batch_size: 32                  # 🔥 OOM対策: 緊急削減（768→32）
  epochs: 7                       # 7エポック設定
  learning_rate: 1e-3
  weight_decay: 1e-4
  accumulate_grad_batches: 1      # 🔥 GPU利用率最適化：1に固定
  gradient_clip: 0.012            # 5倍データ対応で緩和（0.008→0.012）
  warmup_epochs: 1                # 5倍データ用に短縮（全体の約5%）
  precision: "bf16-mixed"         # 🔥 H100最適化：BF16 Mixed Precisionで高速化
  early_stop:
    patience: 3                   # 3エポック改善なしで早期終了
    min_delta: 0.001
    
  # 統一されたオプティマイザー設定
  optimizer:
    name: "AdamW"
    lr: 1.5e-3                        # ヘッド & Adapter 用の基準 LR（LR Finder v4推奨値 - バッチサイズ1024用）
    betas: [0.9, 0.98]
    weight_decay: 0.01
    
  # Layerwise LR Decay設定
  layerwise_lr_decay: 0.90             # 下位層ほど LR を 0.90 倍ずつ減衰（緩和）
  t5_lr_top: 3.7e-4                    # T5 エンコーダ最上層用 LR（LR Finder v4推奨値 - バッチサイズ1024用）
    
  # 統一されたLRスケジューラー設定
  scheduler:
    name: "linear_with_warmup"  # Linear Warmup + Cosine Decay
    max_lr: 1.5e-3              # 最大学習率（LR Finder v4推奨値 - バッチサイズ1024用）
    div_factor: 10.0            # OneCycleLR用（互換性保持）
    final_div_factor: 50
    pct_start: 0.10             
    interval: "step"            # ステップ単位で更新

# データ拡張（無効化）
augmentation:
  ddim_noise:
    probability: 0.0
  time_warp:
    probability: 0.0
  regime_mix:
    probability: 0.0

# 検証・メトリクス
validation:
  val_split: 0.2
  val_gap_days: 60.0             # 訓練と検証の間の時間的ギャップ（日数）- 60日間で完全分離
  metrics:
    - "correlation_per_tf"
    - "consistency_ratio"
    - "spectral_delta"

# 評価設定 (M1改善 + val_corr=0問題修正)
evaluation:
  eval_mask_ratio: 0.15              # 評価時も15%マスクを適用（学習時と同条件）
  # 🔥 M1は高マスク率で評価（高周波ノイズ軽減）
  tf_specific_mask_ratios:
    m1: 0.25                         # M1は25%マスク（他より厳しく）
    m5: 0.15
    m15: 0.15  
    m30: 0.15
    h1: 0.15
    h4: 0.15

# ログ・チェックポイント（10エポック最適化）
logging:
  log_every_n_steps: 200          # 🔥 stdout削減: 200step毎のみログ
  checkpoint_every_n_epochs: 1    # 毎エポック評価
  save_top_k: 2                   # ベスト2のみ保存（ディスク節約）
  monitor: "val_correlation_mean"
  progress_bar_refresh_rate: 5    # 🔥 進捗確認: 5step毎に更新

# 実行設定
runtime:
  seed: 42
  experiment_name: "t5_large_nofreeze"
  
# DataLoader最適化（H100クラウドGPU用）- 高性能設定
dataloader:
  num_workers: 32                 # 🔥 H100用: スレッド数の1/2（Python GIL + I/O並列効率最適化）
  pin_memory: true                # 🔥 H100用: HBM→GPU転送ボトルネック解消（PCIe待ち短縮）
  persistent_workers: true        # 🔥 H100用: 毎エポックfork/exit無しで10-15%高速化
  prefetch_factor: 4              # 🔥 H100用: 各ワーカー4バッチ先読み（GPUを空かさない）
  # メモリ増加予想: +4-5GB（num_workers:+1GB, pin_memory:+2GB, prefetch:+1-2GB）

# マスキング設定（ベクトル化を一時無効化）
masking:
  mask_ratio: 0.15
  mask_span_min: 3
  mask_span_max: 10
  sync_across_tf: false
  mask_token_lr_scale: 1.0
  use_vectorized: true            # 🔥 ベクトル化有効（10倍高速）

# 正規化設定
normalization:
  method: "zscore"
  per_tf: true

# ロス設定（パフォーマンス最適化）
loss:
  weights:
    recon_tf: 0.6
    spec_tf: 0.0        # 🔥 一時無効化（パフォーマンス重視）
    cross: 0.02         # 🔥 33%増加（クロス整合性重視）
    amp_phase: 0.0      # 🔥 一時無効化（パフォーマンス重視）
  huber_delta: 1.0
  stft_scales: [64, 128]  # 🔥 256→128に削減（計算量半減）

# 開発用設定（中間規模テスト）
development:
  limit_train_batches: 3000       # 🔥 中間テスト: 3000バッチ
  limit_val_batches: 600          # 🔥 中間テスト: 600バッチ（比例）

# LR Finder設定
lr_finder:
  enabled: false                  # LR Finder完了 - 通常訓練モード
  min_lr: 1e-8                   # 開始学習率
  max_lr: 1.0                    # 終了学習率
  num_training: 100              # 学習ステップ数
  mode: "exponential"            # 学習率増加方法
  save_path: "lr_finder_results" # 結果保存ディレクトリ