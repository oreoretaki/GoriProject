# T5è»¢ç§»å­¦ç¿’ - å‡çµãªã—ï¼ˆ10ã‚¨ãƒãƒƒã‚¯ç‰ˆï¼‰
# T5ã‚’æœ€åˆã‹ã‚‰å¾®èª¿æ•´ã€Early Stoppingä»˜ã

# å…±é€šè¨­å®šã‚’ç¶™æ‰¿
extends: shared_base.yaml

# T5è»¢ç§»å­¦ç¿’è¨­å®š
transfer_learning:
  use_pretrained_lm: true
  lm_name_or_path: "t5-large"           # æ­£å¼IDï¼ˆç´„738M parametersï¼‰
  freeze_lm_epochs: 0                   # æœ€åˆã‹ã‚‰è§£å‡ + Layerwise LR Decay
  patch_len: 16                         # 128/16=8 patches (6*16=96æ¬¡å…ƒ)
  lm_learning_rate_factor: 0.01         # T5å­¦ç¿’ç‡ã‚’10å€ã«èª¿æ•´

# ãƒ‡ãƒ¼ã‚¿è¨­å®šã¯ shared_base.yaml ã‹ã‚‰ç¶™æ‰¿

# ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆT5çµ±åˆï¼‰
model:
  # Model v2: éåŒæœŸãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨­å®š
  async_sampler: true               # ğŸ”¥ éåŒæœŸãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–ã§H4åˆ¶ç´„è§£é™¤
  cross_pairs:                      # coarseâ†’fine cross-attention pairs
    - ["h4", "m1"]
    - ["h1", "m1"] 
    - ["m30", "m1"]
    
  tf_stem:
    kernel_size: 3
    d_model: 1024                 # T5ã®d_modelã«åˆã‚ã›ã‚‹
    
  encoder:
    n_layers: 4                   # T5å¾Œã®è¿½åŠ å±¤æ•°
    d_model: 1024                 # T5ã®d_modelã«çµ±ä¸€
    d_state: 8
    d_conv: 4
    expand: 2
    cross_attn_every: 2
    flash_attn: true
    
  bottleneck:
    latent_len: 16                # seq_len/8 = 128/8 = 16
    stride: 8
    
  decoder:
    n_layers: 2
    kernel_size: 3
    
  positional:
    intra_tf: "rotary"
    inter_tf: "learned"

# è¨“ç·´è¨­å®šï¼ˆT5-Largeå®‰å®šåŒ–ï¼‰
training:
  batch_size: 768                 # ğŸ”¥ H100æœ€é©åŒ–: ä¸­é–“ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆ500-1000ã®ç¯„å›²ï¼‰
  epochs: 7                       # 7ã‚¨ãƒãƒƒã‚¯è¨­å®š
  accumulate_grad_batches: 1      # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º1024
  warmup_epochs: 1              # 5å€ãƒ‡ãƒ¼ã‚¿ç”¨ã«çŸ­ç¸®ï¼ˆå…¨ä½“ã®ç´„5%ï¼‰
  early_stop:
    patience: 3                   # 3ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãªã—ã§æ—©æœŸçµ‚äº†
    min_delta: 0.001
    
  precision: "bf16"           # H100æœ€é©åŒ–ï¼šBF16ã§Tensor Coreæ´»ç”¨

# ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆç„¡åŠ¹åŒ–ï¼‰
augmentation:
  ddim_noise:
    probability: 0.0
  time_warp:
    probability: 0.0
  regime_mix:
    probability: 0.0

# æ¤œè¨¼ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹
validation:
  val_split: 0.2
  metrics:
    - "correlation_per_tf"
    - "consistency_ratio"
    - "spectral_delta"

# è©•ä¾¡è¨­å®š (val_corr=0å•é¡Œä¿®æ­£)
evaluation:
  eval_mask_ratio: 0.15              # è©•ä¾¡æ™‚ã‚‚15%ãƒã‚¹ã‚¯ã‚’é©ç”¨ï¼ˆå­¦ç¿’æ™‚ã¨åŒæ¡ä»¶ï¼‰

# ãƒ­ã‚°ãƒ»ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆ10ã‚¨ãƒãƒƒã‚¯æœ€é©åŒ–ï¼‰
logging:
  log_every_n_steps: 10
  checkpoint_every_n_epochs: 1    # æ¯ã‚¨ãƒãƒƒã‚¯è©•ä¾¡
  save_top_k: 2                   # ãƒ™ã‚¹ãƒˆ2ã®ã¿ä¿å­˜ï¼ˆãƒ‡ã‚£ã‚¹ã‚¯ç¯€ç´„ï¼‰
  monitor: "val_correlation_mean"
  progress_bar_refresh_rate: 50

# å®Ÿè¡Œè¨­å®š
runtime:
  seed: 42
  experiment_name: "t5_large_nofreeze"
  
# DataLoaderæœ€é©åŒ–ï¼ˆH100ã‚¯ãƒ©ã‚¦ãƒ‰GPUç”¨ï¼‰- é«˜æ€§èƒ½è¨­å®š
dataloader:
  num_workers: 32                 # ğŸ”¥ H100ç”¨: ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã®1/2ï¼ˆPython GIL + I/Oä¸¦åˆ—åŠ¹ç‡æœ€é©åŒ–ï¼‰
  pin_memory: true                # ğŸ”¥ H100ç”¨: HBMâ†’GPUè»¢é€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è§£æ¶ˆï¼ˆPCIeå¾…ã¡çŸ­ç¸®ï¼‰
  persistent_workers: true        # ğŸ”¥ H100ç”¨: æ¯ã‚¨ãƒãƒƒã‚¯fork/exitç„¡ã—ã§10-15%é«˜é€ŸåŒ–
  prefetch_factor: 4              # ğŸ”¥ H100ç”¨: å„ãƒ¯ãƒ¼ã‚«ãƒ¼4ãƒãƒƒãƒå…ˆèª­ã¿ï¼ˆGPUã‚’ç©ºã‹ã•ãªã„ï¼‰
  # ãƒ¡ãƒ¢ãƒªå¢—åŠ äºˆæƒ³: +4-5GBï¼ˆnum_workers:+1GB, pin_memory:+2GB, prefetch:+1-2GBï¼‰

# é–‹ç™ºç”¨è¨­å®šï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ï¼‰
development:
  limit_train_batches: 100        # ğŸ”¥ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”¨: 100ãƒãƒƒãƒé™å®š
  limit_val_batches: 20           # ğŸ”¥ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”¨: 20ãƒãƒƒãƒé™å®š