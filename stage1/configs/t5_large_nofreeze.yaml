# ---------------------------------
# Stage-1 T5-Large No-Freeze (H100)
# ---------------------------------

runtime:
  seed: 42
  experiment_name: "t5_large_nofreeze"

data:
  seq_len: 128
  timeframes: [m1, m5, m15, m30, h1, h4]
  n_timeframes: 6
  n_features: 6
  total_channels: 36
  data_dir: "../data/derived"
  stats_file: "stats.json"
  sampling_probs: {m1: 1.00, m5: 1.00, m15: 0.55, m30: 0.70, h1: 0.85, h4: 1.00}
  sampling_probs_val: {m1: 1.00, m5: 1.00, m15: 1.00, m30: 1.00, h1: 1.00, h4: 1.00}

dataloader:
  num_workers: 32
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 4

masking:
  mask_ratio: 0.15
  mask_span_min: 3
  mask_span_max: 10
  sync_across_tf: false
  use_vectorized: true
  mask_token_lr_scale: 1.0

loss:
  weights: {recon_tf: 0.6, spec_tf: 0.0, cross: 0.02, amp_phase: 0.0}
  huber_delta: 1.0
  stft_scales: [64, 128]

training:
  batch_size: 256   # H100最大活用サイズ
  accumulate_grad_batches: 1
  epochs: 7
  precision: "bf16-mixed"
  gradient_clip: 0.012
  warmup_epochs: 0
  layerwise_lr_decay: 0.90
  t5_lr_top: 3.7e-4
  early_stop:
    patience: 3
    min_delta: 0.001
  optimizer:
    name: AdamW
    lr: 4.0e-3  # 線形スケーリング (192→256 で ×1.333)
    betas: [0.9, 0.98]
    weight_decay: 0.01
  scheduler:
    name: linear_with_warmup
    max_lr: 4.0e-3  # バッチサイズ256用
    num_warmup_steps: 1000
    interval: "step"
    div_factor: 10.0
    final_div_factor: 50
    pct_start: 0.10

development:
  limit_train_batches: 2000   # 2000ステップでテスト
  limit_val_batches: 1000     # 1000ステップで検証

evaluation:
  # 全体のデフォルト
  eval_mask_ratio: 0.15
  
  # TF ごとの上書き（seq_len考慮調整）
  tf_specific_mask_ratios:
    m1: 0.25  # seq_len 128 → 32本masked
    m5: 0.15  # seq_len 26 → 4本
    m15: 0.05 # seq_len 9 → 0-1本
    m30: 0.05 # ★ 下げる seq_len 5 → 0-1本  
    h1: 0.20  # seq_len 3 → 1本
    h4: 0.00  # ★ 最低に seq_len 1 → 0本（常に評価可能）

logging:
  log_every_n_steps: 200
  checkpoint_every_n_epochs: 1
  save_top_k: 1                # 最良1個のみ保存（容量節約）
  monitor: "val_correlation_mean"
  progress_bar_refresh_rate: 5

transfer_learning:
  use_pretrained_lm: true
  lm_name_or_path: "t5-large"
  freeze_lm_epochs: 0
  patch_len: 8
  lm_learning_rate_factor: 0.01

model:
  async_sampler: true
  cross_pairs:
    - ["h4", "m1"]
    - ["h1", "m1"]
    - ["m30", "m1"]
  tf_stem: 
    kernel_size: 3
    d_model: 1024
  encoder: 
    n_layers: 4
    d_model: 1024
    d_state: 8
    d_conv: 4
    expand: 2
    cross_attn_every: 2
    flash_attn: true
  bottleneck: 
    latent_len: 16
    stride: 8
  decoder: 
    n_layers: 2
    kernel_size: 3
  positional: 
    intra_tf: rotary
    inter_tf: learned

validation:
  val_split: 0.2
  val_gap_days: 60.0
  metrics:
    - "correlation_per_tf"
    - "consistency_ratio"
    - "spectral_delta"

normalization:
  method: zscore
  per_tf: true

augmentation:
  ddim_noise:
    probability: 0.0
  time_warp:
    probability: 0.0
  regime_mix:
    probability: 0.0

lr_finder:
  enabled: false
  min_lr: 1e-8
  max_lr: 1.0
  num_training: 100
  mode: exponential
  save_path: lr_finder_results