# T5転移学習 - 凍結なし（10エポック版）
# T5を最初から微調整、Early Stopping付き

# 共通設定を継承
extends: shared_base.yaml

# T5転移学習設定
transfer_learning:
  use_pretrained_lm: true
  lm_name_or_path: "t5-large"           # 正式ID（約738M parameters）
  freeze_lm_epochs: 0                   # 最初から解凍 + Layerwise LR Decay
  patch_len: 16                         # 128/16=8 patches (6*16=96次元)
  lm_learning_rate_factor: 0.01         # T5学習率を10倍に調整

# データ設定は shared_base.yaml から継承

# モデルアーキテクチャ（T5統合）
model:
  # Model v2: 非同期マルチスケール設定
  async_sampler: true               # 🔥 非同期モード有効化でH4制約解除
  cross_pairs:                      # coarse→fine cross-attention pairs
    - ["h4", "m1"]
    - ["h1", "m1"] 
    - ["m30", "m1"]
    
  tf_stem:
    kernel_size: 3
    d_model: 1024                 # T5のd_modelに合わせる
    
  encoder:
    n_layers: 4                   # T5後の追加層数
    d_model: 1024                 # T5のd_modelに統一
    d_state: 8
    d_conv: 4
    expand: 2
    cross_attn_every: 2
    flash_attn: true
    
  bottleneck:
    latent_len: 16                # seq_len/8 = 128/8 = 16
    stride: 8
    
  decoder:
    n_layers: 2
    kernel_size: 3
    
  positional:
    intra_tf: "rotary"
    inter_tf: "learned"

# 訓練設定（T5-Large安定化）
training:
  batch_size: 512                 # H100最適化: 1024→512（OOM対策）
  epochs: 7                       # 7エポック設定
  accumulate_grad_batches: 1      # 実効バッチサイズ1024
  warmup_epochs: 1              # 5倍データ用に短縮（全体の約5%）
  early_stop:
    patience: 3                   # 3エポック改善なしで早期終了
    min_delta: 0.001
    
  precision: "bf16"           # H100最適化：BF16でTensor Core活用

# データ拡張（無効化）
augmentation:
  ddim_noise:
    probability: 0.0
  time_warp:
    probability: 0.0
  regime_mix:
    probability: 0.0

# 検証・メトリクス
validation:
  val_split: 0.2
  metrics:
    - "correlation_per_tf"
    - "consistency_ratio"
    - "spectral_delta"

# 評価設定 (val_corr=0問題修正)
evaluation:
  eval_mask_ratio: 0.15              # 評価時も15%マスクを適用（学習時と同条件）

# ログ・チェックポイント（10エポック最適化）
logging:
  log_every_n_steps: 10
  checkpoint_every_n_epochs: 1    # 毎エポック評価
  save_top_k: 2                   # ベスト2のみ保存（ディスク節約）
  monitor: "val_correlation_mean"
  progress_bar_refresh_rate: 50

# 実行設定
runtime:
  seed: 42
  experiment_name: "t5_large_nofreeze"
  
# DataLoader最適化（T5用）- shared_baseから継承されるが、T5用に調整
dataloader:
  num_workers: 12                 # 18→12に削減（プロセス制御問題回避）
  pin_memory: true                # GPU転送高速化
  persistent_workers: true        # プロセス立ち上げオーバーヘッド削減
  prefetch_factor: 6              # 先読み強化（2→6）

# 開発用設定（全データ投入）
development:
  # limit_train_batches: 2383      # 🔥 制限解除：全データ投入
  # limit_val_batches: 606         # 🔥 制限解除：全データ投入
  # 🔥 shared_base.yamlの制限をオーバーライド
  limit_train_batches: null         # 完全に制限解除
  limit_val_batches: null           # 完全に制限解除