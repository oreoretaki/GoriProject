Learning Rate Finder Results v4 - Batch Size 1024
========================================
Configuration:
  Batch Size: 1024 (H100 optimized)
  Min LR: 1.0e-07
  Max LR: 1.0e-02
  Steps: 200
  Mode: exponential
  Execution time: ~2 minutes

Results:
  Suggested LR: 1.5e-03
  Current base_lr: 7.1e-07
  Current t5_lr_top: 1.8e-07

Recommendations:
  - Set optimizer.lr to: 1.5e-03
  - Set t5_lr_top to: 3.7e-04

Batch Size Scaling Analysis:
  Previous (batch 256): LR 7.1e-07
  Current (batch 1024): LR 1.5e-03
  Scaling factor: ~200x (4x batch size → 200x LR increase)

Hardware & Optimization:
  - GPU: H100 NVL (100GB VRAM)
  - Precision: BF16 (Tensor Core optimized)
  - TorchCompile: enabled with warm-up
  - DataLoader: optimized (num_workers=18, prefetch=6)

Status: ✅ Applied to shared_base.yaml (committed: 1f08f10)
