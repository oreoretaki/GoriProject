data:
  seq_len: 128
  n_timeframes: 6
  n_features: 6
  total_channels: 36
  timeframes:
  - m1
  - m5
  - m15
  - m30
  - h1
  - h4
  data_dir: ../data/derived
  stats_file: stats.json
  sampling_probs:
    m1: 1.0
    m5: 0.4
    m15: 0.55
    m30: 0.7
    h1: 0.85
    h4: 1.0
model:
  async_sampler: true
  tf_stem:
    kernel_size: 3
    d_model: 1024
  encoder:
    n_layers: 4
    d_model: 1024
    d_state: 8
    d_conv: 4
    expand: 2
    cross_attn_every: 2
    flash_attn: false
  bottleneck:
    latent_len: 16
    stride: 8
  decoder:
    n_layers: 2
    kernel_size: 3
  positional:
    intra_tf: rotary
    inter_tf: learned
  cross_pairs:
  - - h4
    - m1
  - - h1
    - m1
  - - m30
    - m1
training:
  batch_size: 768
  epochs: 7
  learning_rate: '1e-3'
  weight_decay: '1e-4'
  warmup_epochs: 1
  accumulate_grad_batches: 1
  gradient_clip: 0.012
  precision: bf16-mixed
  early_stop:
    patience: 3
    min_delta: 0.001
  optimizer:
    name: AdamW
    lr: 0.0015
    betas:
    - 0.9
    - 0.98
    weight_decay: 0.01
  layerwise_lr_decay: 0.9
  t5_lr_top: 0.00037
  scheduler:
    name: linear_with_warmup
    max_lr: 0.0015
    div_factor: 10.0
    final_div_factor: 50
    pct_start: 0.1
    interval: step
masking:
  mask_ratio: 0.15
  mask_span_min: 3
  mask_span_max: 10
  sync_across_tf: false
  mask_token_lr_scale: 1.0
  use_vectorized: true
normalization:
  method: zscore
  per_tf: true
loss:
  weights:
    recon_tf: 0.6
    spec_tf: 0.01
    cross: 0.02
    amp_phase: 0.02
  huber_delta: 1.0
  stft_scales:
  - 128
  - 256
augmentation:
  ddim_noise:
    probability: 0.0
  time_warp:
    probability: 0.0
  regime_mix:
    probability: 0.0
validation:
  val_split: 0.2
  val_gap_days: 60.0
  metrics:
  - correlation_per_tf
  - consistency_ratio
  - spectral_delta
evaluation:
  eval_mask_ratio: 0.15
logging:
  log_every_n_steps: 200
  checkpoint_every_n_epochs: 1
  save_top_k: 2
  monitor: val_correlation_mean
  progress_bar_refresh_rate: 5
runtime:
  seed: 42
  experiment_name: t5_large_nofreeze
dataloader:
  num_workers: 32
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 4
development:
  limit_train_batches: 100
  limit_val_batches: 20
lr_finder:
  enabled: false
  min_lr: '1e-8'
  max_lr: 1.0
  num_training: 100
  mode: exponential
  save_path: lr_finder_results
transfer_learning:
  use_pretrained_lm: true
  lm_name_or_path: t5-large
  freeze_lm_epochs: 0
  patch_len: 16
  lm_learning_rate_factor: 0.01
