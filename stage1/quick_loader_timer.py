#!/usr/bin/env python3
"""
DataLoaderå˜ä½“ã®é€Ÿåº¦ãƒ†ã‚¹ãƒˆ
GPUè¨ˆç®—ã‚’é™¤å¤–ã—ã¦ã€ç´”ç²‹ãªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é€Ÿåº¦ã‚’æ¸¬å®š
"""

import time
import torch
import yaml
from pathlib import Path
import sys
sys.path.append('src')

from src.data_loader import create_stage1_dataloaders

def test_dataloader_speed():
    print("ğŸ” DataLoaderå˜ä½“é€Ÿåº¦ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    with open("configs/t5_large_nofreeze.yaml", 'r') as f:
        config = yaml.safe_load(f)
    
    # ç¶™æ‰¿è¨­å®šå‡¦ç†
    if 'extends' in config:
        base_path = Path("configs") / config['extends']
        with open(base_path, 'r') as f:
            base_config = yaml.safe_load(f)
        
        # ã•ã‚‰ã«ç¶™æ‰¿ãŒã‚ã‚‹å ´åˆ
        if 'extends' in base_config:
            root_path = Path("configs") / base_config['extends']
            with open(root_path, 'r') as f:
                root_config = yaml.safe_load(f)
            root_config.update(base_config)
            base_config = root_config
        
        base_config.update(config)
        config = base_config
    
    # DataLoaderä½œæˆ
    print("ğŸ“Š DataLoaderä½œæˆä¸­...")
    train_loader, _ = create_stage1_dataloaders("../data/derived", config)
    
    print(f"   ç·ãƒãƒƒãƒæ•°: {len(train_loader):,}")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {config['training']['batch_size']}")
    print(f"   num_workers: {config['dataloader']['num_workers']}")
    print(f"   pin_memory: {config['dataloader']['pin_memory']}")
    print(f"   persistent_workers: {config['dataloader']['persistent_workers']}")
    print(f"   prefetch_factor: {config['dataloader']['prefetch_factor']}")
    
    # é€Ÿåº¦æ¸¬å®š (200ãƒãƒƒãƒ)
    test_batches = min(200, len(train_loader))
    print(f"\nğŸš€ é€Ÿåº¦æ¸¬å®šé–‹å§‹ ({test_batches}ãƒãƒƒãƒ)")
    
    t0 = time.time()
    for i, batch in zip(range(test_batches), train_loader):
        # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’è»½ãè§¦ã‚‹ï¼ˆé…å»¶èª­ã¿è¾¼ã¿å›é¿ï¼‰
        if isinstance(batch, dict):
            # Model v2ã®Dictå½¢å¼ã®å ´åˆ
            if 'features' in batch:
                for tf_name, tf_tensor in batch['features'].items():
                    if hasattr(tf_tensor, 'shape'):
                        _ = tf_tensor.shape  # shapeç¢ºèªã ã‘ï¼ˆGPUè»¢é€ãªã—ï¼‰
            if 'targets' in batch:
                for tf_name, tf_tensor in batch['targets'].items():
                    if hasattr(tf_tensor, 'shape'):
                        _ = tf_tensor.shape  # shapeç¢ºèªã ã‘ï¼ˆGPUè»¢é€ãªã—ï¼‰
        else:
            # Legacyå½¢å¼ã®å ´åˆ
            if hasattr(batch, 'shape'):
                _ = batch.shape
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
        if i % 50 == 0:
            elapsed = time.time() - t0
            if elapsed > 0:
                current_speed = (i + 1) / elapsed
                print(f"   {i+1:3d}/{test_batches} ãƒãƒƒãƒ - {current_speed:.2f} it/s")
    
    # çµæœè¨ˆç®—
    dt = time.time() - t0
    speed = test_batches / dt
    
    print(f"\nğŸ“ˆ çµæœ:")
    print(f"   æ¸¬å®šæ™‚é–“: {dt:.2f}ç§’")
    print(f"   DataLoaderå˜ä½“é€Ÿåº¦: {speed:.2f} it/s")
    
    # è¨ºæ–­
    print(f"\nğŸ” è¨ºæ–­:")
    if speed >= 5.0:
        print("   âœ… DataLoaderã¯å•é¡Œãªã— â†’ ãƒ¢ãƒ‡ãƒ«è¨ˆç®—ã«ç„¦ç‚¹")
        print("   ğŸ’¡ æ¬¡ã®æ‰‹: torch.compileè©¦è¡Œ / batch_sizeå¢—åŠ ")
    elif speed >= 1.0:
        print("   âš ï¸ DataLoaderãŒã‚„ã‚„é…ã„ â†’ è¨­å®šèª¿æ•´æ¨å¥¨")
        print("   ğŸ’¡ æ¬¡ã®æ‰‹: num_workersèª¿æ•´ / prefetch_factorå¤‰æ›´")
    else:
        print("   âŒ DataLoaderãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ â†’ è¨­å®šè¦‹ç›´ã—å¿…è¦")
        print("   ğŸ’¡ æ¬¡ã®æ‰‹: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸é€Ÿåº¦ç¢ºèª / num_workerså‰Šæ¸›")
    
    # å‚è€ƒå€¤
    print(f"\nğŸ“Š å‚è€ƒå€¤:")
    print(f"   æœŸå¾…å€¤: >5 it/s (DataLoaderå•é¡Œãªã—)")
    print(f"   ç¾åœ¨å€¤: {speed:.2f} it/s")
    
    return speed

if __name__ == "__main__":
    test_dataloader_speed()