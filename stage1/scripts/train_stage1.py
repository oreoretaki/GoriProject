#!/usr/bin/env python3
"""
Stage 1 è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Multi-TF Self-Supervised Reconstruction Training
"""

import os
# PyTorchåˆ†æ•£å­¦ç¿’ã‚’å®Œå…¨ç„¡åŠ¹åŒ–ï¼ˆimportã‚ˆã‚Šå‰ã«è¨­å®šï¼‰
os.environ['PYTORCH_LIGHTNING_DISABLE_MPI'] = '1'
os.environ['PL_DISABLE_FORK'] = '1' 
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['PL_TORCH_DISTRIBUTED_BACKEND'] = 'gloo'
os.environ['MASTER_ADDR'] = 'localhost'
os.environ['MASTER_PORT'] = '29500'
os.environ['NCCL_DISABLE_WARN'] = '1'
os.environ['TORCH_DISTRIBUTED_DETAIL'] = 'OFF'

import sys
import argparse
import yaml
import math
import gc  # ğŸ”¥ ãƒ¡ãƒ¢ãƒªç®¡ç†ç”¨
import numpy as np
import torch
import torch.profiler
import torch.nn as nn
import pytorch_lightning as pl
from typing import Dict, Optional, Tuple, List
from pytorch_lightning.callbacks import Callback

# Tensor Coreæœ€é©åŒ–ï¼ˆPyTorch 2.0+ï¼‰
torch.set_float32_matmul_precision('high')

# TF32ã‚’æœ‰åŠ¹åŒ–ï¼ˆRTX 30xx/40xx/A100ã§é«˜é€ŸåŒ–ï¼‰
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# TorchDynamoæœ€é©åŒ–ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼å‡ºåŠ›ã‚­ãƒ£ãƒ—ãƒãƒ£æœ‰åŠ¹åŒ–ï¼‰
if hasattr(torch, '_dynamo'):
    torch._dynamo.config.capture_scalar_outputs = True
    print("âœ… TorchDynamo ã‚¹ã‚«ãƒ©ãƒ¼å‡ºåŠ›ã‚­ãƒ£ãƒ—ãƒãƒ£æœ‰åŠ¹åŒ–")

print("âœ… TF32ã¨Tensor Coreæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor, TQDMProgressBar
from pytorch_lightning.loggers import TensorBoardLogger
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# WSL/MPIäº’æ›æ€§ã®ãŸã‚ã®ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆimportã®ç›´å¾Œã«å®Ÿè¡Œï¼‰
os.environ['PL_DISABLE_FORK'] = '1'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['PL_TORCH_DISTRIBUTED_BACKEND'] = 'gloo'
# MPIæ¤œå‡ºã‚’å®Œå…¨ç„¡åŠ¹åŒ–
os.environ['PYTORCH_LIGHTNING_DISABLE_MPI'] = '1'
os.environ['SLURM_DISABLED'] = '1'

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’PATHã«è¿½åŠ 
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(current_dir.parent))

from src.data_loader import create_stage1_dataloaders
from src.model import Stage1Model
from src.losses import Stage1CombinedLoss

# T5è»¢ç§»å­¦ç¿’ç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from src.lm_adapter import GradualUnfreezingCallback, create_differential_learning_rate_groups
    T5_CALLBACKS_AVAILABLE = True
except ImportError:
    T5_CALLBACKS_AVAILABLE = False
    GradualUnfreezingCallback = None
    create_differential_learning_rate_groups = None

class CustomProgressBar(TQDMProgressBar):
    """â—† ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ï¼šé‡è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿è¡¨ç¤º"""
    
    def __init__(self, refresh_rate: int = 200):  # ğŸ”¥ 200stepæ¯ã«æ›´æ–°ï¼ˆstdoutå‰Šæ¸›ï¼‰
        super().__init__(refresh_rate=refresh_rate)
    
    def get_metrics(self, trainer, pl_module):
        # æ—¢å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
        metrics = super().get_metrics(trainer, pl_module)
        
        # LightningãŒè‡ªå‹•ã§ä»˜ã‘ã‚‹suffixã¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        rename_map = {
            "train_loss_step": "train_loss",
            "train_loss_epoch": "train_loss_ep",  # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ç”¨ã«åˆ†é›¢
            "val_loss": "val_loss_ep",
            "val_loss_live": "val_loss",  # ãƒ©ã‚¤ãƒ–ç‰ˆã‚’ä¸»è¡¨ç¤º
            "val_correlation": "val_corr_ep",
            "val_corr_live": "val_corr",  # ãƒ©ã‚¤ãƒ–ç‰ˆã‚’ä¸»è¡¨ç¤º
            "lr-AdamW": "lr",
            "grad_norm": "grad_norm",
            "amp_overflow": "amp",  # çŸ­ç¸®è¡¨ç¤º
        }
        
        filtered = {}
        for src_key, dst_key in rename_map.items():
            if src_key in metrics:
                val = metrics[src_key]
                # â˜… Tensor â†’ floatå¤‰æ›
                if isinstance(val, torch.Tensor):
                    val = val.item()
                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ•´å½¢
                if isinstance(val, float):
                    if dst_key in {"train_loss", "train_loss_ep", "val_loss", "val_loss_ep"}:
                        val = f"{val:.4f}"
                    elif dst_key in {"val_corr", "val_corr_ep"}:
                        val = f"{val:+.3f}"
                    elif dst_key == "grad_norm":
                        val = f"{val:.2e}"  # æŒ‡æ•°è¡¨è¨˜ã§æ¡å¹…ã‚’æŠ‘ãˆã‚‹
                    elif dst_key == "lr":
                        val = f"{val:.2e}"  # æŒ‡æ•°è¡¨è¨˜
                    elif dst_key == "amp":
                        val = f"{int(val)}"
                filtered[dst_key] = val
        
        return filtered

class MemoryManagementCallback(Callback):
    """ğŸ”¥ ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ - DataFrameãƒªãƒ¼ã‚¯å¯¾ç­–"""
    
    def __init__(self, gc_every_n_steps: int = 20):
        super().__init__()
        self.gc_every_n_steps = gc_every_n_steps
        
    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        """20ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«GCå®Ÿè¡Œ"""
        if batch_idx % self.gc_every_n_steps == 0:
            # â‘  å¾ªç’°å‚ç…§ã‚’å³å›å
            gc.collect()
            # â‘¡ CUDAã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            if batch_idx % (self.gc_every_n_steps * 5) == 0:  # 100ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ­ã‚°
                print(f"ğŸ—‘ï¸ GC executed at step {batch_idx}")

class Stage1LightningModule(pl.LightningModule):
    """Stage 1 PyTorch Lightning ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"""
    
    def __init__(self, config: dict):
        super().__init__()
        self.config = config
        self.save_hyperparameters(config)
        
        # ãƒ¢ãƒ‡ãƒ«
        self.model = Stage1Model(config)
        
        # æå¤±é–¢æ•°
        self.criterion = Stage1CombinedLoss(config)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.train_losses = []
        self.val_losses = []
        
        print("âš¡ Stage1LightningModuleåˆæœŸåŒ–å®Œäº†")
        print(f"   ãƒ¢ãƒ‡ãƒ«æƒ…å ±: {self.model.get_model_info()}")
        
    def forward(self, features, training_masks=None):
        return self.model(features, training_masks=training_masks)
        
    def training_step(self, batch, batch_idx):
        features = batch['features']
        targets = batch['targets']
        
        # Dictå½¢å¼å¯¾å¿œ: async_samplerãƒ¢ãƒ¼ãƒ‰ã‹ã‚’åˆ¤å®š
        async_sampler = self.config.get('model', {}).get('async_sampler', False)
        
        # T5å‹¾é…ãƒ•ãƒ­ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆT5è»¢ç§»å­¦ç¿’æ™‚ã®ã¿ã€æœ€åˆã®æ•°ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        if batch_idx < 3:
            # T5 encoderã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•ã‚’ä¿®æ­£
            if async_sampler and hasattr(self.model, 'encoders'):
                # async mode: æœ€åˆã®TFã®encoderã‚’ãƒã‚§ãƒƒã‚¯
                first_tf = list(self.model.encoders.keys())[0]
                if hasattr(self.model.encoders[first_tf], 't5_encoder'):
                    t5_encoder = self.model.encoders[first_tf].t5_encoder
                    self._debug_t5_gradients(t5_encoder, batch_idx)
            elif hasattr(self.model, 'shared_encoder') and hasattr(self.model.shared_encoder, 't5_encoder'):
                t5_encoder = self.model.shared_encoder.t5_encoder
                self._debug_t5_gradients(t5_encoder, batch_idx)
        
        # eval_mask_ratioã®æ¸¡ã—æ–¹ã‚’çµ±ä¸€
        eval_mask_ratio = None  # è¨“ç·´æ™‚ã¯None
        
        # Forward passï¼ˆDictå¯¾å¿œï¼‰
        if async_sampler:
            # Model v2: Dictå½¢å¼
            outputs = self.model(features, eval_mask_ratio=eval_mask_ratio)
            
            # M1ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆã‚¯ãƒ­ã‚¹æå¤±ç”¨ï¼‰
            # ğŸ”¥ CRITICAL FIX: targetsâ†’featuresã‹ã‚‰æ­£ã—ãm1ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            m1_data = features.get('m1') if isinstance(features, dict) else None
            
            # æå¤±è¨ˆç®—ï¼ˆDictç‰ˆï¼‰
            losses = self.criterion(outputs, targets, masks=None, m1_data={'m1': m1_data} if m1_data is not None else None)
        else:
            # Legacy: tensorå½¢å¼
            outputs = self.model(features, eval_mask_ratio=eval_mask_ratio)
            reconstructed = outputs  # æ—§å½¢å¼ã§ã¯ç›´æ¥ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã™
            
            # M1ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆã‚¯ãƒ­ã‚¹æå¤±ç”¨ï¼‰
            m1_data = targets[:, 0]  # [batch, seq_len, 4]
            
            # æå¤±è¨ˆç®—ï¼ˆtensorç‰ˆï¼‰
            losses = self.criterion(reconstructed, targets, masks=None, m1_data=m1_data)
        
        # â—† å­¦ç¿’æå¤±ã‚’æ¯ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ­ã‚°ï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«è¡¨ç¤ºï¼‰
        loss = losses['total']
        self.log("train_loss_step", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        
        # ğŸ”¥ å­¦ç¿’ç‡ã¯200stepæ¯ã®ã¿ãƒ­ã‚°ï¼ˆstdoutå‰Šæ¸›ï¼‰
        if hasattr(self.trainer, 'global_step') and self.trainer.global_step % 200 == 0:
            current_lr = self.trainer.optimizers[0].param_groups[0]['lr']
            self.log("lr-AdamW", current_lr, on_step=True, prog_bar=True, logger=True)
        
        # è©³ç´°æå¤±ã‚‚ãƒ­ã‚°ï¼ˆã‚¨ãƒãƒƒã‚¯å˜ä½ã®ã¿ï¼‰
        for loss_name, loss_value in losses.items():
            if loss_name != 'total':  # totalã¯ä¸Šè¨˜ã§æ—¢ã«ãƒ­ã‚°æ¸ˆã¿
                self.log(f'train_{loss_name}', loss_value, 
                        on_step=False, on_epoch=True, prog_bar=False, logger=True)
            
        return loss
        
    def _debug_t5_gradients(self, t5_encoder, batch_idx):
        """T5å‹¾é…ãƒ•ãƒ­ãƒ¼ã®ãƒ‡ãƒãƒƒã‚°ï¼ˆ200stepæ¯ã®ã¿ï¼‰"""
        if batch_idx % 200 != 0:  # ğŸ”¥ 200stepæ¯ã®ã¿å®Ÿè¡Œ
            return
            
        try:
            # T5EncoderModelã®æ­£ã—ã„æ§‹é€ ã‚’ä½¿ç”¨
            sample_param = t5_encoder.encoder.block[0].layer[0].SelfAttention.q.weight
            print(f"   [T5 DBG] Step {batch_idx}: requires_grad={sample_param.requires_grad}")
            if sample_param.grad is not None:
                grad_norm = sample_param.grad.abs().sum().item()
                print(f"   [T5 DBG] Step {batch_idx}: grad_norm={grad_norm:.6f}")
            else:
                print(f"   [T5 DBG] Step {batch_idx}: grad=None")
        except (AttributeError, IndexError) as e:
            print(f"   [T5 DBG] Step {batch_idx}: T5æ§‹é€ ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼ - {e}")
    
    def on_train_epoch_start(self):
        """ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚ã®åˆæœŸåŒ–"""
        self.overflow_count = 0
        self._amp_scale_start = None
        # BF16ã§ã¯ scaler ãŒ None ã®ãŸã‚ã€å®‰å…¨ã«ã‚¢ã‚¯ã‚»ã‚¹
        if (hasattr(self.trainer, 'precision_plugin') and 
            hasattr(self.trainer.precision_plugin, 'scaler') and 
            self.trainer.precision_plugin.scaler is not None):
            self._amp_scale_start = self.trainer.precision_plugin.scaler.get_scale()
    
    def on_after_backward(self) -> None:
        """å‹¾é…ãƒãƒ«ãƒ ç›£è¦– & AMPã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼æ¤œçŸ¥"""
        
        # ---- 1) grad_norm è¨ˆæ¸¬ & ãƒ­ã‚° ----
        grad_norms = [p.grad.detach().float().norm() 
                      for p in self.parameters() if p.grad is not None]
        if grad_norms:
            grad_norm = torch.linalg.vector_norm(torch.stack(grad_norms), ord=2)
        else:
            grad_norm = torch.tensor(0.0)
            
        # inf/nanã‚’1e3ã«ä¸¸ã‚ã¦å¯è¦–åŒ–ã ã‘ã¯ç¶šè¡Œ
        if not torch.isfinite(grad_norm):
            grad_norm = torch.tensor(1e3, device=self.device)
            
        # ğŸ”¥ grad_norm ã¯200stepæ¯ã®ã¿ãƒ­ã‚°ï¼ˆstdoutå‰Šæ¸›ï¼‰
        if hasattr(self.trainer, 'global_step') and self.trainer.global_step % 200 == 0:
            self.log("grad_norm", grad_norm,
                     on_step=True, on_epoch=False, prog_bar=True, logger=True)
        
        # ---- 2) AMPã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼æ¤œçŸ¥ï¼ˆGradScalerã‹ã‚‰æ­£ç¢ºã«å–å¾—ï¼‰----
        overflow = 0.0
        if (hasattr(self.trainer, 'precision_plugin') and 
            hasattr(self.trainer.precision_plugin, 'scaler') and 
            self.trainer.precision_plugin.scaler is not None):
            scaler = self.trainer.precision_plugin.scaler
            # ã‚¹ã‚±ãƒ¼ãƒ«ãŒ0ãªã‚‰ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ç™ºç”Ÿ
            if hasattr(scaler, '_scale') and scaler._scale is not None:
                overflow = 1.0 if scaler._scale.item() == 0 else 0.0
                
        # ğŸ”¥ amp_overflow ã¯200stepæ¯ã®ã¿ãƒ­ã‚°ï¼ˆstdoutå‰Šæ¸›ï¼‰
        if hasattr(self.trainer, 'global_step') and self.trainer.global_step % 200 == 0:
            self.log("amp_overflow", overflow,
                     on_step=True, on_epoch=False, prog_bar=True, logger=True)
        
        # ---- 3) å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–ã‚‚Lightningã«ä»»ã›ã‚‹ï¼‰----
        # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å‰å¾Œã®å€¤ã‚’è¨˜éŒ²
        grad_norm_before = grad_norm if torch.isfinite(grad_norm) else torch.tensor(1e3)
        grad_norm_after = torch.nn.utils.clip_grad_norm_(self.parameters(), 
                                                         max_norm=self.config['training']['gradient_clip'])
        # ã‚¯ãƒªãƒƒãƒ—ã•ã‚ŒãŸã‹ã‚’è¨˜éŒ²
        if grad_norm_before > self.config['training']['gradient_clip']:
            clipped_ratio = grad_norm_after / grad_norm_before
            self.log("grad_norm_clipped", clipped_ratio,
                     on_step=True, on_epoch=False, prog_bar=False, logger=True)
    
    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure=None):
        """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—"""
        # AMP/GradScalerã®å†…éƒ¨ãƒ­ã‚¸ãƒƒã‚¯ã‚’å£Šã•ãªã„ã‚ˆã†Lightningã«ä»»ã›ã‚‹
        return super().optimizer_step(epoch, batch_idx, optimizer, optimizer_closure)
        
    def validation_step(self, batch, batch_idx):
        features = batch['features']
        targets = batch['targets']
        
        # Dictå½¢å¼å¯¾å¿œ: async_samplerãƒ¢ãƒ¼ãƒ‰ã‹ã‚’åˆ¤å®š
        async_sampler = self.config.get('model', {}).get('async_sampler', False)
        
        # eval_mask_ratioã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ãƒã‚§ãƒƒã‚¯
        eval_mask_ratio = self.config.get('evaluation', {}).get('eval_mask_ratio')
        
        # Forward passï¼ˆDictå¯¾å¿œï¼‰
        if async_sampler:
            # Model v2: Dictå½¢å¼
            outputs = self.model(features, eval_mask_ratio=eval_mask_ratio)
            
            # M1ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆã‚¯ãƒ­ã‚¹æå¤±ç”¨ï¼‰
            # ğŸ”¥ CRITICAL FIX: targetsâ†’featuresã‹ã‚‰æ­£ã—ãm1ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            m1_data = features.get('m1') if isinstance(features, dict) else None
            
            # æå¤±è¨ˆç®—ï¼ˆDictç‰ˆï¼‰
            losses = self.criterion(outputs, targets, masks=None, m1_data={'m1': m1_data} if m1_data is not None else None)
        else:
            # Legacy: tensorå½¢å¼
            if eval_mask_ratio is not None:
                # ğŸ”¥ eval_mask_ratioæŒ‡å®šæ™‚ï¼šã‚«ã‚¹ã‚¿ãƒ ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
                batch_size, n_tf, seq_len, n_features = features.shape
                training_masks = torch.stack([
                    self.model.masking_strategy.generate_masks(
                        features[b], 
                        seed=batch_idx * batch_size + b, 
                        eval_mask_ratio_override=eval_mask_ratio
                    )
                    for b in range(batch_size)
                ], dim=0)  # [batch, n_tf, seq_len]
            else:
                training_masks = None
            
            # Forward passï¼ˆLegacy APIï¼‰
            outputs = self.model(features, eval_mask_ratio=eval_mask_ratio)
            reconstructed = outputs  # æ—§å½¢å¼ã§ã¯ç›´æ¥ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã™
            
            # M1ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            m1_data = targets[:, 0]
            
            # æå¤±è¨ˆç®—ï¼ˆtensorç‰ˆï¼‰
            losses = self.criterion(reconstructed, targets, masks=training_masks, m1_data=m1_data)
        
        # â—† æ¤œè¨¼æå¤±ã‚’ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«è¡¨ç¤ºï¼ˆã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ï¼‰
        self.log("val_loss", losses['total'],
                 on_epoch=True, prog_bar=True, logger=True)
        
        # â—† ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰ˆï¼ˆæ¤œè¨¼ä¸­ã«å³åº§ã«è¡¨ç¤ºï¼‰
        self.log("val_loss_live", losses['total'],
                 on_step=True, on_epoch=False, prog_bar=True, logger=False)
        
        # è©³ç´°æå¤±ï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«ã¯è¡¨ç¤ºã—ãªã„ï¼‰
        for loss_name, loss_value in losses.items():
            if loss_name != 'total':  # totalã¯ä¸Šè¨˜ã§æ—¢ã«ãƒ­ã‚°æ¸ˆã¿
                self.log(f'val_{loss_name}', loss_value, 
                        on_epoch=True, prog_bar=False, logger=True)
            
        # ğŸ”¥ ç›¸é–¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆDictå¯¾å¿œï¼‰
        if async_sampler:
            correlations = self._calculate_correlations_dict(outputs, targets)
            timeframes = self.config['data']['timeframes']
            for tf_idx, tf_name in enumerate(timeframes):
                if tf_name in correlations:
                    # ğŸ”¥ per-TFãƒ¡ãƒˆãƒªã‚¯ã‚¹å¸¸æ™‚å¯è¦–åŒ–ï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤ºï¼‰
                    self.log(f'val_corr_{tf_name}', correlations[tf_name], on_epoch=True, prog_bar=True, logger=True)
            
            # å¹³å‡ç›¸é–¢
            corr_values = [correlations[tf] for tf in timeframes if tf in correlations]
            if corr_values:
                mean_corr = torch.mean(torch.stack(corr_values))
            else:
                mean_corr = torch.tensor(0.0)
        else:
            # Legacyç›¸é–¢è¨ˆç®—
            correlations = self._calculate_correlations(outputs, targets, training_masks)
            for tf_idx, corr in enumerate(correlations):
                tf_name = self.config['data']['timeframes'][tf_idx]
                self.log(f'val_corr_{tf_name}', corr, on_epoch=True, prog_bar=False, logger=True)
            
            # å¹³å‡ç›¸é–¢
            mean_corr = torch.mean(torch.stack(correlations))
            
        # â—† å¹³å‡ç›¸é–¢ã‚’ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«è¡¨ç¤ºï¼ˆã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ï¼‰
        self.log('val_correlation', mean_corr, on_epoch=True, prog_bar=True, logger=True)
        self.log('val_correlation_mean', mean_corr, on_epoch=True, prog_bar=False, logger=True)  # å¾Œæ–¹äº’æ›æ€§
        
        # â—† ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰ˆï¼ˆæ¤œè¨¼ä¸­ã«å³åº§ã«è¡¨ç¤ºï¼‰
        self.log("val_corr_live", mean_corr,
                 on_step=True, on_epoch=False, prog_bar=True, logger=False)
        
        return losses['total']
    
    def test_step(self, batch, batch_idx):
        """ãƒ†ã‚¹ãƒˆã‚¹ãƒ†ãƒƒãƒ—ï¼ˆè©•ä¾¡ç”¨ï¼‰"""
        features = batch['features']  # ç”Ÿã®ç‰¹å¾´é‡
        targets = batch['targets']
        
        # eval_mask_ratioã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ãƒã‚§ãƒƒã‚¯
        eval_mask_ratio = self.config.get('evaluation', {}).get('eval_mask_ratio')
        training_masks = None
        
        if eval_mask_ratio is not None:
            # ğŸ”¥ eval_mask_ratioæŒ‡å®šæ™‚ï¼šã‚«ã‚¹ã‚¿ãƒ ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
            batch_size, n_tf, seq_len, n_features = features.shape
            training_masks = torch.stack([
                self.model.masking_strategy.generate_masks(
                    features[b], 
                    seed=batch_idx * batch_size + b, 
                    eval_mask_ratio_override=eval_mask_ratio
                )
                for b in range(batch_size)
            ], dim=0)  # [batch, n_tf, seq_len]
        
        # Forward passï¼ˆæ–°ã—ã„APIï¼‰
        outputs = self.model(features, training_masks=training_masks)
        reconstructed = outputs['reconstructed']
        actual_training_masks = outputs['training_masks']  # å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒã‚¹ã‚¯
        
        # M1ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        m1_data = targets[:, 0]
        
        # æå¤±è¨ˆç®—ï¼ˆæ–°ã—ã„ãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ï¼‰
        losses = self.criterion(reconstructed, targets, actual_training_masks, m1_data)
        
        # ãƒ†ã‚¹ãƒˆæå¤±ã‚’ãƒ­ã‚°
        self.log("test_loss", losses['total'], on_epoch=True, prog_bar=True, logger=True)
        
        # è©³ç´°æå¤±
        for loss_name, loss_value in losses.items():
            if loss_name != 'total':
                self.log(f'test_{loss_name}', loss_value, 
                        on_epoch=True, prog_bar=False, logger=True)
        
        # ğŸ”¥ ç›¸é–¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆãƒã‚¹ã‚¯ä½ç½®ã®ã¿ï¼‰
        correlations = self._calculate_correlations(reconstructed, targets, actual_training_masks)
        for tf_idx, corr in enumerate(correlations):
            tf_name = self.config['data']['timeframes'][tf_idx]
            self.log(f'test_corr_{tf_name}', corr, on_epoch=True, prog_bar=False, logger=True)
            
        # å¹³å‡ç›¸é–¢
        mean_corr = torch.mean(torch.stack(correlations))
        self.log('test_correlation_mean', mean_corr, on_epoch=True, prog_bar=True, logger=True)
        
        return losses['total']
        
    def _calculate_correlations(self, pred, target, training_masks):
        """TFã”ã¨ã®ç›¸é–¢ã‚’è¨ˆç®—ï¼ˆãƒã‚¹ã‚¯ä½ç½®ã®ã¿ï¼‰"""
        correlations = []
        
        for tf_idx in range(pred.size(1)):
            pred_tf = pred[:, tf_idx]  # [batch, seq_len, 4]
            target_tf = target[:, tf_idx]
            mask_tf = training_masks[:, tf_idx]  # [batch, seq_len]
            
            # ãƒã‚¹ã‚¯ã•ã‚ŒãŸéƒ¨åˆ†ã®ã¿ã§ç›¸é–¢è¨ˆç®—
            if mask_tf.sum() > 0:
                pred_masked = pred_tf[mask_tf]  # [n_masked, 4] - æ—¢ã«boolå‹
                target_masked = target_tf[mask_tf]
                
                if pred_masked.numel() > 0:
                    # ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ï¼ˆ4ã¤ã®OHLCç‰¹å¾´é‡ã®å¹³å‡ï¼‰
                    corr_ohlc = []
                    for feat_idx in range(4):
                        pred_feat = pred_masked[:, feat_idx]
                        target_feat = target_masked[:, feat_idx]
                        
                        if pred_feat.numel() > 1:
                            corr = torch.corrcoef(torch.stack([pred_feat, target_feat]))[0, 1]
                            if not torch.isnan(corr):
                                corr_ohlc.append(corr)
                                
                    if corr_ohlc:
                        mean_corr = torch.mean(torch.stack(corr_ohlc))
                        correlations.append(mean_corr)
                    else:
                        correlations.append(torch.tensor(0.0, device=pred.device))
                else:
                    correlations.append(torch.tensor(0.0, device=pred.device))
            else:
                correlations.append(torch.tensor(0.0, device=pred.device))
                
        return correlations
    
    def _calculate_correlations_dict(self, pred: Dict[str, torch.Tensor], target: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """Dictå½¢å¼ã®TFã”ã¨ç›¸é–¢è¨ˆç®—ï¼ˆModel v2ç”¨ï¼‰"""
        correlations = {}
        
        for tf_name, pred_tf in pred.items():
            if tf_name not in target:
                continue
                
            target_tf = target[tf_name]
            
            # NaNå€¤ã‚’é™¤å¤–ï¼ˆpaddingå¯¾å¿œï¼‰
            valid_mask = ~torch.isnan(pred_tf[..., 0])  # [batch, seq_len]
            
            if valid_mask.sum() > 0:
                # æœ‰åŠ¹ãªä½ç½®ã®ã¿ã§ç›¸é–¢è¨ˆç®—
                pred_valid = pred_tf[valid_mask]  # [valid_positions, 4]
                target_valid = target_tf[valid_mask]  # [valid_positions, 4]
                
                if pred_valid.numel() > 0:
                    # ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ï¼ˆ4ã¤ã®OHLCç‰¹å¾´é‡ã®å¹³å‡ï¼‰
                    corr_ohlc = []
                    for feat_idx in range(4):
                        pred_feat = pred_valid[:, feat_idx]
                        target_feat = target_valid[:, feat_idx]
                        
                        if pred_feat.numel() > 1:
                            try:
                                corr = torch.corrcoef(torch.stack([pred_feat, target_feat]))[0, 1]
                                if not torch.isnan(corr):
                                    corr_ohlc.append(corr)
                            except RuntimeError:
                                # corrcoefè¨ˆç®—å¤±æ•—æ™‚ã¯0ã¨ã—ã¦æ‰±ã†
                                corr_ohlc.append(torch.tensor(0.0, device=pred_tf.device))
                                
                    if corr_ohlc:
                        mean_corr = torch.mean(torch.stack(corr_ohlc))
                        correlations[tf_name] = mean_corr
                    else:
                        correlations[tf_name] = torch.tensor(0.0, device=pred_tf.device)
                else:
                    correlations[tf_name] = torch.tensor(0.0, device=pred_tf.device)
            else:
                correlations[tf_name] = torch.tensor(0.0, device=pred_tf.device)
                
        return correlations
    
    def on_validation_epoch_end(self):
        """æ¤œè¨¼ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ï¼šval_corrã®å¹³å‡ã‚’ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«è¡¨ç¤º"""
        # æ—¢ã«ãƒ­ã‚°ã•ã‚ŒãŸval_correlation_meanã‚’ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«ã‚‚è¡¨ç¤º
        if 'val_correlation_mean' in self.trainer.callback_metrics:
            val_corr_mean = self.trainer.callback_metrics['val_correlation_mean']
            self.log('val_corr_mean', val_corr_mean, 
                     on_step=False, on_epoch=True, prog_bar=True, logger=False)
    
    def on_train_epoch_end(self):
        """ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®AMPã‚¹ã‚±ãƒ¼ãƒ«å¤‰åŒ–ãƒ­ã‚°"""
        if (hasattr(self.trainer, 'precision_plugin') and 
            hasattr(self.trainer.precision_plugin, 'scaler') and 
            self.trainer.precision_plugin.scaler is not None):
            current_scale = self.trainer.precision_plugin.scaler.get_scale()
            if self._amp_scale_start is not None:
                scale_change = current_scale / self._amp_scale_start
                self.log('amp_scale_change', scale_change, on_epoch=True, prog_bar=False, logger=True)
                # ã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§ããä¸‹ãŒã£ãŸå ´åˆã®ã¿è­¦å‘Š
                if scale_change < 0.5:
                    print(f"âš ï¸ AMPã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§å¹…ã«æ¸›å°‘: {self._amp_scale_start:.1f} â†’ {current_scale:.1f}")
    
    def configure_optimizers(self):
        """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šï¼ˆT5 Layerwise LR Decayå¯¾å¿œï¼‰"""
        
        # åŸºæº–å­¦ç¿’ç‡ã®å–å¾—
        base_lr = self.config['training']['optimizer'].get('lr', self.config['training']['scheduler']['max_lr'])
        
        # T5è»¢ç§»å­¦ç¿’ãŒæœ‰åŠ¹ãªå ´åˆã¯å·®åˆ†å­¦ç¿’ç‡ã‚’é©ç”¨
        if (T5_CALLBACKS_AVAILABLE and 
            self.config.get('transfer_learning', {}).get('use_pretrained_lm', False)):
            
            # Layerwise LR Decayè¨­å®šã®å–å¾—
            layerwise_lr_decay = self.config['training'].get('layerwise_lr_decay')
            t5_lr_top = self.config['training'].get('t5_lr_top')
            t5_lr_factor = self.config.get('transfer_learning', {}).get('lm_learning_rate_factor', 0.1)
            
            param_groups = create_differential_learning_rate_groups(
                self.model, 
                base_lr=base_lr, 
                t5_lr_factor=t5_lr_factor,
                layerwise_lr_decay=layerwise_lr_decay,
                t5_lr_top=t5_lr_top
            )
            
            # ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³å­¦ç¿’ç‡ã‚¹ã‚±ãƒ¼ãƒ«å¯¾å¿œï¼ˆT5è»¢ç§»å­¦ç¿’æ™‚ï¼‰
            mask_token_lr_scale = self.config.get('masking', {}).get('mask_token_lr_scale', 1.0)
            if mask_token_lr_scale != 1.0:
                # æ—¢å­˜ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ†é›¢
                mask_token_params = []
                for group in param_groups:
                    remaining_params = []
                    for param in group['params']:
                        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã«é€†å¼•ã
                        for name, model_param in self.model.named_parameters():
                            if param is model_param and 'masking_strategy.mask_token' in name:
                                mask_token_params.append(param)
                                break
                        else:
                            remaining_params.append(param)
                    group['params'] = remaining_params
                
                # ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³å°‚ç”¨ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¿½åŠ 
                if mask_token_params:
                    param_groups.append({
                        'params': mask_token_params,
                        'lr': base_lr * mask_token_lr_scale,
                        'name': 'mask_token'
                    })
                    print(f"ğŸ­ T5+ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³: mask_token_lr={base_lr * mask_token_lr_scale:.2e} (scale={mask_token_lr_scale})")
            
            optimizer = torch.optim.AdamW(
                param_groups,
                betas=self.config['training']['optimizer']['betas'],
                weight_decay=self.config['training']['optimizer']['weight_decay']
            )
            
            # è¨­å®šæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
            if layerwise_lr_decay is not None and t5_lr_top is not None:
                print(f"ğŸ”§ Layerwise LR Decay: base_lr={base_lr:.2e}, t5_top_lr={t5_lr_top:.2e}, decay={layerwise_lr_decay}")
            else:
                print(f"ğŸ¤— T5å·®åˆ†å­¦ç¿’ç‡: base_lr={base_lr:.2e}, t5_lr={base_lr*t5_lr_factor:.2e}")
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
            for i, group in enumerate(param_groups):
                print(f"  ParamGroup[{i}] ({group.get('name', 'unknown')}): lr={group['lr']:.2e}")
            
        else:
            # å¾“æ¥ã®å˜ä¸€å­¦ç¿’ç‡ + ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³å­¦ç¿’ç‡ã‚¹ã‚±ãƒ¼ãƒ«å¯¾å¿œ
            mask_token_lr_scale = self.config.get('masking', {}).get('mask_token_lr_scale', 1.0)
            
            if mask_token_lr_scale != 1.0:
                # ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³å°‚ç”¨ã®å­¦ç¿’ç‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
                mask_token_params = []
                other_params = []
                
                for name, param in self.model.named_parameters():
                    if 'masking_strategy.mask_token' in name:
                        mask_token_params.append(param)
                    else:
                        other_params.append(param)
                
                param_groups = [
                    {
                        'params': other_params,
                        'lr': base_lr,
                        'name': 'main_params'
                    },
                    {
                        'params': mask_token_params,
                        'lr': base_lr * mask_token_lr_scale,
                        'name': 'mask_token'
                    }
                ]
                
                optimizer = torch.optim.AdamW(
                    param_groups,
                    betas=self.config['training']['optimizer']['betas'],
                    weight_decay=self.config['training']['optimizer']['weight_decay']
                )
                
                print(f"ğŸ“ ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³å­¦ç¿’ç‡ã‚¹ã‚±ãƒ¼ãƒ«: scale={mask_token_lr_scale}")
                print(f"  - ãƒ¡ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: lr={base_lr:.2e}")
                print(f"  - ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³: lr={base_lr * mask_token_lr_scale:.2e}")
            else:
                # é€šå¸¸ã®å˜ä¸€å­¦ç¿’ç‡
                optimizer = torch.optim.AdamW(
                    self.parameters(),
                    lr=base_lr,
                    betas=self.config['training']['optimizer']['betas'],
                    weight_decay=self.config['training']['optimizer']['weight_decay']
                )
                print(f"ğŸ“ å˜ä¸€å­¦ç¿’ç‡: lr={base_lr:.2e}")
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        scheduler_config = self.config['training']['scheduler']
        total_steps = self.trainer.estimated_stepping_batches
        
        # Linear Warmup + Cosine Decay ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®å®Ÿè£…
        if scheduler_config['name'].lower() == 'linear_with_warmup':
            # Warmupè¨­å®šã®è¨ˆç®—
            warmup_epochs = self.config['training'].get('warmup_epochs', 3)
            steps_per_epoch = total_steps // self.config['training']['epochs']
            num_warmup_steps = steps_per_epoch * warmup_epochs  # 970 * 3 = 2910
            
            print(f"ğŸ“ Linear Warmup ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š:")
            print(f"  - Warmup steps: {num_warmup_steps} ({warmup_epochs} epochs)")
            print(f"  - Total steps: {total_steps}")
            print(f"  - Steps per epoch: {steps_per_epoch}")
            
            # PyTorchã®æ¨™æº–çš„ãªLinear Warmup + Cosine Decayã®å®Ÿè£…
            from torch.optim.lr_scheduler import LambdaLR
            
            def lr_lambda(current_step: int):
                if current_step < num_warmup_steps:
                    # Linear warmup
                    return float(current_step) / float(max(1, num_warmup_steps))
                else:
                    # Cosine decay
                    progress = float(current_step - num_warmup_steps) / float(max(1, total_steps - num_warmup_steps))
                    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))
            
            scheduler = LambdaLR(optimizer, lr_lambda)
        else:
            # å¾“æ¥ã®OneCycleLR
            scheduler = torch.optim.lr_scheduler.OneCycleLR(
                optimizer,
                max_lr=scheduler_config['max_lr'],
                total_steps=total_steps,
                div_factor=scheduler_config['div_factor'],
                final_div_factor=scheduler_config['final_div_factor'],
                pct_start=scheduler_config['pct_start']
            )
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šå–å¾—
        interval = scheduler_config.get('interval', 'epoch')
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'interval': interval
            }
        }

def load_config(config_path: str) -> dict:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆç¶™æ‰¿å¯¾å¿œï¼‰"""
    config_path = Path(config_path)
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # extendsæŒ‡å®šãŒã‚ã‚‹å ´åˆã¯è¦ªè¨­å®šã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒ¼ã‚¸
    if 'extends' in config:
        parent_path = config_path.parent / config['extends']
        parent_config = load_config(str(parent_path))
        # è¦ªè¨­å®šã‚’ãƒ™ãƒ¼ã‚¹ã«ç¾åœ¨ã®è¨­å®šã§ä¸Šæ›¸ã
        merged_config = deep_merge(parent_config, config)
        # extendsè‡ªä½“ã¯æœ€çµ‚è¨­å®šã‹ã‚‰å‰Šé™¤
        merged_config.pop('extends', None)
        return merged_config
    
    return config

def deep_merge(base: dict, override: dict) -> dict:
    """è¨­å®šã‚’å†å¸°çš„ã«ãƒãƒ¼ã‚¸"""
    result = base.copy()
    for key, value in override.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge(result[key], value)
        else:
            result[key] = value
    return result

def main():
    # PyTorchåˆ†æ•£å­¦ç¿’ã‚’å®Œå…¨ç„¡åŠ¹åŒ–ã—ã¦WSLäº’æ›æ€§ã‚’å‘ä¸Š
    os.environ['PL_DISABLE_FORK'] = '1'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    os.environ['PL_TORCH_DISTRIBUTED_BACKEND'] = 'gloo'
    os.environ['WORLD_SIZE'] = '1'
    os.environ['RANK'] = '0'
    os.environ['LOCAL_RANK'] = '0'
    os.environ['PYTORCH_LIGHTNING_DISABLE_MPI'] = '1'
    os.environ['SLURM_DISABLED'] = '1'
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    os.environ['NCCL_DISABLE_WARN'] = '1'
    os.environ['TORCH_DISTRIBUTED_DETAIL'] = 'OFF'
    
    parser = argparse.ArgumentParser(description='Stage 1 Training')
    parser.add_argument('--config', type=str, required=True, help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--data_dir', type=str, required=True, help='ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--devices', type=int, default=1, help='ãƒ‡ãƒã‚¤ã‚¹æ•°')
    parser.add_argument('--resume_from', type=str, default=None, help='ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹')
    parser.add_argument('--fast_dev_run', action='store_true', help='é–‹ç™ºç”¨é«˜é€Ÿå®Ÿè¡Œï¼ˆ1ãƒãƒƒãƒã®ã¿ï¼‰')
    parser.add_argument('--max_epochs', type=int, default=None, help='æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°ä¸Šæ›¸ã')
    parser.add_argument('--profiler', type=str, default=None, help='ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ï¼ˆsimple, advancedï¼‰')
    parser.add_argument('--dry_run', action='store_true', help='ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆãƒ‡ãƒ¼ã‚¿ç¢ºèªã®ã¿ï¼‰')
    parser.add_argument('--plot_lr', action='store_true', help='å­¦ç¿’ç‡ã‚«ãƒ¼ãƒ–ã‚’ãƒ—ãƒ­ãƒƒãƒˆ')
    parser.add_argument('--batch_size', type=int, default=None, help='ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸Šæ›¸ã')
    parser.add_argument('--gradient_clip_val', type=float, default=None, help='å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å€¤ä¸Šæ›¸ã')
    parser.add_argument('--check_early_stop', action='store_true', help='æ—©æœŸåœæ­¢å‹•ä½œãƒ†ã‚¹ãƒˆ')
    parser.add_argument('--val_gap_days', type=float, default=None, help='è¨“ç·´ã¨æ¤œè¨¼ã®é–“ã®æ™‚é–“çš„ã‚®ãƒ£ãƒƒãƒ—ï¼ˆæ—¥æ•°ï¼‰')
    parser.add_argument('--eval_mask_ratio', type=float, default=None, help='è©•ä¾¡æ™‚ã®ãƒã‚¹ã‚¯ç‡ (0=ãƒã‚¹ã‚¯ãªã—, 1=å…¨ãƒã‚¹ã‚¯)')
    parser.add_argument('--mask_token_lr_scale', type=float, default=None, help='ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã®å­¦ç¿’ç‡ã‚¹ã‚±ãƒ¼ãƒ« (ä¾‹: 0.1)')
    parser.add_argument('--async_sampler', action='store_true', help='éåŒæœŸãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ– (Model v2)')
    parser.add_argument('--seeds', type=int, nargs='+', default=None, help='è¤‡æ•°ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œ (ä¾‹: --seeds 42 123 2025)')
    parser.add_argument('--profile_mode', action='store_true', help='ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰ (100ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿å®Ÿè¡Œ)')
    
    args = parser.parse_args()
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config(args.config)
    config['data']['data_dir'] = args.data_dir
    
    # å¼•æ•°ã§ã®è¨­å®šä¸Šæ›¸ã
    if args.max_epochs:
        config['training']['epochs'] = args.max_epochs
    if args.batch_size:
        config['training']['batch_size'] = args.batch_size
    if args.gradient_clip_val:
        config['training']['gradient_clip'] = args.gradient_clip_val
    if args.val_gap_days:
        config['validation']['val_gap_days'] = args.val_gap_days
    if args.eval_mask_ratio is not None:
        config['evaluation']['eval_mask_ratio'] = args.eval_mask_ratio
    if args.mask_token_lr_scale is not None:
        if 'masking' not in config:
            config['masking'] = {}
        config['masking']['mask_token_lr_scale'] = args.mask_token_lr_scale
    if args.async_sampler:
        if 'model' not in config:
            config['model'] = {}
        config['model']['async_sampler'] = True
        print("ğŸ”„ éåŒæœŸãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ¼ãƒ‰ (Model v2) æœ‰åŠ¹åŒ–")
    
    print("ğŸš€ Stage 1 è¨“ç·´é–‹å§‹")
    print(f"   è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {args.config}")
    print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.data_dir}")
    print(f"   ãƒ‡ãƒã‚¤ã‚¹æ•°: {args.devices}")
    
    # ã‚·ãƒ¼ãƒ‰è¨­å®š
    pl.seed_everything(config['runtime']['seed'])
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆä¸­...")
    train_loader, val_loader = create_stage1_dataloaders(args.data_dir, config)
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print("ğŸ§  ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    model = Stage1LightningModule(config)
    
    # PyTorch 2.0 ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–ï¼ˆäº‹å‰ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä»˜ãï¼‰
    if False:  # torch.__version__ >= '2.0.0':
        print("ğŸš€ PyTorch 2.0 ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
        try:
            # GPUã«ç§»å‹•
            if torch.cuda.is_available():
                model = model.cuda()
            
            # 1) ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç”¨ã«FP32ã«çµ±ä¸€
            print("ğŸ”§ ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç”¨ã«FP32ã«çµ±ä¸€...")
            model.model = model.model.to(torch.float32)
            
            # 2) äº‹å‰ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ã‚’éš è”½ï¼ˆFP32ã§ï¼‰
            print("ğŸ”¥ ãƒ€ãƒŸãƒ¼å…¥åŠ›ã§ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...")
            with torch.no_grad():
                # ãƒãƒƒãƒã‚µã‚¤ã‚º1ã§ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆï¼ˆFP32ï¼‰
                dummy_features = torch.randn(1, 6, 128, 6, device=model.device, dtype=torch.float32)
                dummy_masks = torch.zeros(1, 6, 128, device=model.device, dtype=torch.bool)
                
                # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
                _ = model.model(dummy_features, dummy_masks)
                print("âœ… ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
            
            # 3) ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«é©ç”¨ï¼ˆFP32ãƒ¢ãƒ‡ãƒ«ã§ï¼‰
            model.model = torch.compile(model.model, backend="inductor", mode="max-autotune")
            print("âœ… TorchCompileé©ç”¨å®Œäº†ï¼ˆäº‹å‰ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¸ˆã¿ï¼‰")
            
        except Exception as e:
            print(f"âš ï¸ TorchCompileå¤±æ•—ã€é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: {e}")
    else:
        print("âš ï¸ PyTorch 2.0+ãŒå¿…è¦ã§ã™ï¼ˆTorchCompileã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    
    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
    callbacks = []
    
    # ğŸ”¥ ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆDataFrameãƒªãƒ¼ã‚¯å¯¾ç­–ï¼‰
    memory_callback = MemoryManagementCallback(gc_every_n_steps=20)
    callbacks.append(memory_callback)
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
    checkpoint_callback = ModelCheckpoint(
        dirpath=Path(args.config).parent.parent / 'checkpoints',
        filename='stage1-{epoch:02d}-{val_correlation_mean:.4f}',
        monitor='val_correlation_mean',
        mode='max',
        save_top_k=config['logging']['save_top_k'],
        save_last=True
    )
    callbacks.append(checkpoint_callback)
    
    # æ—©æœŸåœæ­¢
    early_stopping = EarlyStopping(
        monitor='val_correlation_mean',
        mode='max',
        patience=config['training']['early_stop']['patience'],
        min_delta=config['training']['early_stop']['min_delta']
    )
    callbacks.append(early_stopping)
    
    # å­¦ç¿’ç‡ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
    lr_monitor = LearningRateMonitor(logging_interval='step')
    callbacks.append(lr_monitor)
    
    # â—† ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ï¼ˆ200ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°ãƒ»stdoutå‰Šæ¸›ï¼‰
    custom_progress = CustomProgressBar(refresh_rate=200)  # ğŸ”¥ stdoutå‰Šæ¸›
    callbacks.append(custom_progress)
    
    # ğŸ”¥ T5è»¢ç§»å­¦ç¿’ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯å»ƒæ­¢ï¼ˆT5ã¯å¸¸ã«è§£å‡çŠ¶æ…‹ï¼‰
    unfreezing_callback = None  # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–
    if T5_CALLBACKS_AVAILABLE and config.get('transfer_learning', {}).get('use_pretrained_lm', False):
        print("ğŸ”“ T5ã¯å¸¸ã«è§£å‡çŠ¶æ…‹ã®ãŸã‚ã€è§£å‡ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ã‚¹ã‚­ãƒƒãƒ—")
    
    # ãƒ­ã‚¬ãƒ¼
    try:
        from pytorch_lightning.loggers import TensorBoardLogger
        logger = TensorBoardLogger(
            save_dir=Path(args.config).parent.parent / 'logs',
            name='stage1'
        )
    except ImportError:
        print("âš ï¸ TensorBoardæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - CSVãƒ­ã‚¬ãƒ¼ã‚’ä½¿ç”¨")
        from pytorch_lightning.loggers import CSVLogger
        logger = CSVLogger(
            save_dir=Path(args.config).parent.parent / 'logs',
            name='stage1'
        )
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šï¼ˆMPIå®Œå…¨å›é¿ï¼‰
    trainer_kwargs = {
        'max_epochs': config['training']['epochs'],
        'devices': 1 if torch.cuda.is_available() and args.devices > 0 else 'auto',
        'accelerator': 'gpu' if torch.cuda.is_available() and args.devices > 0 else 'cpu',
        'strategy': 'auto',  # DDPã‚’é¿ã‘ã¦autoã«è¨­å®š
        'precision': config['training']['precision'],
        'accumulate_grad_batches': config['training']['accumulate_grad_batches'],
        'gradient_clip_val': config['training']['gradient_clip'],
        'callbacks': callbacks,
        'logger': logger,
        'log_every_n_steps': config['logging']['log_every_n_steps'],
        'check_val_every_n_epoch': 1,
        'enable_progress_bar': True,
        'enable_model_summary': True,
        'num_nodes': 1,  # å˜ä¸€ãƒãƒ¼ãƒ‰å¼·åˆ¶
        'sync_batchnorm': False,  # ãƒãƒƒãƒæ­£è¦åŒ–åŒæœŸç„¡åŠ¹
        'use_distributed_sampler': False,  # åˆ†æ•£ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ç„¡åŠ¹
    }
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°ã‚’è¿½åŠ 
    if args.fast_dev_run:
        trainer_kwargs['fast_dev_run'] = True
        print("ğŸš€ Fast Dev Run ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹")
    if args.profiler:
        trainer_kwargs['profiler'] = args.profiler
        
    # é–‹ç™ºç”¨è¨­å®šï¼ˆãƒãƒƒãƒæ•°åˆ¶é™ï¼‰
    print(f"ğŸ” DEBUG: config['development'] = {config.get('development', 'NOT_FOUND')}")
    if 'development' in config and config['development'] is not None:
        if 'limit_train_batches' in config['development']:
            limit_train = config['development']['limit_train_batches']
            if limit_train is not None:
                trainer_kwargs['limit_train_batches'] = limit_train
                print(f"ğŸ”¢ é™å®šè¨“ç·´ãƒãƒƒãƒæ•°: {limit_train}")
            else:
                print("ğŸš€ è¨“ç·´ãƒãƒƒãƒæ•°åˆ¶é™ãªã—ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰")
        if 'limit_val_batches' in config['development']:
            limit_val = config['development']['limit_val_batches']
            if limit_val is not None:
                trainer_kwargs['limit_val_batches'] = limit_val
                print(f"ğŸ”¢ é™å®šæ¤œè¨¼ãƒãƒƒãƒæ•°: {limit_val}")
            else:
                print("ğŸš€ æ¤œè¨¼ãƒãƒƒãƒæ•°åˆ¶é™ãªã—ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰")
    
    # ğŸ”¥ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰
    if args.profile_mode:
        print("ğŸ” ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰: 100ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿å®Ÿè¡Œ")
        from pytorch_lightning.profilers import PyTorchProfiler
        profiler = PyTorchProfiler(
            schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=30),
            on_trace_ready=torch.profiler.tensorboard_trace_handler("log/prof"),
            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
            record_shapes=True,
            profile_memory=True,
            with_stack=True
        )
        trainer_kwargs['profiler'] = profiler
        trainer_kwargs['max_steps'] = 100  # 100ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿
        trainer_kwargs['max_epochs'] = -1  # ã‚¨ãƒãƒƒã‚¯åˆ¶é™ç„¡åŠ¹
        trainer_kwargs['logger'] = False   # ãƒ­ã‚¬ãƒ¼ç„¡åŠ¹åŒ–ï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«é›†ä¸­ï¼‰
        trainer_kwargs['callbacks'] = [custom_progress, memory_callback]  # ğŸ”¥ ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚‚ç¶™ç¶š
        print("ğŸ“ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«çµæœ: log/prof (ç¢ºèª: tensorboard --logdir=log/prof)")
        
    trainer = pl.Trainer(**trainer_kwargs)
    
    # LR Finderå®Ÿè¡Œï¼ˆè¨­å®šã§æœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    if config.get('lr_finder', {}).get('enabled', False):
        print("ğŸ” Learning Rate Finderå®Ÿè¡Œä¸­...")
        import matplotlib.pyplot as plt
        
        # LR Finderè¨­å®š
        lr_finder_config = config['lr_finder']
        min_lr = float(lr_finder_config.get('min_lr', 1e-8))
        max_lr = float(lr_finder_config.get('max_lr', 1.0))
        num_training = int(lr_finder_config.get('num_training', 100))
        save_path = lr_finder_config.get('save_path', 'lr_finder_results')
        
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        save_dir = Path(save_path)
        save_dir.mkdir(exist_ok=True)
        
        print(f"   ç¯„å›²: {min_lr:.1e} ï½ {max_lr:.1e}")
        print(f"   ã‚¹ãƒ†ãƒƒãƒ—æ•°: {num_training}")
        
        try:
            # PyTorch Lightning 2.4.0 å¯¾å¿œã® LR Finderå®Ÿè¡Œ
            from pytorch_lightning.tuner import Tuner
            tuner = Tuner(trainer)
            lr_finder = tuner.lr_find(
                model,
                train_dataloaders=train_loader,
                min_lr=min_lr,
                max_lr=max_lr,
                num_training=num_training,
                mode='exponential',
                early_stop_threshold=4.0,  # æå¤±ãŒ4å€ã«ãªã£ãŸã‚‰åœæ­¢
                update_attr=False  # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ç‡ã¯æ›´æ–°ã—ãªã„
            )
            
            # çµæœãƒ—ãƒ­ãƒƒãƒˆ
            fig = lr_finder.plot(suggest=True, show=False)
            
            # æ¨å¥¨å­¦ç¿’ç‡ã‚’å–å¾—
            suggested_lr = lr_finder.suggestion()
            
            # ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜
            plot_path = save_dir / "lr_finder_plot.png"
            fig.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close(fig)
            
            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            results_path = save_dir / "lr_finder_results.txt"
            with open(results_path, 'w') as f:
                f.write(f"Learning Rate Finder Results\n")
                f.write(f"=" * 40 + "\n")
                f.write(f"Configuration:\n")
                f.write(f"  Min LR: {min_lr:.1e}\n")
                f.write(f"  Max LR: {max_lr:.1e}\n")
                f.write(f"  Steps: {num_training}\n")
                f.write(f"\nResults:\n")
                f.write(f"  Suggested LR: {suggested_lr:.1e}\n")
                f.write(f"  Current base_lr: {config['training']['optimizer']['lr']:.1e}\n")
                if 't5_lr_top' in config['training']:
                    f.write(f"  Current t5_lr_top: {config['training']['t5_lr_top']:.1e}\n")
                f.write(f"\nRecommendations:\n")
                f.write(f"  - Set optimizer.lr to: {suggested_lr:.1e}\n")
                if 't5_lr_top' in config['training']:
                    t5_suggested = suggested_lr * 0.25  # T5ç”¨ã«ã‚ˆã‚Šä½ã„å­¦ç¿’ç‡
                    f.write(f"  - Set t5_lr_top to: {t5_suggested:.1e}\n")
            
            print(f"âœ… LR Finderå®Œäº†")
            print(f"   æ¨å¥¨å­¦ç¿’ç‡: {suggested_lr:.1e}")
            print(f"   ç¾åœ¨ã®å­¦ç¿’ç‡: {config['training']['optimizer']['lr']:.1e}")
            print(f"   ãƒ—ãƒ­ãƒƒãƒˆ: {plot_path}")
            print(f"   è©³ç´°çµæœ: {results_path}")
            
            # T5ä½¿ç”¨æ™‚ã®æ¨å¥¨å€¤ã‚‚è¡¨ç¤º
            if 't5_lr_top' in config['training']:
                current_t5_lr = config['training']['t5_lr_top']
                t5_suggested = suggested_lr * 0.25
                print(f"   T5ç¾åœ¨å€¤: {current_t5_lr:.1e}")
                print(f"   T5æ¨å¥¨å€¤: {t5_suggested:.1e}")
            
            print("\nğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
            print(f"   1. shared_base.yaml ã® optimizer.lr ã‚’ {suggested_lr:.1e} ã«è¨­å®š")
            if 't5_lr_top' in config['training']:
                print(f"   2. shared_base.yaml ã® t5_lr_top ã‚’ {suggested_lr * 0.25:.1e} ã«è¨­å®š")
            print("   3. è¨­å®šã‚’ä¿å­˜ã—ã¦å†åº¦è¨“ç·´ã‚’å®Ÿè¡Œ")
            
            # LR Finderå®Ÿè¡Œå¾Œã¯çµ‚äº†
            print("\nğŸ”„ LR Finderå®Œäº†ã€‚è¨­å®šã‚’æ›´æ–°ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return
            
        except Exception as e:
            print(f"âŒ LR Finderå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            print("âš ï¸  é€šå¸¸ã®è¨“ç·´ã‚’ç¶šè¡Œã—ã¾ã™...")
    
    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å‡¦ç†
    if args.dry_run:
        print("ğŸ§ª ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œä¸­...")
        print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æƒ…å ±:")
        print(f"     è¨“ç·´ãƒãƒƒãƒæ•°: {len(train_loader)}")
        print(f"     æ¤œè¨¼ãƒãƒƒãƒæ•°: {len(val_loader)}")
        
        # 1ãƒãƒƒãƒãƒ†ã‚¹ãƒˆ
        sample_batch = next(iter(train_loader))
        print(f"     ãƒãƒƒãƒå½¢çŠ¶: {[k + ': ' + str(v.shape) for k, v in sample_batch.items()]}")
        
        if args.plot_lr:
            # LRã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å¯è¦–åŒ–ï¼ˆå®Ÿè£…çœç•¥ï¼‰
            print("   LRã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: OneCycleLRè¨­å®šæ¸ˆã¿")
        
        print("âœ… ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†")
        return
    
    # è¤‡æ•°ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œ
    if args.seeds:
        print(f"ğŸ² è¤‡æ•°ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œ: {args.seeds}")
        results = []
        
        for seed in args.seeds:
            print(f"\nğŸ¯ ã‚·ãƒ¼ãƒ‰ {seed} ã§å®Ÿè¡Œä¸­...")
            
            # ã‚·ãƒ¼ãƒ‰è¨­å®š
            pl.seed_everything(seed)
            
            # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆ
            model = Stage1LightningModule(config)
            
            # freeze_epochs=0ã®å ´åˆã¯å³åº§ã«T5ã‚’è§£å‡
            freeze_epochs = config.get('transfer_learning', {}).get('freeze_lm_epochs', 0)
            if freeze_epochs == 0 and hasattr(model.model, 'shared_encoder') and hasattr(model.model.shared_encoder, 't5_encoder'):
                # T5ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å³åº§ã«è§£å‡
                for param in model.model.shared_encoder.t5_encoder.parameters():
                    param.requires_grad = True
                print(f"ğŸ”“ ã‚·ãƒ¼ãƒ‰{seed}: T5ã‚’å³æ™‚è§£å‡æ¸ˆã¿ (freeze_epochs=0)")
            
            # å„ã‚·ãƒ¼ãƒ‰ç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ãƒ­ã‚¬ãƒ¼ã‚’ä½œæˆ
            seed_checkpoint_callback = ModelCheckpoint(
                dirpath=Path(args.config).parent.parent / 'checkpoints' / f'seed_{seed}',
                filename=f'stage1-seed{seed}-{{epoch:02d}}-{{val_correlation_mean:.4f}}',
                monitor='val_correlation_mean',
                mode='max',
                save_top_k=config['logging']['save_top_k'],
                save_last=True
            )
            
            seed_early_stopping = EarlyStopping(
                monitor='val_correlation_mean',
                mode='max',
                patience=config['training']['early_stop']['patience'],
                min_delta=config['training']['early_stop']['min_delta']
            )
            
            try:
                from pytorch_lightning.loggers import TensorBoardLogger
                seed_logger = TensorBoardLogger(
                    save_dir=Path(args.config).parent.parent / 'logs',
                    name=f'stage1_seed_{seed}'
                )
            except ImportError:
                from pytorch_lightning.loggers import CSVLogger
                seed_logger = CSVLogger(
                    save_dir=Path(args.config).parent.parent / 'logs',
                    name=f'stage1_seed_{seed}'
                )
            
            # è¤‡æ•°ã‚·ãƒ¼ãƒ‰ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®š
            seed_trainer_kwargs = {
                'max_epochs': config['training']['epochs'],
                'devices': 1 if torch.cuda.is_available() and args.devices > 0 else 'auto',
                'accelerator': 'gpu' if torch.cuda.is_available() and args.devices > 0 else 'cpu',
                'callbacks': [seed_checkpoint_callback, seed_early_stopping, lr_monitor, custom_progress],  # ğŸ”¥ unfreezing_callbackã‚’å‰Šé™¤
                'logger': seed_logger,
                'precision': config['training']['precision'],
                'gradient_clip_val': config['training']['gradient_clip'],
                'accumulate_grad_batches': config['training']['accumulate_grad_batches'],
                'strategy': 'auto',
                'log_every_n_steps': config['logging']['log_every_n_steps'],
                'enable_model_summary': False,
                'num_nodes': 1,
                'sync_batchnorm': False,
                'use_distributed_sampler': False
            }
            
            # é–‹ç™ºç”¨è¨­å®šï¼ˆãƒãƒƒãƒæ•°åˆ¶é™ï¼‰ã‚’è¤‡æ•°ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œã«ã‚‚é©ç”¨
            if 'development' in config:
                if 'limit_train_batches' in config['development']:
                    seed_trainer_kwargs['limit_train_batches'] = config['development']['limit_train_batches']
                if 'limit_val_batches' in config['development']:
                    seed_trainer_kwargs['limit_val_batches'] = config['development']['limit_val_batches']
            
            # fast_dev_runã‚’è¤‡æ•°ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œã«ã‚‚é©ç”¨
            if args.fast_dev_run:
                seed_trainer_kwargs['fast_dev_run'] = True
            
            trainer = pl.Trainer(**seed_trainer_kwargs)
            
            # è¨“ç·´å®Ÿè¡Œ
            if args.resume_from:
                trainer.fit(model, train_loader, val_loader, ckpt_path=args.resume_from)
            else:
                trainer.fit(model, train_loader, val_loader)
                
            # çµæœè¨˜éŒ²
            best_score = seed_checkpoint_callback.best_model_score
            results.append({
                'seed': seed,
                'best_score': best_score,
                'best_checkpoint': seed_checkpoint_callback.best_model_path
            })
            
            print(f"   ã‚·ãƒ¼ãƒ‰ {seed} å®Œäº† - ã‚¹ã‚³ã‚¢: {best_score}")
        
        # è¤‡æ•°ã‚·ãƒ¼ãƒ‰çµæœã®çµ±è¨ˆ
        scores = [r['best_score'].item() if hasattr(r['best_score'], 'item') else float(r['best_score']) for r in results]
        mean_score = np.mean(scores)
        std_score = np.std(scores)
        
        print(f"\nğŸ“Š è¤‡æ•°ã‚·ãƒ¼ãƒ‰çµæœçµ±è¨ˆ:")
        print(f"   å¹³å‡ã‚¹ã‚³ã‚¢: {mean_score:.6f} Â± {std_score:.6f}")
        print(f"   æœ€é«˜ã‚¹ã‚³ã‚¢: {max(scores):.6f}")
        print(f"   æœ€ä½ã‚¹ã‚³ã‚¢: {min(scores):.6f}")
        print(f"   å®Ÿè¡Œã‚·ãƒ¼ãƒ‰æ•°: {len(scores)}")
        
        # è©³ç´°çµæœ
        print(f"\nğŸ“‹ è©³ç´°çµæœ:")
        for r in results:
            print(f"   ã‚·ãƒ¼ãƒ‰ {r['seed']}: {r['best_score']:.6f} ({r['best_checkpoint']})")
            
    else:
        # å˜ä¸€ã‚·ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆå¾“æ¥ã®å‡¦ç†ï¼‰
        if args.resume_from:
            print(f"ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹: {args.resume_from}")
            trainer.fit(model, train_loader, val_loader, ckpt_path=args.resume_from)
        else:
            trainer.fit(model, train_loader, val_loader)
        
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        print("âœ… è¨“ç·´å®Œäº†")
        print(f"   æœ€è‰¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {checkpoint_callback.best_model_path}")
        print(f"   æœ€è‰¯ã‚¹ã‚³ã‚¢: {checkpoint_callback.best_model_score}")
    
    # æœ€çµ‚è©•ä¾¡ï¼ˆfast_dev_runã§ã¯çœç•¥ï¼‰
    if not args.fast_dev_run:
        print("ğŸ“ˆ æœ€çµ‚è©•ä¾¡å®Ÿè¡Œä¸­...")
        # é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ†ã‚¹ãƒˆã‚‚ãƒãƒƒãƒæ•°åˆ¶é™
        if 'development' in config and 'limit_val_batches' in config['development']:
            # æ–°ã—ã„ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ã«limit_test_batchesã‚’è¨­å®šï¼‰
            test_trainer_kwargs = trainer_kwargs.copy()
            test_trainer_kwargs['limit_test_batches'] = config['development']['limit_val_batches']
            test_trainer = pl.Trainer(**test_trainer_kwargs)
            test_trainer.test(model, val_loader)
        else:
            trainer.test(model, val_loader)

if __name__ == "__main__":
    main()