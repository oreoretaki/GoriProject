#!/usr/bin/env python3
"""
GPUä½¿ç”¨ç‡ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import torch
import torch.profiler
import time
import psutil
import threading
from pathlib import Path
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir.parent))

from src.data_loader import create_stage1_dataloaders
from scripts.train_stage1 import load_config, Stage1LightningModule

def monitor_system(stop_event, results):
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚’ç›£è¦–"""
    cpu_usage = []
    memory_usage = []
    
    while not stop_event.is_set():
        cpu_usage.append(psutil.cpu_percent(interval=1))
        memory_usage.append(psutil.virtual_memory().percent)
        time.sleep(1)
    
    results['cpu_avg'] = sum(cpu_usage) / len(cpu_usage) if cpu_usage else 0
    results['memory_avg'] = sum(memory_usage) / len(memory_usage) if memory_usage else 0

def profile_data_loading(train_loader, num_batches=10):
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ€§èƒ½ã‚’æ¸¬å®š"""
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ€§èƒ½æ¸¬å®šä¸­...")
    
    load_times = []
    transfer_times = []
    
    for i, batch in enumerate(train_loader):
        if i >= num_batches:
            break
        
        load_start = time.time()
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†
        load_end = time.time()
        
        # GPUè»¢é€æ™‚é–“æ¸¬å®š
        transfer_start = time.time()
        if torch.cuda.is_available():
            batch = {k: v.cuda() if torch.is_tensor(v) else v for k, v in batch.items()}
        transfer_end = time.time()
        
        load_time = load_end - load_start
        transfer_time = transfer_end - transfer_start
        total_time = load_time + transfer_time
        
        load_times.append(load_time)
        transfer_times.append(transfer_time)
        
        print(f"  Batch {i+1}: Load {load_time:.3f}s + Transfer {transfer_time:.3f}s = {total_time:.3f}s")
    
    avg_load = sum(load_times) / len(load_times)
    avg_transfer = sum(transfer_times) / len(transfer_times)
    avg_total = avg_load + avg_transfer
    
    print(f"ğŸ“Š å¹³å‡ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰æ™‚é–“: {avg_load:.3f}s/batch")
    print(f"ğŸ“Š å¹³å‡GPUè»¢é€æ™‚é–“: {avg_transfer:.3f}s/batch")
    print(f"ğŸ“Š å¹³å‡ç·æ™‚é–“: {avg_total:.3f}s/batch")
    return avg_total

def profile_model_forward(model, train_loader, num_batches=5):
    """ãƒ¢ãƒ‡ãƒ«forward passæ€§èƒ½ã‚’æ¸¬å®š"""
    print("ğŸ” ãƒ¢ãƒ‡ãƒ«forward passæ€§èƒ½æ¸¬å®šä¸­...")
    
    model.eval()
    times = []
    
    with torch.no_grad():
        for i, batch in enumerate(train_loader):
            if i >= num_batches:
                break
            
            if torch.cuda.is_available():
                batch = {k: v.cuda() if torch.is_tensor(v) else v for k, v in batch.items()}
            
            start_time = time.time()
            
            # Forward pass
            features = batch['features']
            masks = batch['masks']
            outputs = model.model(features, masks)
            
            if torch.cuda.is_available():
                torch.cuda.synchronize()  # GPUåŒæœŸ
            
            end_time = time.time()
            times.append(end_time - start_time)
            print(f"  Forward {i+1}: {times[-1]:.3f}s")
    
    avg_time = sum(times) / len(times)
    print(f"ğŸ“Š å¹³å‡forwardæ™‚é–“: {avg_time:.3f}s/batch")
    return avg_time

def profile_with_pytorch_profiler(model, train_loader):
    """PyTorch Profilerã§è©³ç´°åˆ†æ"""
    print("ğŸ” PyTorch Profilerå®Ÿè¡Œä¸­...")
    
    model.train()
    
    def trace_handler(prof):
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«çµæœã‚’ä¿å­˜
        prof.export_chrome_trace("trace.json")
        prof.export_stacks("profiler_stacks.txt", "self_cuda_time_total")
        
        # ä¸Šä½å‡¦ç†ã‚’è¡¨ç¤º
        print("\nğŸ“Š GPUä½¿ç”¨ç‡ä¸Šä½å‡¦ç†:")
        print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
        
        print("\nğŸ“Š CPUä½¿ç”¨ç‡ä¸Šä½å‡¦ç†:")
        print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))
    
    with torch.profiler.profile(
        activities=[
            torch.profiler.ProfilerActivity.CPU,
            torch.profiler.ProfilerActivity.CUDA,
        ],
        schedule=torch.profiler.schedule(
            wait=1,
            warmup=1,
            active=3,
            repeat=2
        ),
        on_trace_ready=trace_handler,
        record_shapes=True,
        profile_memory=True,
        with_stack=True
    ) as prof:
        
        for batch_idx, batch in enumerate(train_loader):
            if batch_idx >= 10:  # 10ãƒãƒƒãƒã§çµ‚äº†
                break
            
            if torch.cuda.is_available():
                batch = {k: v.cuda() if torch.is_tensor(v) else v for k, v in batch.items()}
            
            # Forward pass
            features = batch['features']
            targets = batch['targets']
            masks = batch['masks']
            
            outputs = model.model(features, masks)
            reconstructed = outputs['reconstructed']
            
            # æå¤±è¨ˆç®—
            m1_data = targets[:, 0]
            losses = model.criterion(reconstructed, targets, masks, m1_data)
            
            prof.step()

def main():
    print("ğŸš€ GPUä½¿ç”¨ç‡ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æé–‹å§‹")
    print("=" * 50)
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config_path = "configs/t5_large_nofreeze.yaml"
    config = load_config(config_path)
    config['data']['data_dir'] = "../data/derived"
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚º256ç”¨ã®è¨­å®šç¢ºèª
    print(f"ğŸ“Š ãƒãƒƒãƒã‚µã‚¤ã‚º: {config['training']['batch_size']}")
    print(f"ğŸ“Š accumulate_grad_batches: {config['training']['accumulate_grad_batches']}")
    print(f"ğŸ“Š å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {config['training']['batch_size'] * config['training']['accumulate_grad_batches']}")
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚º1024ç”¨ã®ãƒ†ã‚¹ãƒˆè¨­å®š
    config['development'] = {
        'limit_train_batches': 10,    # ãƒãƒƒãƒã‚µã‚¤ã‚º1024ç”¨ã«æ¸›å°‘
        'limit_val_batches': 2        # ãƒãƒƒãƒã‚µã‚¤ã‚º1024ç”¨ã«æ¸›å°‘
    }
    
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆä¸­...")
    train_loader, val_loader = create_stage1_dataloaders("../data/derived", config)
    
    print("ğŸ§  ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    model = Stage1LightningModule(config)
    
    # PyTorch 2.0 ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–ï¼ˆäº‹å‰ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä»˜ãï¼‰
    if torch.__version__ >= '2.0.0':
        print("ğŸš€ PyTorch 2.0 ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
        try:
            # äº‹å‰ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ã‚’éš è”½
            print("ğŸ”¥ ãƒ€ãƒŸãƒ¼å…¥åŠ›ã§ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...")
            with torch.no_grad():
                # ãƒãƒƒãƒã‚µã‚¤ã‚º1ã§ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ
                dummy_features = torch.randn(1, 6, 128, 36, device=model.device, dtype=torch.bfloat16)
                dummy_masks = torch.ones(1, 6, 128, device=model.device, dtype=torch.bool)
                
                # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
                _ = model.model(dummy_features, dummy_masks)
                print("âœ… ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
            
            # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«é©ç”¨
            model.model = torch.compile(model.model, backend="inductor", mode="max-autotune")
            print("âœ… TorchCompileé©ç”¨å®Œäº†ï¼ˆäº‹å‰ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¸ˆã¿ï¼‰")
            
        except Exception as e:
            print(f"âš ï¸ TorchCompileå¤±æ•—ã€é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: {e}")
    else:
        print("âš ï¸ PyTorch 2.0+ãŒå¿…è¦ã§ã™ï¼ˆTorchCompileã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    
    if torch.cuda.is_available():
        model = model.cuda()
        print(f"âœ… CUDAä½¿ç”¨: {torch.cuda.get_device_name()}")
        print(f"ğŸ“Š CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹
    stop_event = threading.Event()
    monitor_results = {}
    monitor_thread = threading.Thread(target=monitor_system, args=(stop_event, monitor_results))
    monitor_thread.start()
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ€§èƒ½æ¸¬å®š
        data_load_time = profile_data_loading(train_loader, num_batches=10)
        
        # 2. ãƒ¢ãƒ‡ãƒ«forwardæ€§èƒ½æ¸¬å®š
        forward_time = profile_model_forward(model, train_loader, num_batches=5)
        
        # 3. PyTorch Profilerå®Ÿè¡Œ
        profile_with_pytorch_profiler(model, train_loader)
        
        # åˆ†æçµæœ
        print("\n" + "=" * 50)
        print("ğŸ“Š ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ")
        print("=" * 50)
        print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰æ™‚é–“: {data_load_time:.3f}s/batch")
        print(f"ğŸ”„ Forwardæ™‚é–“: {forward_time:.3f}s/batch")
        print(f"âš¡ ç·å‡¦ç†æ™‚é–“: {data_load_time + forward_time:.3f}s/batch")
        
        # GPUä½¿ç”¨ç‡ãŒä½ã„åŸå› æ¨å®š
        if data_load_time > forward_time * 2:
            print("ğŸ”´ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (CPU/IO bound)")
            print("ğŸ’¡ å¯¾ç­–: num_workerså¢—åŠ ã€pin_memory=Trueã€SSDä½¿ç”¨")
        elif forward_time > data_load_time * 2:
            print("ğŸ”´ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: ãƒ¢ãƒ‡ãƒ«è¨ˆç®— (GPU bound)")
            print("ğŸ’¡ å¯¾ç­–: ãƒãƒƒãƒã‚µã‚¤ã‚ºå¢—åŠ ã€mixed precisionä½¿ç”¨")
        else:
            print("ğŸŸ¡ ãƒãƒ©ãƒ³ã‚¹å‹: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«è¨ˆç®—ãŒå‡è¡¡")
            print("ğŸ’¡ å¯¾ç­–: ä¸¦åˆ—åº¦èª¿æ•´ã€prefetchæœ€é©åŒ–")
        
    finally:
        # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–åœæ­¢
        stop_event.set()
        monitor_thread.join()
        
        print(f"\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡:")
        print(f"  CPUå¹³å‡: {monitor_results.get('cpu_avg', 0):.1f}%")
        print(f"  Memoryå¹³å‡: {monitor_results.get('memory_avg', 0):.1f}%")
        
        print("\nğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«:")
        print("  - trace.json (Chrome Tracingç”¨)")
        print("  - profiler_stacks.txt (è©³ç´°ã‚¹ã‚¿ãƒƒã‚¯)")

if __name__ == "__main__":
    main()