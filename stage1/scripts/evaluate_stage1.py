#!/usr/bin/env python3
"""
Stage 1 è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Correlation, Consistency ratio, Spectral deltaè¨ˆç®—
"""

import os
import sys
import argparse
import yaml
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List
import json
import warnings
warnings.filterwarnings('ignore')

# TF32ã‚’æœ‰åŠ¹åŒ–ï¼ˆRTX 30xx/40xx/A100ã§é«˜é€ŸåŒ–ï¼‰
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.set_float32_matmul_precision('high')
print("âœ… TF32ã¨Tensor Coreæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’PATHã«è¿½åŠ 
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(current_dir.parent))

from src.data_loader import create_stage1_dataloaders
from scripts.train_stage1 import Stage1LightningModule

class Stage1Evaluator:
    """Stage 1 è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model, config: dict):
        self.model = model
        self.config = config
        self.timeframes = config['data']['timeframes']
        self.device = next(model.parameters()).device
        
    def evaluate_model(self, dataloader) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Ÿè¡Œ"""
        
        print("ğŸ“Š Stage 1 ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹")
        
        self.model.eval()
        all_predictions = []
        all_targets = []
        all_masks = []
        
        # äºˆæ¸¬åé›†
        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                if batch_idx % 10 == 0:
                    print(f"   ãƒãƒƒãƒ {batch_idx}/{len(dataloader)} å‡¦ç†ä¸­...")
                    
                features = batch['features'].to(self.device)
                targets = batch['targets'].to(self.device)
                masks = batch['masks'].to(self.device)
                
                # äºˆæ¸¬
                outputs = self.model(features, masks)
                predictions = outputs['reconstructed']
                
                all_predictions.append(predictions.cpu())
                all_targets.append(targets.cpu())
                all_masks.append(masks.cpu())
                
        # ãƒ†ãƒ³ã‚½ãƒ«çµåˆ
        all_predictions = torch.cat(all_predictions, dim=0)  # [n_samples, n_tf, seq_len, 4]
        all_targets = torch.cat(all_targets, dim=0)
        all_masks = torch.cat(all_masks, dim=0)
        
        print(f"   è©•ä¾¡ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {all_predictions.shape}")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        metrics = self._calculate_metrics(all_predictions, all_targets, all_masks)
        
        return metrics
        
    def _calculate_metrics(self, predictions, targets, masks) -> Dict:
        """å„ç¨®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        
        metrics = {
            'correlation_per_tf': {},
            'consistency_ratio': {},
            'spectral_delta': {},
            'reconstruction_quality': {}
        }
        
        n_samples, n_tf, seq_len, n_features = predictions.shape
        
        for tf_idx, tf_name in enumerate(self.timeframes):
            print(f"   {tf_name.upper()} ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ä¸­...")
            
            pred_tf = predictions[:, tf_idx]  # [n_samples, seq_len, 4]
            target_tf = targets[:, tf_idx]
            mask_tf = masks[:, tf_idx]  # [n_samples, seq_len]
            
            # 1. Correlation@TF
            correlations = self._calculate_correlation_per_tf(pred_tf, target_tf, mask_tf)
            metrics['correlation_per_tf'][tf_name] = correlations
            
            # 2. Reconstruction Quality (MSE, MAE)
            quality = self._calculate_reconstruction_quality(pred_tf, target_tf, mask_tf)
            metrics['reconstruction_quality'][tf_name] = quality
            
            # 3. Spectral Delta
            spectral_delta = self._calculate_spectral_delta(pred_tf, target_tf, mask_tf)
            metrics['spectral_delta'][tf_name] = spectral_delta
            
        # 4. Consistency ratio (M1ãƒ™ãƒ¼ã‚¹ã§ä»–TFã¨ã®æ•´åˆæ€§)
        if n_tf > 1:
            consistency = self._calculate_consistency_ratio(predictions, targets, masks)
            metrics['consistency_ratio'] = consistency
            
        # 5. å…¨ä½“ã‚µãƒãƒªãƒ¼
        metrics['summary'] = self._calculate_summary(metrics)
        
        return metrics
        
    def _calculate_correlation_per_tf(self, pred, target, mask) -> Dict:
        """TFã”ã¨ã®ç›¸é–¢è¨ˆç®—"""
        
        correlations = {'ohlc': []}
        ohlc_names = ['open', 'high', 'low', 'close']
        
        for feat_idx, feat_name in enumerate(ohlc_names):
            # ãƒã‚¹ã‚¯ã•ã‚ŒãŸéƒ¨åˆ†ã®ã¿
            masked_pred = pred[mask.bool(), feat_idx]
            masked_target = target[mask.bool(), feat_idx]
            
            if len(masked_pred) > 1:
                # ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢
                corr_matrix = np.corrcoef(masked_pred.numpy(), masked_target.numpy())
                correlation = corr_matrix[0, 1] if not np.isnan(corr_matrix[0, 1]) else 0.0
            else:
                correlation = 0.0
                
            correlations['ohlc'].append(correlation)
            correlations[feat_name] = correlation
            
        # å¹³å‡ç›¸é–¢
        correlations['mean'] = np.mean(correlations['ohlc'])
        
        return correlations
        
    def _calculate_reconstruction_quality(self, pred, target, mask) -> Dict:
        """å†æ§‹ç¯‰å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
        
        # ãƒã‚¹ã‚¯ã•ã‚ŒãŸéƒ¨åˆ†ã®ã¿
        masked_pred = pred[mask.bool()]  # [n_masked, 4]
        masked_target = target[mask.bool()]
        
        if len(masked_pred) == 0:
            return {'mse': float('inf'), 'mae': float('inf'), 'rmse': float('inf')}
            
        # MSE, MAE, RMSE
        mse = torch.mean((masked_pred - masked_target) ** 2).item()
        mae = torch.mean(torch.abs(masked_pred - masked_target)).item()
        rmse = np.sqrt(mse)
        
        return {
            'mse': mse,
            'mae': mae,
            'rmse': rmse,
            'n_samples': len(masked_pred)
        }
        
    def _calculate_spectral_delta(self, pred, target, mask) -> Dict:
        """ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ å·®åˆ†è¨ˆç®—"""
        
        spectral_deltas = []
        
        for feat_idx in range(4):  # OHLC
            # ãƒã‚¹ã‚¯ã•ã‚ŒãŸéƒ¨åˆ†ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æŠ½å‡º
            valid_sequences = []
            
            for sample_idx in range(pred.size(0)):
                sample_mask = mask[sample_idx]
                if sample_mask.sum() > 10:  # æœ€ä½10ãƒã‚¤ãƒ³ãƒˆå¿…è¦
                    pred_seq = pred[sample_idx, sample_mask.bool(), feat_idx]
                    target_seq = target[sample_idx, sample_mask.bool(), feat_idx]
                    
                    if len(pred_seq) > 10:
                        valid_sequences.append((pred_seq, target_seq))
                        
            if valid_sequences:
                seq_deltas = []
                for pred_seq, target_seq in valid_sequences[:100]:  # æœ€å¤§100ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
                    # FFTã§ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ è¨ˆç®—
                    pred_fft = torch.fft.fft(pred_seq)
                    target_fft = torch.fft.fft(target_seq)
                    
                    # ãƒ‘ãƒ¯ãƒ¼ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ 
                    pred_power = torch.abs(pred_fft) ** 2
                    target_power = torch.abs(target_fft) ** 2
                    
                    # å¯¾æ•°ãƒ‘ãƒ¯ãƒ¼ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ ã®å·®åˆ†
                    log_pred = torch.log(pred_power + 1e-8)
                    log_target = torch.log(target_power + 1e-8)
                    
                    delta = torch.mean(torch.abs(log_pred - log_target)).item()
                    seq_deltas.append(delta)
                    
                spectral_deltas.append(np.mean(seq_deltas))
            else:
                spectral_deltas.append(float('inf'))
                
        return {
            'per_feature': spectral_deltas,
            'mean': np.mean([d for d in spectral_deltas if d != float('inf')])
        }
        
    def _calculate_consistency_ratio(self, predictions, targets, masks) -> Dict:
        """æ•´åˆæ€§æ¯”ç‡è¨ˆç®—ï¼ˆTFé–“ã®ä¸€è²«æ€§ï¼‰"""
        
        # M1ã‚’åŸºæº–ã¨ã—ã¦ã€ä»–ã®TFã¨ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯
        m1_pred = predictions[:, 0]  # [n_samples, seq_len, 4]
        m1_target = targets[:, 0]
        
        consistency_ratios = {}
        
        for tf_idx in range(1, predictions.size(1)):
            tf_name = self.timeframes[tf_idx]
            tf_pred = predictions[:, tf_idx]
            tf_target = targets[:, tf_idx]
            
            # ç°¡æ˜“æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼šäºˆæ¸¬ã¨å®Ÿéš›ã®å·®åˆ†ãŒè¨±å®¹ç¯„å›²å†…ã‹
            tolerance = 0.01  # 1%è¨±å®¹å·®
            
            consistent_samples = 0
            total_samples = 0
            
            for sample_idx in range(tf_pred.size(0)):
                tf_sample = tf_pred[sample_idx]
                target_sample = tf_target[sample_idx]
                
                if torch.any(target_sample != 0):  # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨
                    relative_error = torch.abs(tf_sample - target_sample) / (torch.abs(target_sample) + 1e-8)
                    if torch.mean(relative_error) < tolerance:
                        consistent_samples += 1
                    total_samples += 1
                    
            if total_samples > 0:
                ratio = consistent_samples / total_samples
            else:
                ratio = 0.0
                
            consistency_ratios[tf_name] = {
                'ratio': ratio,
                'consistent_samples': consistent_samples,
                'total_samples': total_samples
            }
            
        return consistency_ratios
        
    def _calculate_summary(self, metrics: Dict) -> Dict:
        """å…¨ä½“ã‚µãƒãƒªãƒ¼è¨ˆç®—"""
        
        # å¹³å‡ç›¸é–¢
        correlations = [tf_metrics['mean'] for tf_metrics in metrics['correlation_per_tf'].values()]
        mean_correlation = np.mean(correlations)
        
        # å¹³å‡å†æ§‹ç¯‰å“è³ª
        mse_values = [tf_metrics['mse'] for tf_metrics in metrics['reconstruction_quality'].values() 
                     if tf_metrics['mse'] != float('inf')]
        mean_mse = np.mean(mse_values) if mse_values else float('inf')
        
        # å¹³å‡ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ å·®åˆ†
        spectral_values = [tf_metrics['mean'] for tf_metrics in metrics['spectral_delta'].values()
                          if tf_metrics['mean'] != float('inf')]
        mean_spectral_delta = np.mean(spectral_values) if spectral_values else float('inf')
        
        # å¹³å‡æ•´åˆæ€§æ¯”ç‡
        if metrics['consistency_ratio']:
            consistency_values = [tf_metrics['ratio'] for tf_metrics in metrics['consistency_ratio'].values()]
            mean_consistency = np.mean(consistency_values)
        else:
            mean_consistency = 0.0
            
        return {
            'mean_correlation': mean_correlation,
            'mean_mse': mean_mse,
            'mean_spectral_delta': mean_spectral_delta,
            'mean_consistency_ratio': mean_consistency,
            'n_timeframes': len(self.timeframes)
        }
        
    def save_results(self, metrics: Dict, output_path: str):
        """çµæœä¿å­˜"""
        
        # JSONä¿å­˜
        with open(output_path, 'w') as f:
            json.dump(metrics, f, indent=2, default=lambda x: float(x) if isinstance(x, np.ndarray) else x)
            
        print(f"ğŸ’¾ è©•ä¾¡çµæœä¿å­˜: {output_path}")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        summary = metrics['summary']
        print("ğŸ“ˆ è©•ä¾¡ã‚µãƒãƒªãƒ¼:")
        print(f"   å¹³å‡ç›¸é–¢: {summary['mean_correlation']:.4f}")
        print(f"   å¹³å‡MSE: {summary['mean_mse']:.6f}")
        print(f"   å¹³å‡ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ å·®åˆ†: {summary['mean_spectral_delta']:.4f}")
        print(f"   å¹³å‡æ•´åˆæ€§æ¯”ç‡: {summary['mean_consistency_ratio']:.4f}")

def main():
    parser = argparse.ArgumentParser(description='Stage 1 Evaluation')
    parser.add_argument('--config', type=str, required=True, help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--ckpt', type=str, required=True, help='ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹')
    parser.add_argument('--data_dir', type=str, required=True, help='ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--output', type=str, default='evaluation_results.json', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«')
    
    args = parser.parse_args()
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    config['data']['data_dir'] = args.data_dir
    
    print("ğŸ” Stage 1 è©•ä¾¡é–‹å§‹")
    print(f"   ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {args.ckpt}")
    print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.data_dir}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    _, val_loader = create_stage1_dataloaders(args.data_dir, config)
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    print("ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    model = Stage1LightningModule.load_from_checkpoint(args.ckpt, config=config)
    model.eval()
    
    # è©•ä¾¡å®Ÿè¡Œ
    evaluator = Stage1Evaluator(model, config)
    metrics = evaluator.evaluate_model(val_loader)
    
    # çµæœä¿å­˜
    evaluator.save_results(metrics, args.output)
    
    print("âœ… è©•ä¾¡å®Œäº†")

if __name__ == "__main__":
    main()