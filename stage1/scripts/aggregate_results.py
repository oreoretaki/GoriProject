#!/usr/bin/env python3
"""
T5å®Ÿé¨“çµæžœã®å–ã‚Šã¾ã¨ã‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¤‡æ•°ã®å®Ÿé¨“ãƒ­ã‚°ã‹ã‚‰val_loss/val_corr/æ™‚é–“/VRAMã‚’è¡¨å½¢å¼ã§å‡ºåŠ›
"""

import os
import re
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import argparse

def extract_metrics_from_log(log_file_path):
    """å­¦ç¿’ãƒ­ã‚°ã‹ã‚‰æœ€çµ‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º"""
    metrics = {
        'config_name': None,
        'experiment_name': None,
        'final_epoch': None,
        'duration_seconds': None,
        'final_val_loss': None,
        'final_val_corr': None,
        'best_val_corr': None,
        'final_grad_norm': None,
        'amp_overflow_count': 0,
        'early_stopped': False
    }
    
    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            log_content = f.read()
        
        # è¨­å®šåã‚’æŠ½å‡º
        config_match = re.search(r'--config configs/([^\.]+)\.yaml', log_content)
        if config_match:
            metrics['config_name'] = config_match.group(1)
        
        # å®Ÿé¨“æ™‚é–“ã‚’æŠ½å‡º
        duration_match = re.search(r'å®Ÿè¡Œæ™‚é–“: (\d+)ç§’', log_content)
        if duration_match:
            metrics['duration_seconds'] = int(duration_match.group(1))
        
        # æœ€çµ‚ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æŠ½å‡º
        epoch_matches = re.findall(r'Epoch (\d+):', log_content)
        if epoch_matches:
            metrics['final_epoch'] = int(epoch_matches[-1])
        
        # æœ€çµ‚val_lossã¨val_corrã‚’æŠ½å‡º
        val_patterns = [
            r'val_loss_ep=([0-9.]+)',
            r'val_corr_ep=([0-9.-]+)',
            r'val_correlation_mean\s*([0-9.-]+)',
            r'grad_norm=([0-9.e+-]+)'
        ]
        
        # æœ€å¾Œã®å€¤ã‚’å–å¾—
        for pattern in val_patterns:
            matches = re.findall(pattern, log_content)
            if matches:
                try:
                    if 'val_loss' in pattern:
                        metrics['final_val_loss'] = float(matches[-1])
                    elif 'val_corr' in pattern or 'correlation' in pattern:
                        metrics['final_val_corr'] = float(matches[-1])
                    elif 'grad_norm' in pattern:
                        metrics['final_grad_norm'] = float(matches[-1])
                except ValueError:
                    pass
        
        # ãƒ™ã‚¹ãƒˆval_correlationã‚’æŠ½å‡º
        best_corr_match = re.search(r'æœ€è‰¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ.*val_correlation_mean=([0-9.-]+)', log_content)
        if best_corr_match:
            metrics['best_val_corr'] = float(best_corr_match.group(1))
        
        # AMP overflow ã‚«ã‚¦ãƒ³ãƒˆ
        amp_overflow_matches = re.findall(r'amp_overflow', log_content)
        metrics['amp_overflow_count'] = len(amp_overflow_matches)
        
        # Early stoppingæ¤œå‡º
        if 'EarlyStopping' in log_content and 'stopped' in log_content:
            metrics['early_stopped'] = True
            
    except Exception as e:
        print(f"âš ï¸ ãƒ­ã‚°è§£æžã‚¨ãƒ©ãƒ¼ {log_file_path}: {e}")
    
    return metrics

def find_experiment_results(results_dir):
    """å®Ÿé¨“çµæžœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
    results_path = Path(results_dir)
    log_files = []
    
    if not results_path.exists():
        print(f"âŒ çµæžœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {results_dir}")
        return log_files
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
    for timestamp_dir in results_path.iterdir():
        if timestamp_dir.is_dir():
            # å„å®Ÿé¨“ã®training.logã‚’æ¤œç´¢
            for exp_dir in timestamp_dir.iterdir():
                if exp_dir.is_dir():
                    log_file = exp_dir / 'training.log'
                    if log_file.exists():
                        log_files.append(log_file)
    
    return sorted(log_files)

def create_comparison_table(metrics_list):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§ã‹ã‚‰æ¯”è¼ƒè¡¨ã‚’ä½œæˆ"""
    if not metrics_list:
        return pd.DataFrame()
    
    df = pd.DataFrame(metrics_list)
    
    # ã‚«ãƒ©ãƒ é †åºã‚’æ•´ç†
    columns_order = [
        'config_name', 'final_epoch', 'duration_seconds',
        'final_val_loss', 'final_val_corr', 'best_val_corr',
        'final_grad_norm', 'amp_overflow_count', 'early_stopped'
    ]
    
    existing_columns = [col for col in columns_order if col in df.columns]
    df = df[existing_columns]
    
    # ä¸¦ã³æ›¿ãˆï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³â†’T5ç³»ï¼‰
    config_order = ['t5_baseline_10ep', 't5_frozen_all_10ep', 't5_freeze_10ep', 't5_nofreeze_10ep']
    df['config_order'] = df['config_name'].map({config: i for i, config in enumerate(config_order)})
    df = df.sort_values('config_order').drop('config_order', axis=1)
    
    return df

def main():
    parser = argparse.ArgumentParser(description='T5å®Ÿé¨“çµæžœå–ã‚Šã¾ã¨ã‚')
    parser.add_argument('--results_dir', type=str, default='./t5_experiment_results',
                       help='å®Ÿé¨“çµæžœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--output', type=str, default='t5_comparison_summary.csv',
                       help='å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--format', type=str, choices=['csv', 'markdown', 'both'], default='both',
                       help='å‡ºåŠ›å½¢å¼')
    
    args = parser.parse_args()
    
    print("ðŸ“Š T5å®Ÿé¨“çµæžœå–ã‚Šã¾ã¨ã‚é–‹å§‹")
    print(f"   çµæžœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.results_dir}")
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
    log_files = find_experiment_results(args.results_dir)
    print(f"   è¦‹ã¤ã‹ã£ãŸãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {len(log_files)}")
    
    if not log_files:
        print("âŒ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º
    metrics_list = []
    for log_file in log_files:
        print(f"   è§£æžä¸­: {log_file}")
        metrics = extract_metrics_from_log(log_file)
        if metrics['config_name']:
            metrics_list.append(metrics)
    
    if not metrics_list:
        print("âŒ æœ‰åŠ¹ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # æ¯”è¼ƒè¡¨ä½œæˆ
    df = create_comparison_table(metrics_list)
    
    print(f"\nðŸ“ˆ å®Ÿé¨“çµæžœã‚µãƒžãƒªãƒ¼ ({len(metrics_list)}å®Ÿé¨“)")
    print("=" * 80)
    print(df.to_string(index=False, float_format='%.4f'))
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    if args.format in ['csv', 'both']:
        csv_file = args.output
        df.to_csv(csv_file, index=False, float_format='%.6f')
        print(f"\nðŸ’¾ CSVä¿å­˜: {csv_file}")
    
    if args.format in ['markdown', 'both']:
        md_file = args.output.replace('.csv', '.md')
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# T5è»¢ç§»å­¦ç¿’å®Ÿé¨“çµæžœæ¯”è¼ƒ\n\n")
            f.write(f"**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("## çµæžœã‚µãƒžãƒªãƒ¼\n\n")
            f.write(df.to_markdown(index=False, floatfmt='.4f'))
            f.write("\n\n## è¨­å®šèª¬æ˜Ž\n\n")
            f.write("| è¨­å®šå | èª¬æ˜Ž |\n")
            f.write("|--------|------|\n")
            f.write("| t5_baseline_10ep | ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ï¼ˆå¾“æ¥æ‰‹æ³•ï¼‰ |\n")
            f.write("| t5_frozen_all_10ep | T5å®Œå…¨å‡çµï¼ˆè¡¨ç¾ã®ã¿åˆ©ç”¨ï¼‰ |\n") 
            f.write("| t5_freeze_10ep | T5ã‚’2ã‚¨ãƒãƒƒã‚¯å‡çµå¾Œè§£å‡ |\n")
            f.write("| t5_nofreeze_10ep | T5ã‚’æœ€åˆã‹ã‚‰å­¦ç¿’ |\n")
        
        print(f"ðŸ’¾ Markdownä¿å­˜: {md_file}")
    
    # ç°¡æ˜“åˆ†æž
    print("\nðŸ” ç°¡æ˜“åˆ†æž")
    if len(df) > 1:
        baseline_idx = df[df['config_name'] == 't5_baseline_10ep'].index
        if len(baseline_idx) > 0:
            baseline_corr = df.loc[baseline_idx[0], 'final_val_corr']
            print(f"   ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ val_corr: {baseline_corr:.4f}")
            
            for _, row in df.iterrows():
                if row['config_name'] != 't5_baseline_10ep':
                    improvement = row['final_val_corr'] - baseline_corr
                    print(f"   {row['config_name']}: {improvement:+.4f} ({improvement/abs(baseline_corr)*100:+.1f}%)")

if __name__ == "__main__":
    main()