#!/usr/bin/env python3
"""
Stage 1 æ­£è¦åŒ–
TFã”ã¨ã®z-scoreæ­£è¦åŒ–ãƒ»çµ±è¨ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
"""

import pandas as pd
import numpy as np
import torch
import json
from pathlib import Path
from typing import Dict, Optional, Union
import warnings
warnings.filterwarnings('ignore')

class TFNormalizer:
    """TFåˆ¥æ­£è¦åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: dict, cache_stats: bool = True):
        """
        Args:
            config: è¨­å®šè¾æ›¸
            cache_stats: çµ±è¨ˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã‹
        """
        self.config = config
        self.cache_stats = cache_stats
        self.timeframes = config['data']['timeframes']
        self.n_features = config['data']['n_features']
        
        # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        data_dir = Path(config['data']['data_dir'])
        self.stats_file = data_dir / config['data']['stats_file']
        self.stats_train_file = data_dir / 'stats_train.json'  # trainå°‚ç”¨çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«
        
        # æ­£è¦åŒ–çµ±è¨ˆ {tf: {'mean': [...], 'std': [...]}}
        self.stats = {}
        
        # ç‰¹å¾´é‡åï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        self.feature_names = ['open', 'high', 'low', 'close', 'delta_close', 'body_ratio']
        
        print(f"ğŸ“ TFNormalizeråˆæœŸåŒ–")
        print(f"   çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«: {self.stats_file}")
        print(f"   ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹: {cache_stats}")
        
    def fit(self, tf_data: Dict[str, pd.DataFrame]) -> None:
        """
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ­£è¦åŒ–çµ±è¨ˆã‚’è¨ˆç®—
        
        Args:
            tf_data: {tf_name: DataFrame} TFãƒ‡ãƒ¼ã‚¿
        """
        print(f"ğŸ“Š æ­£è¦åŒ–çµ±è¨ˆè¨ˆç®—ä¸­...")
        
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒå¿…è¦ãªã®ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        from .feature_engineering import FeatureEngineer
        feature_engineer = FeatureEngineer(self.config)
        
        for tf_name in self.timeframes:
            if tf_name not in tf_data:
                continue
                
            df = tf_data[tf_name]
            print(f"   {tf_name.upper()}: {len(df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
            
            # ç‰¹å¾´é‡è¨ˆç®—ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
            if len(df) > 100000:
                # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆã¯10%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                sample_size = len(df) // 10
                indices = np.random.choice(len(df) - 1, sample_size, replace=False)
                indices = np.sort(indices)  # æ™‚ç³»åˆ—é †ã‚’ä¿æŒ
                sample_df = df.iloc[indices]
            else:
                sample_df = df
                
            # ç‰¹å¾´é‡è¨ˆç®—
            features = feature_engineer._calculate_features(sample_df)  # [n_samples, n_features]
            
            # çµ±è¨ˆè¨ˆç®—
            mean = np.mean(features, axis=0)
            std = np.std(features, axis=0)
            
            # ã‚¼ãƒ­åˆ†æ•£å›é¿
            std = np.where(std < 1e-8, 1.0, std)
            
            self.stats[tf_name] = {
                'mean': mean.tolist(),
                'std': std.tolist(),
                'n_samples': len(features)
            }
            
            print(f"     å¹³å‡: {mean[:4].round(6)}")  # æœ€åˆã®4ç‰¹å¾´é‡ã®ã¿è¡¨ç¤º
            print(f"     æ¨™æº–åå·®: {std[:4].round(6)}")
            
        # çµ±è¨ˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        if self.cache_stats:
            self._save_stats()
            
    def load_stats(self, split: str = "all") -> None:
        """
        ä¿å­˜æ¸ˆã¿çµ±è¨ˆã‚’ãƒ­ãƒ¼ãƒ‰
        
        Args:
            split: "all" (é€šå¸¸), "train" (trainå°‚ç”¨çµ±è¨ˆ)
        """
        
        if split == "train":
            stats_file = self.stats_train_file
        else:
            stats_file = self.stats_file
            
        if not stats_file.exists():
            raise FileNotFoundError(f"çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {stats_file}")
            
        with open(stats_file, 'r') as f:
            data = json.load(f)
            
        # æ–°å½¢å¼ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰ã‹æ—§å½¢å¼ã‹ã‚’åˆ¤å®š
        if 'timeframes' in data:
            self.stats = data['timeframes']
        else:
            self.stats = data  # æ—§å½¢å¼
            
        print(f"ğŸ“‚ æ­£è¦åŒ–çµ±è¨ˆãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(self.stats)}å€‹ã®TF ({split})")
        
    def _save_stats(self) -> None:
        """çµ±è¨ˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        stats_with_meta = {
            'metadata': {
                'n_timeframes': len(self.timeframes),
                'n_features': self.n_features,
                'feature_names': self.feature_names,
                'normalization_method': 'zscore'
            },
            'timeframes': self.stats
        }
        
        with open(self.stats_file, 'w') as f:
            json.dump(stats_with_meta, f, indent=2)
            
        print(f"ğŸ’¾ æ­£è¦åŒ–çµ±è¨ˆä¿å­˜å®Œäº†: {self.stats_file}")
        
    def save_stats(self, split: str = "all") -> None:
        """
        çµ±è¨ˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆsplitæŒ‡å®šå¯èƒ½ï¼‰
        
        Args:
            split: "all" (é€šå¸¸), "train" (trainå°‚ç”¨çµ±è¨ˆ)
        """
        if split == "train":
            # trainå°‚ç”¨çµ±è¨ˆã‚’ä¿å­˜
            stats_with_meta = {
                'metadata': {
                    'n_timeframes': len(self.timeframes),
                    'n_features': self.n_features,
                    'feature_names': self.feature_names,
                    'normalization_method': 'zscore',
                    'split': 'train_only'
                },
                'timeframes': self.stats
            }
            
            with open(self.stats_train_file, 'w') as f:
                json.dump(stats_with_meta, f, indent=2)
                
            print(f"ğŸ’¾ trainå°‚ç”¨çµ±è¨ˆä¿å­˜å®Œäº†: {self.stats_train_file}")
        else:
            # é€šå¸¸ã®çµ±è¨ˆä¿å­˜
            self._save_stats()
        
    def normalize(self, features: torch.Tensor) -> torch.Tensor:
        """
        ç‰¹å¾´é‡ã‚’æ­£è¦åŒ–
        
        Args:
            features: [n_tf, seq_len, n_features] ç‰¹å¾´é‡ãƒ†ãƒ³ã‚½ãƒ«
            
        Returns:
            normalized: [n_tf, seq_len, n_features] æ­£è¦åŒ–æ¸ˆã¿ç‰¹å¾´é‡
        """
        if not self.stats:
            raise ValueError("æ­£è¦åŒ–çµ±è¨ˆãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«fit()ã¾ãŸã¯load_stats()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            
        normalized = features.clone()
        
        for i, tf_name in enumerate(self.timeframes):
            if tf_name not in self.stats:
                continue
                
            tf_stats = self.stats[tf_name]
            mean = torch.tensor(tf_stats['mean'], dtype=features.dtype, device=features.device)
            std = torch.tensor(tf_stats['std'], dtype=features.dtype, device=features.device)
            
            # z-scoreæ­£è¦åŒ–: (x - mean) / std
            normalized[i] = (features[i] - mean) / std
            
        # NaN/Infå‡¦ç†
        normalized = torch.nan_to_num(normalized, nan=0.0, posinf=1.0, neginf=-1.0)
        
        return normalized
        
    def normalize_targets(self, targets: torch.Tensor) -> torch.Tensor:
        """
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆOHLCï¼‰ã‚’æ­£è¦åŒ–
        
        Args:
            targets: [n_tf, seq_len, 4] OHLCã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            
        Returns:
            normalized: [n_tf, seq_len, 4] æ­£è¦åŒ–æ¸ˆã¿ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        """
        if not self.stats:
            raise ValueError("æ­£è¦åŒ–çµ±è¨ˆãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            
        normalized = targets.clone()
        
        for i, tf_name in enumerate(self.timeframes):
            if tf_name not in self.stats:
                continue
                
            tf_stats = self.stats[tf_name]
            # OHLCç”¨ã®çµ±è¨ˆï¼ˆæœ€åˆã®4ç‰¹å¾´é‡ï¼‰
            mean = torch.tensor(tf_stats['mean'][:4], dtype=targets.dtype, device=targets.device)
            std = torch.tensor(tf_stats['std'][:4], dtype=targets.dtype, device=targets.device)
            
            # z-scoreæ­£è¦åŒ–
            normalized[i] = (targets[i] - mean) / std
            
        # NaN/Infå‡¦ç†
        normalized = torch.nan_to_num(normalized, nan=0.0, posinf=1.0, neginf=-1.0)
        
        return normalized
        
    def normalize_single_tf(self, features: np.ndarray, tf_name: str) -> np.ndarray:
        """
        å˜ä¸€TFã®ç‰¹å¾´é‡ã‚’æ­£è¦åŒ–ï¼ˆModel v2ç”¨ï¼‰
        
        Args:
            features: [seq_len, n_features] ç‰¹å¾´é‡é…åˆ—
            tf_name: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å
            
        Returns:
            normalized: [seq_len, n_features] æ­£è¦åŒ–æ¸ˆã¿ç‰¹å¾´é‡
        """
        if not self.stats:
            raise ValueError("æ­£è¦åŒ–çµ±è¨ˆãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«fit()ã¾ãŸã¯load_stats()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            
        if tf_name not in self.stats:
            print(f"âš ï¸ {tf_name}ã®çµ±è¨ˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ­£è¦åŒ–ã›ãšã«è¿”ã—ã¾ã™ã€‚")
            return features
            
        tf_stats = self.stats[tf_name]
        mean = np.array(tf_stats['mean'])
        std = np.array(tf_stats['std'])
        
        # z-scoreæ­£è¦åŒ–: (x - mean) / std
        normalized = (features - mean) / std
        
        # NaN/Infå‡¦ç†
        normalized = np.nan_to_num(normalized, nan=0.0, posinf=1.0, neginf=-1.0)
        
        return normalized
        
    def normalize_targets_single_tf(self, targets: np.ndarray, tf_name: str) -> np.ndarray:
        """
        å˜ä¸€TFã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆOHLCï¼‰ã‚’æ­£è¦åŒ–ï¼ˆModel v2ç”¨ï¼‰
        
        Args:
            targets: [seq_len, 4] OHLCã‚¿ãƒ¼ã‚²ãƒƒãƒˆé…åˆ—
            tf_name: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å
            
        Returns:
            normalized: [seq_len, 4] æ­£è¦åŒ–æ¸ˆã¿ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        """
        if not self.stats:
            raise ValueError("æ­£è¦åŒ–çµ±è¨ˆãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            
        if tf_name not in self.stats:
            print(f"âš ï¸ {tf_name}ã®çµ±è¨ˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ­£è¦åŒ–ã›ãšã«è¿”ã—ã¾ã™ã€‚")
            return targets
            
        tf_stats = self.stats[tf_name]
        # OHLCç”¨ã®çµ±è¨ˆï¼ˆæœ€åˆã®4ç‰¹å¾´é‡ï¼‰
        mean = np.array(tf_stats['mean'][:4])
        std = np.array(tf_stats['std'][:4])
        
        # z-scoreæ­£è¦åŒ–
        normalized = (targets - mean) / std
        
        # NaN/Infå‡¦ç†
        normalized = np.nan_to_num(normalized, nan=0.0, posinf=1.0, neginf=-1.0)
        
        return normalized
        
    def denormalize(self, normalized: torch.Tensor, tf_indices: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        æ­£è¦åŒ–æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
        
        Args:
            normalized: [n_tf, seq_len, n_features] æ­£è¦åŒ–æ¸ˆã¿ãƒ†ãƒ³ã‚½ãƒ«
            tf_indices: TFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆNoneã®å ´åˆã¯å…¨TFï¼‰
            
        Returns:
            denormalized: å…ƒã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
        """
        if not self.stats:
            raise ValueError("æ­£è¦åŒ–çµ±è¨ˆãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            
        denormalized = normalized.clone()
        
        tf_list = self.timeframes if tf_indices is None else [self.timeframes[i] for i in tf_indices]
        
        for i, tf_name in enumerate(tf_list):
            if tf_name not in self.stats:
                continue
                
            tf_stats = self.stats[tf_name]
            mean = torch.tensor(tf_stats['mean'], dtype=normalized.dtype, device=normalized.device)
            std = torch.tensor(tf_stats['std'], dtype=normalized.dtype, device=normalized.device)
            
            # é€†æ­£è¦åŒ–: x * std + mean
            idx = i if tf_indices is None else tf_indices[i]
            denormalized[idx] = normalized[idx] * std + mean
            
        return denormalized
        
    def get_stats_summary(self) -> Dict:
        """çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        
        if not self.stats:
            return {"error": "çµ±è¨ˆãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
            
        summary = {}
        
        for tf_name, tf_stats in self.stats.items():
            mean = np.array(tf_stats['mean'])
            std = np.array(tf_stats['std'])
            
            summary[tf_name] = {
                'n_samples': tf_stats['n_samples'],
                'features': {}
            }
            
            for j, feature_name in enumerate(self.feature_names):
                summary[tf_name]['features'][feature_name] = {
                    'mean': float(mean[j]),
                    'std': float(std[j])
                }
                
        return summary