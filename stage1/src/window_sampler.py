#!/usr/bin/env python3
"""
ãƒãƒ«ãƒTFã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µãƒ³ãƒ—ãƒ©ãƒ¼
åŒã˜ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’å…¨TFã‹ã‚‰ã‚¹ãƒ©ã‚¤ã‚¹ãƒ»å³ç«¯æ•´åˆ—
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import warnings
from scipy.ndimage import convolve1d
import time
import hashlib
from pathlib import Path
warnings.filterwarnings('ignore')

class MultiTFWindowSampler:
    """ãƒãƒ«ãƒTFåŒæœŸã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µãƒ³ãƒ—ãƒ©ãƒ¼"""
    
    def __init__(
        self,
        tf_data: Dict[str, pd.DataFrame],
        seq_len: int,
        split: str = "train",
        val_split: float = 0.2,
        min_coverage: float = 0.8,
        cache_dir: Optional[str] = None,
        val_gap_days: float = 1.0
    ):
        """
        Args:
            tf_data: {tf_name: DataFrame} å½¢å¼ã®TFãƒ‡ãƒ¼ã‚¿
            seq_len: M1ã§ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            split: "train" or "val"
            val_split: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å‰²åˆ
            min_coverage: æœ€å°ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆå…¨TFã§ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å‰²åˆï¼‰
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            val_gap_days: è¨“ç·´ã¨æ¤œè¨¼ã®é–“ã®æ™‚é–“çš„ã‚®ãƒ£ãƒƒãƒ—ï¼ˆæ—¥æ•°ï¼‰
        """
        self.tf_data = tf_data
        self.seq_len = seq_len
        self.split = split
        self.val_split = val_split
        self.min_coverage = min_coverage
        self.cache_dir = Path(cache_dir) if cache_dir else Path("./cache")
        self.val_gap_days = val_gap_days
        
        # M1ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ç”¨
        self.base_tf = 'm1'
        if self.base_tf not in tf_data:
            raise ValueError(f"ãƒ™ãƒ¼ã‚¹TF '{self.base_tf}' ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“")
            
        # TFã”ã¨ã®ã‚¹ãƒ†ãƒƒãƒ—é–“éš”ï¼ˆåˆ†ï¼‰
        self.step_map = {
            'm1': 1,
            'm5': 5, 
            'm15': 15,
            'm30': 30,
            'h1': 60,
            'h4': 240
        }
            
        print(f"ğŸ”„ MultiTFWindowSampleråˆæœŸåŒ– ({split})")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½
        self.cache_dir.mkdir(exist_ok=True)
        cache_hash = self._compute_cache_hash()
        cache_file = self.cache_dir / f"windows_{cache_hash}.npy"
        
        # æœ‰åŠ¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç¯„å›²ã‚’è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¾ãŸã¯ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‰ˆï¼‰
        start_time = time.time()
        if cache_file.exists():
            print(f"   ğŸ“‚ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦èª­ã¿è¾¼ã¿: {cache_file.name}")
            valid_indices = np.load(cache_file)
            self.valid_windows = self._indices_to_windows(valid_indices)
        else:
            print(f"   ğŸ”„ æ–°è¦ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è¨ˆç®—ä¸­...")
            self.valid_windows = self._find_valid_windows()
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            valid_indices = self._windows_to_indices(self.valid_windows)
            np.save(cache_file, valid_indices)
            print(f"   ğŸ’¾ ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜: {cache_file.name}")
        
        elapsed_time = time.time() - start_time
        print(f"   âš¡ ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
        
        # è¨“ç·´/æ¤œè¨¼åˆ†å‰²
        self.split_windows = self._split_windows()
        
        print(f"   ç·ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°: {len(self.valid_windows)}")
        print(f"   {split}ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°: {len(self.split_windows)}")
        
    def _find_valid_windows(self) -> List[Tuple[pd.Timestamp, pd.Timestamp]]:
        """å…¨TFã§ãƒ‡ãƒ¼ã‚¿ãŒååˆ†å­˜åœ¨ã™ã‚‹æœ‰åŠ¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ç™ºè¦‹ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‰ˆï¼‰"""
        
        base_df = self.tf_data[self.base_tf]
        n_windows = len(base_df) - self.seq_len + 1
        
        if n_windows <= 0:
            return []
            
        print(f"   ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ¢ç´¢é–‹å§‹: {n_windows:,} å€™è£œ")
        
        # å„TFã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—
        all_valid = np.ones(n_windows, dtype=bool)
        
        for tf_name, df in self.tf_data.items():
            # ã“ã®TFã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãƒã‚¹ã‚¯ã‚’ä½œæˆ
            tf_valid = self._compute_tf_coverage_vectorized(
                base_df, df, tf_name
            )
            all_valid &= tf_valid
            
            valid_count = np.sum(tf_valid)
            print(f"     {tf_name}: {valid_count:,}/{n_windows:,} ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒæœ‰åŠ¹")
        
        # æœ‰åŠ¹ãªã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        valid_indices = np.where(all_valid)[0]
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒšã‚¢ã«å¤‰æ›
        valid_windows = []
        for idx in valid_indices:
            start_time = base_df.index[idx]
            end_time = base_df.index[idx + self.seq_len - 1]
            valid_windows.append((start_time, end_time))
            
        final_count = len(valid_windows)
        print(f"   ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¢ç´¢å®Œäº†: {final_count:,} æœ‰åŠ¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦")
        
        # æ€§èƒ½çµ±è¨ˆ
        if n_windows > 0:
            efficiency = (final_count / n_windows) * 100
            print(f"   ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡: {efficiency:.1f}% ({final_count:,}/{n_windows:,})")
        
        return valid_windows
        
    def _compute_tf_coverage_vectorized(self, base_df: pd.DataFrame, tf_df: pd.DataFrame, tf_name: str) -> np.ndarray:
        """ç‰¹å®šTFã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—"""
        
        n_windows = len(base_df) - self.seq_len + 1
        
        if tf_name == self.base_tf:
            # M1ã¯å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§æœ‰åŠ¹ï¼ˆæ—¢ã«ã‚µã‚¤ã‚ºç¢ºèªæ¸ˆã¿ï¼‰
            return np.ones(n_windows, dtype=bool)
            
        # ã“ã®TFã«å¿…è¦ãªæœ€å°ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ã‚’è¨ˆç®—
        window_duration_min = self.seq_len - 1  # M1åˆ†å˜ä½ã§ã®æœŸé–“
        tf_intervals = {
            'm1': 1, 'm5': 5, 'm15': 15, 'm30': 30, 
            'h1': 60, 'h4': 240, 'd': 1440
        }
        interval = tf_intervals[tf_name]
        expected_len = max(1, int(window_duration_min / interval) + 1)
        min_required = int(expected_len * self.min_coverage)
        
        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ä¸€æ‹¬è¨ˆç®—
        try:
            # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€ - ä¸¡æ–¹ã‚’tz-naiveã«å¤‰æ›
            base_index = base_df.index
            tf_index = tf_df.index
            
            # tz-awareã®å ´åˆã¯UTCã«å¤‰æ›ã—ã¦ã‹ã‚‰tz-naiveã«
            if hasattr(base_index, 'tz') and base_index.tz is not None:
                base_index = base_index.tz_convert('UTC').tz_localize(None)
            if hasattr(tf_index, 'tz') and tf_index.tz is not None:
                tf_index = tf_index.tz_convert('UTC').tz_localize(None)
            
            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–‹å§‹æ™‚åˆ»é…åˆ—ã‚’ä½œæˆ
            start_times = base_index[:n_windows].values
            end_times = base_index[self.seq_len-1:self.seq_len-1+n_windows].values
            
            # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸæ¤œç´¢ã§ãƒ‡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—
            start_indices = tf_index.searchsorted(start_times, side='left')
            end_indices = tf_index.searchsorted(end_times, side='right')
            
            # å„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—
            data_counts = end_indices - start_indices
            
            # é—¾å€¤ä»¥ä¸Šã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ç‰¹å®š
            valid_mask = data_counts >= min_required
            
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒƒãƒå‡¦ç†ã§è¨ˆç®—
            print(f"     ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§{tf_name}ã‚’å‡¦ç†: {str(e)}")
            valid_mask = np.zeros(n_windows, dtype=bool)
            
            batch_size = 10000
            for batch_start in range(0, n_windows, batch_size):
                batch_end = min(batch_start + batch_size, n_windows)
                
                for i in range(batch_start, batch_end):
                    start_time = base_df.index[i]
                    end_time = base_df.index[i + self.seq_len - 1]
                    
                    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€
                    if hasattr(start_time, 'tz') and start_time.tz is not None:
                        start_time = start_time.tz_convert('UTC').tz_localize(None)
                    if hasattr(end_time, 'tz') and end_time.tz is not None:
                        end_time = end_time.tz_convert('UTC').tz_localize(None)
                    
                    # tf_indexã‚‚çµ±ä¸€
                    tf_index_safe = tf_df.index
                    if hasattr(tf_index_safe, 'tz') and tf_index_safe.tz is not None:
                        tf_index_safe = tf_index_safe.tz_convert('UTC').tz_localize(None)
                    
                    start_idx = tf_index_safe.searchsorted(start_time, side='left')
                    end_idx = tf_index_safe.searchsorted(end_time, side='right')
                    
                    data_count = end_idx - start_idx
                    valid_mask[i] = data_count >= min_required
                    
        return valid_mask
    
    def _check_coverage(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> bool:
        """æŒ‡å®šæœŸé–“ã§ã®å…¨TFãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæ—§å®Ÿè£…ãƒ»ãƒ¬ã‚¬ã‚·ãƒ¼ç”¨ï¼‰"""
        
        for tf_name, df in self.tf_data.items():
            # ã“ã®æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            window_data = df.loc[start_time:end_time]
            
            if tf_name == self.base_tf:
                # M1ã¯æ­£ç¢ºã« seq_len å¿…è¦
                expected_len = self.seq_len
            else:
                # ä»–ã®TFã¯æœŸé–“ã«å¿œã˜ãŸæœŸå¾…é•·ã‚’è¨ˆç®—
                expected_len = self._calculate_expected_length(tf_name, start_time, end_time)
                
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯
            if len(window_data) < expected_len * self.min_coverage:
                return False
                
        return True
        
    def _calculate_expected_length(self, tf_name: str, start_time: pd.Timestamp, end_time: pd.Timestamp) -> int:
        """TFã¨æœŸé–“ã«åŸºã¥ãæœŸå¾…ãƒ‡ãƒ¼ã‚¿é•·ã‚’è¨ˆç®—"""
        
        # æœŸé–“ã®é•·ã•ï¼ˆåˆ†ï¼‰
        duration_minutes = (end_time - start_time).total_seconds() / 60
        
        # TFã”ã¨ã®é–“éš”ï¼ˆåˆ†ï¼‰
        tf_intervals = {
            'm1': 1,
            'm5': 5,
            'm15': 15,
            'm30': 30,
            'h1': 60,
            'h4': 240,
            'd': 1440
        }
        
        interval = tf_intervals[tf_name]
        expected_len = int(duration_minutes / interval) + 1
        
        return max(1, expected_len)  # æœ€ä½1ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ
        
    def _split_windows(self) -> List[Tuple[pd.Timestamp, pd.Timestamp]]:
        """è¨“ç·´/æ¤œè¨¼åˆ†å‰²ï¼ˆæ™‚é–“çš„ã‚®ãƒ£ãƒƒãƒ—ä»˜ãï¼‰"""
        
        n_total = len(self.valid_windows)
        n_val = int(n_total * self.val_split)
        
        # val_gap_days ã‚’åˆ†å˜ä½ã«å¤‰æ›
        val_gap_minutes = int(self.val_gap_days * 24 * 60)
        
        # ãƒ™ãƒ¼ã‚¹TFï¼ˆM1ï¼‰ã®å›ºå®šé–“éš”ã‚’ä½¿ç”¨ã—ã¦ã‚®ãƒ£ãƒƒãƒ—è¨ˆç®—
        base_step_minutes = self.step_map[self.base_tf]  # M1 = 1åˆ†
        gap_windows = int(val_gap_minutes / base_step_minutes)
        
        if n_val == 0:
            return self.valid_windows if self.split == "train" else []
            
        # ã‚®ãƒ£ãƒƒãƒ—ã‚’è€ƒæ…®ã—ãŸåˆ†å‰²ï¼ˆä¿®æ­£ç‰ˆï¼‰
        if self.split == "train":
            print(f"   ğŸ• æ™‚é–“çš„ã‚®ãƒ£ãƒƒãƒ—: {self.val_gap_days}æ—¥ = {val_gap_minutes}åˆ† = {gap_windows}çª“ (ãƒ™ãƒ¼ã‚¹é–“éš”={base_step_minutes}åˆ†)")
            
            # è¨“ç·´: æœ€å¾Œã® (n_val + gap_windows) ã‚’é™¤å¤–
            return self.valid_windows[:-(n_val + gap_windows)]
        else:  # val
            # æ¤œè¨¼: æœ€å¾Œã® n_val ã®ã¿ä½¿ç”¨ï¼ˆgapã®å¾Œã‹ã‚‰ï¼‰
            val_windows = self.valid_windows[-n_val:]
            
            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼šæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¡¨ç¤º
            if val_windows:
                first_val_ts = val_windows[0][0]  # (start_time, end_time)ã®start_time
                print(f"   [DBG] æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿é–‹å§‹æ™‚åˆ»: {first_val_ts}")
                print(f"   [DBG] è¨ˆç®—ã•ã‚ŒãŸã‚®ãƒ£ãƒƒãƒ—çª“æ•°: {gap_windows} (ãƒ™ãƒ¼ã‚¹é–“éš”={base_step_minutes}åˆ†)")
                
                # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚‚è¡¨ç¤º
                if n_val + gap_windows < len(self.valid_windows):
                    last_train_window = self.valid_windows[-(n_val + gap_windows) - 1]
                    last_train_ts = last_train_window[1]  # end_time
                    gap_actual = (first_val_ts - last_train_ts).total_seconds() / 86400  # æ—¥æ•°
                    print(f"   [DBG] è¨“ç·´ãƒ‡ãƒ¼ã‚¿çµ‚äº†æ™‚åˆ»: {last_train_ts}")
                    print(f"   [DBG] å®Ÿéš›ã®ã‚®ãƒ£ãƒƒãƒ—: {gap_actual:.1f}æ—¥")
                else:
                    print(f"   [DBG] è­¦å‘Š: ã‚®ãƒ£ãƒƒãƒ—è¨ˆç®—ã§ç¯„å›²å¤–ã‚¢ã‚¯ã‚»ã‚¹ (n_val={n_val}, gap_windows={gap_windows}, total={len(self.valid_windows)})")
                    
            return val_windows
            
    def __len__(self) -> int:
        """ã‚µãƒ³ãƒ—ãƒ«æ•°"""
        return len(self.split_windows)
        
    def __getitem__(self, idx: int) -> Dict[str, pd.DataFrame]:
        """
        æŒ‡å®šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒ«ãƒTFã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’å–å¾—
        
        Returns:
            Dict[tf_name, DataFrame]: å„TFã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ‡ãƒ¼ã‚¿
        """
        if idx >= len(self.split_windows):
            raise IndexError(f"Index {idx} out of range (max: {len(self.split_windows)-1})")
            
        start_time, end_time = self.split_windows[idx]
        
        window_data = {}
        
        for tf_name, df in self.tf_data.items():
            # æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            tf_window = df.loc[start_time:end_time].copy()
            
            if tf_name == self.base_tf:
                # M1ã¯æ­£ç¢ºã« seq_len ã«ãƒªã‚µãƒ³ãƒ—ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                if len(tf_window) != self.seq_len:
                    # æ™‚é–“ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§è£œé–“ã—ã¦seq_lené•·ã«ã™ã‚‹
                    tf_window = self._resample_to_length(tf_window, self.seq_len, start_time, end_time)
                    
            window_data[tf_name] = tf_window
            
        return window_data
        
    def _resample_to_length(
        self, 
        df: pd.DataFrame, 
        target_len: int, 
        start_time: pd.Timestamp, 
        end_time: pd.Timestamp
    ) -> pd.DataFrame:
        """DataFrameã‚’æŒ‡å®šé•·ã«ãƒªã‚µãƒ³ãƒ—ãƒ«"""
        
        # ç­‰é–“éš”æ™‚é–“ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        time_index = pd.date_range(start=start_time, end=end_time, periods=target_len)
        
        # OHLCãƒ‡ãƒ¼ã‚¿ã®é©åˆ‡ãªãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        resampled = df.reindex(time_index, method='nearest')
        
        # æ¬ æå€¤ã‚’å‰æ–¹åŸ‹ã‚
        resampled = resampled.fillna(method='ffill').fillna(method='bfill')
        
        return resampled
        
    def get_sample_window_info(self, idx: int = 0) -> Dict:
        """ã‚µãƒ³ãƒ—ãƒ«ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        
        if len(self.split_windows) == 0:
            return {"error": "No valid windows"}
            
        start_time, end_time = self.split_windows[idx]
        window_data = self[idx]
        
        info = {
            "window_index": idx,
            "start_time": start_time,
            "end_time": end_time,
            "duration_hours": (end_time - start_time).total_seconds() / 3600,
            "tf_lengths": {tf: len(data) for tf, data in window_data.items()}
        }
        
        return info
    
    def _compute_cache_hash(self) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        # ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã«åŸºã¥ããƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
        base_df = self.tf_data[self.base_tf]
        hash_data = f"{len(base_df)}_{self.seq_len}_{self.min_coverage}"
        
        # ãƒ‡ãƒ¼ã‚¿ã®é–‹å§‹ãƒ»çµ‚äº†æ™‚åˆ»ã‚‚å«ã‚ã‚‹
        hash_data += f"_{base_df.index[0]}_{base_df.index[-1]}"
        
        return hashlib.md5(hash_data.encode()).hexdigest()[:8]
    
    def _windows_to_indices(self, windows: List[Tuple]) -> np.ndarray:
        """ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒªã‚¹ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é…åˆ—ã«å¤‰æ›"""
        base_df = self.tf_data[self.base_tf]
        indices = []
        
        for start_time, end_time in windows:
            # é–‹å§‹æ™‚åˆ»ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            start_idx = base_df.index.get_loc(start_time)
            indices.append(start_idx)
            
        return np.array(indices, dtype=np.int32)
    
    def _indices_to_windows(self, indices: np.ndarray) -> List[Tuple]:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é…åˆ—ã‚’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒªã‚¹ãƒˆã«å¤‰æ›"""
        base_df = self.tf_data[self.base_tf]
        windows = []
        
        for idx in indices:
            start_time = base_df.index[idx]
            end_time = base_df.index[idx + self.seq_len - 1]
            windows.append((start_time, end_time))
            
        return windows