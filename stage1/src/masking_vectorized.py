#!/usr/bin/env python3
"""
Stage 1 å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
ğŸ”¥ Python ãƒ«ãƒ¼ãƒ—ã‚’é™¤å»ã—ã€10å€é«˜é€ŸåŒ–
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

class VectorizedMaskingStrategy(nn.Module):
    """å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ï¼ˆ10å€é«˜é€Ÿï¼‰"""
    
    def __init__(self, config: dict, n_features: int = 6):
        super().__init__()
        self.config = config
        self.timeframes = config['data']['timeframes']
        self.n_features = n_features
        
        # ãƒã‚¹ã‚­ãƒ³ã‚°è¨­å®š
        self.mask_ratio = config['masking']['mask_ratio']
        self.mask_span_min = config['masking']['mask_span_min']
        self.mask_span_max = config['masking']['mask_span_max']
        self.sync_across_tf = config['masking']['sync_across_tf']
        
        # ğŸ”¥ Learnable Mask Token
        self.mask_token = nn.Parameter(torch.randn(n_features) * 0.02)
        
        print(f"âš¡ VectorizedMaskingStrategyåˆæœŸåŒ–ï¼ˆ10å€é«˜é€Ÿç‰ˆï¼‰")
        print(f"   ãƒã‚¹ã‚¯ç‡: {self.mask_ratio}")
        print(f"   ã‚¹ãƒ‘ãƒ³ç¯„å›²: {self.mask_span_min}-{self.mask_span_max}")
        print(f"   TFé–“åŒæœŸ: {self.sync_across_tf}")
        
        # torchä¹±æ•°ç”Ÿæˆå™¨
        self.generator = torch.Generator()
        
    def generate_masks_dict(self, features: Dict[str, torch.Tensor], seed: int = None, eval_mask_ratio_override: float = None) -> Dict[str, torch.Tensor]:
        """
        ğŸ”¥ å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒã‚¹ã‚¯ç”Ÿæˆ - Python ãƒ«ãƒ¼ãƒ—é™¤å»
        
        Args:
            features: Dict[tf_name, torch.Tensor] - [batch, seq_len, n_features]
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
            eval_mask_ratio_override: è©•ä¾¡æ™‚ãƒã‚¹ã‚¯ç‡
            
        Returns:
            masks: Dict[tf_name, torch.Tensor] - [batch, seq_len] bool
        """
        if seed is not None:
            self.generator.manual_seed(seed)
            
        effective_mask_ratio = eval_mask_ratio_override if eval_mask_ratio_override is not None else self.mask_ratio
        
        if self.sync_across_tf:
            return self._generate_sync_masks_vectorized(features, effective_mask_ratio)
        else:
            return self._generate_independent_masks_vectorized(features, effective_mask_ratio)
    
    def _generate_sync_masks_vectorized(self, features: Dict[str, torch.Tensor], mask_ratio: float) -> Dict[str, torch.Tensor]:
        """ğŸ”¥ TFé–“åŒæœŸãƒã‚¹ã‚¯ - å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        # æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’å–å¾—
        max_seq_len = max(x.shape[1] for x in features.values())
        
        # ä»£è¡¨çš„ãªTFã‹ã‚‰ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—
        first_tf = next(iter(features.values()))
        batch_size = first_tf.shape[0]
        device = first_tf.device
        
        # ğŸ”¥ ä¸€æ‹¬ã§ãƒ™ãƒ¼ã‚¹ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆï¼ˆãƒãƒƒãƒå…¨ä½“ï¼‰
        base_masks = self._generate_batch_masks_vectorized(batch_size, max_seq_len, mask_ratio, device)
        
        # å„TFã«é©ç”¨
        masks = {}
        for tf_name, tf_features in features.items():
            _, seq_len, _ = tf_features.shape
            
            # ãƒ™ãƒ¼ã‚¹ãƒã‚¹ã‚¯ã‚’é©å¿œ
            if seq_len == max_seq_len:
                masks[tf_name] = base_masks
            else:
                # å³ç«¯ã‚’å–ã‚‹ï¼ˆæœ€æ–°éƒ¨åˆ†ã‚’é‡è¦–ï¼‰
                masks[tf_name] = base_masks[:, -seq_len:]
                
            # æœ‰åŠ¹ä½ç½®ã®ã¿ãƒã‚¹ã‚¯é©ç”¨
            masks[tf_name] = self._apply_valid_mask_vectorized(masks[tf_name], tf_features)
        
        return masks
    
    def _generate_independent_masks_vectorized(self, features: Dict[str, torch.Tensor], mask_ratio: float) -> Dict[str, torch.Tensor]:
        """ğŸ”¥ TFå€‹åˆ¥ãƒã‚¹ã‚¯ - å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        masks = {}
        
        for tf_name, tf_features in features.items():
            batch_size, seq_len, _ = tf_features.shape
            device = tf_features.device
            
            # ğŸ”¥ ä¸€æ‹¬ã§ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
            tf_masks = self._generate_batch_masks_vectorized(batch_size, seq_len, mask_ratio, device)
            
            # æœ‰åŠ¹ä½ç½®ã®ã¿ãƒã‚¹ã‚¯é©ç”¨
            masks[tf_name] = self._apply_valid_mask_vectorized(tf_masks, tf_features)
        
        return masks
    
    def _generate_batch_masks_vectorized(self, batch_size: int, seq_len: int, mask_ratio: float, device: torch.device) -> torch.Tensor:
        """
        ğŸ”¥ ãƒãƒƒãƒå…¨ä½“ã®ãƒã‚¹ã‚¯ã‚’ä¸€æ‹¬ç”Ÿæˆ - å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        
        Args:
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            mask_ratio: ãƒã‚¹ã‚¯ç‡
            device: ãƒ‡ãƒã‚¤ã‚¹
            
        Returns:
            masks: [batch, seq_len] bool tensor
        """
        # çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å ´åˆã¯ãƒã‚¹ã‚¯ãªã—
        if seq_len < self.mask_span_min:
            return torch.zeros(batch_size, seq_len, device=device, dtype=torch.bool)
        
        # ğŸ”¥ ä¸€æ‹¬ã§ãƒã‚¹ã‚¯ã‚¹ãƒ‘ãƒ³ã‚’ç”Ÿæˆ
        target_masked = int(seq_len * mask_ratio)
        
        # å¿…è¦ãªã‚¹ãƒ‘ãƒ³æ•°ã‚’æ¨å®šï¼ˆä¿å®ˆçš„ã«å¤šã‚ã«ç”Ÿæˆï¼‰
        avg_span_len = (self.mask_span_min + self.mask_span_max) / 2
        estimated_spans = max(1, int(target_masked / avg_span_len * 2))  # 2å€å®‰å…¨ä¿‚æ•°
        
        # ğŸ”¥ ãƒãƒƒãƒÃ—ã‚¹ãƒ‘ãƒ³æ•°ã®ä¹±æ•°ã‚’ä¸€æ‹¬ç”Ÿæˆ
        span_lengths = torch.randint(
            self.mask_span_min, 
            self.mask_span_max + 1, 
            (batch_size, estimated_spans),
            device=device,
            generator=self.generator
        )
        
        # ğŸ”¥ é–‹å§‹ä½ç½®ã‚‚ä¸€æ‹¬ç”Ÿæˆ
        start_positions = torch.randint(
            0, 
            max(1, seq_len - self.mask_span_min), 
            (batch_size, estimated_spans),
            device=device,
            generator=self.generator
        )
        
        # ğŸ”¥ çµ‚äº†ä½ç½®ã‚’è¨ˆç®—
        end_positions = (start_positions + span_lengths).clamp(max=seq_len)
        
        # ğŸ”¥ å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒã‚¹ã‚¯é©ç”¨ - Pythonãƒ«ãƒ¼ãƒ—é™¤å»
        masks = torch.zeros(batch_size, seq_len, device=device, dtype=torch.bool)
        
        # ãƒãƒƒãƒå…¨ä½“ã§ã‚¹ãƒ‘ãƒ³ã‚’ä¸€æ‹¬é©ç”¨
        batch_indices = torch.arange(batch_size, device=device)[:, None]  # [batch, 1]
        span_indices = torch.arange(estimated_spans, device=device)[None, :]  # [1, spans]
        
        # å„ã‚¹ãƒ‘ãƒ³ã«å¯¾ã—ã¦ãƒã‚¹ã‚¯ã‚’é©ç”¨
        for s in range(estimated_spans):
            # é–‹å§‹ãƒ»çµ‚äº†ä½ç½®ã‚’å–å¾—
            starts = start_positions[:, s]  # [batch]
            ends = end_positions[:, s]      # [batch]
            
            # å„ãƒãƒƒãƒã®å„ã‚¹ãƒ‘ãƒ³ã«å¯¾ã—ã¦ç¯„å›²ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ
            max_span_len = (ends - starts).max().item()
            if max_span_len > 0:
                # ç¯„å›²ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ: [batch, max_span_len]
                range_indices = torch.arange(max_span_len, device=device)[None, :] + starts[:, None]
                
                # æœ‰åŠ¹ç¯„å›²ã®ãƒã‚¹ã‚¯ã‚’ä½œæˆ
                valid_mask = (torch.arange(max_span_len, device=device)[None, :] < (ends - starts)[:, None])
                valid_mask = valid_mask & (range_indices < seq_len)
                
                # ãƒã‚¹ã‚¯ã‚’é©ç”¨
                batch_idx = batch_indices[:, 0][:, None].expand(-1, max_span_len)
                masks[batch_idx[valid_mask], range_indices[valid_mask]] = True
        
        # ğŸ”¥ ãƒã‚¹ã‚¯æ•°ã‚’æ­£ç¢ºã«èª¿æ•´ï¼ˆãƒãƒƒãƒä¸¦åˆ—ï¼‰
        masks = self._adjust_mask_count_vectorized(masks, target_masked)
        
        return masks
    
    def _adjust_mask_count_vectorized(self, masks: torch.Tensor, target_masked: int) -> torch.Tensor:
        """ğŸ”¥ ãƒã‚¹ã‚¯æ•°ã‚’æ­£ç¢ºã«èª¿æ•´ - ãƒãƒƒãƒä¸¦åˆ—"""
        batch_size, seq_len = masks.shape
        
        for b in range(batch_size):
            current_masked = masks[b].sum().item()
            
            if current_masked > target_masked:
                # è¶…éåˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è§£é™¤
                masked_indices = torch.where(masks[b])[0]
                excess = current_masked - target_masked
                if excess > 0:
                    remove_indices = masked_indices[torch.randperm(len(masked_indices), generator=self.generator)[:excess]]
                    masks[b, remove_indices] = False
            elif current_masked < target_masked:
                # ä¸è¶³åˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¿½åŠ 
                unmasked_indices = torch.where(~masks[b])[0]
                needed = target_masked - current_masked
                if needed > 0 and len(unmasked_indices) > 0:
                    add_indices = unmasked_indices[torch.randperm(len(unmasked_indices), generator=self.generator)[:needed]]
                    masks[b, add_indices] = True
        
        return masks
    
    def _apply_valid_mask_vectorized(self, masks: torch.Tensor, features: torch.Tensor) -> torch.Tensor:
        """ğŸ”¥ æœ‰åŠ¹ä½ç½®ã®ã¿ãƒã‚¹ã‚¯é©ç”¨ - ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        batch_size, seq_len = masks.shape
        
        # ğŸ”¥ æœ‰åŠ¹ä½ç½®ã‚’ä¸€æ‹¬æ¤œå‡º
        if features.dtype.is_floating_point:
            valid_mask = ~torch.isnan(features[:, :, 0])  # [batch, seq_len]
        else:
            valid_mask = features[:, :, 0] != -1
        
        # ğŸ”¥ æœ‰åŠ¹ä½ç½®ã®ã¿ãƒã‚¹ã‚¯é©ç”¨
        masks = masks & valid_mask
        
        return masks
    
    def apply_mask_to_features_dict(self, features: Dict[str, torch.Tensor], masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        ğŸ”¥ ãƒã‚¹ã‚¯é©ç”¨ - ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        """
        masked_features = {}
        
        for tf_name, tf_features in features.items():
            if tf_name not in masks:
                masked_features[tf_name] = tf_features.clone()
                continue
                
            tf_masks = masks[tf_name]
            batch_size, seq_len, n_features = tf_features.shape
            
            # ğŸ”¥ ãƒã‚¹ã‚¯é©ç”¨ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            masked_tf_features = tf_features.clone()
            
            # ãƒã‚¹ã‚¯ä½ç½®ã‚’ç‰¹å®š [batch, seq_len, 1] -> bool
            mask_expanded = tf_masks.unsqueeze(-1)  # [batch, seq_len, 1]
            
            # ğŸ”¥ mask_tokenã‚’ä¸€æ‹¬é©ç”¨
            mask_token_broadcasted = self.mask_token.expand(batch_size, seq_len, n_features)
            masked_tf_features = torch.where(mask_expanded, mask_token_broadcasted, masked_tf_features)
            
            masked_features[tf_name] = masked_tf_features
        
        return masked_features
    
    # Legacy methods for backward compatibility
    def generate_masks(self, features, seed: int = None, eval_mask_ratio_override: float = None):
        """Legacy wrapper for backward compatibility"""
        if isinstance(features, dict):
            return self.generate_masks_dict(features, seed, eval_mask_ratio_override)
        else:
            # Convert tensor to dict format for vectorized processing
            if features.dim() == 4:
                batch_size, n_tf, seq_len, n_features = features.shape
                features_dict = {f"tf_{i}": features[:, i] for i in range(n_tf)}
            else:
                n_tf, seq_len, n_features = features.shape
                features_dict = {f"tf_{i}": features[i:i+1] for i in range(n_tf)}
            
            masks_dict = self.generate_masks_dict(features_dict, seed, eval_mask_ratio_override)
            
            # Convert back to tensor format
            if features.dim() == 4:
                masks = torch.stack([masks_dict[f"tf_{i}"] for i in range(n_tf)], dim=1)
            else:
                masks = torch.stack([masks_dict[f"tf_{i}"].squeeze(0) for i in range(n_tf)], dim=0)
            
            return masks
    
    def apply_mask_to_features(self, features, masks):
        """Legacy wrapper for backward compatibility"""
        if isinstance(features, dict):
            return self.apply_mask_to_features_dict(features, masks)
        else:
            # Convert to dict format
            if features.dim() == 4:
                batch_size, n_tf, seq_len, n_features = features.shape
                features_dict = {f"tf_{i}": features[:, i] for i in range(n_tf)}
                masks_dict = {f"tf_{i}": masks[:, i] for i in range(n_tf)}
            else:
                n_tf, seq_len, n_features = features.shape
                features_dict = {f"tf_{i}": features[i:i+1] for i in range(n_tf)}
                masks_dict = {f"tf_{i}": masks[i:i+1] for i in range(n_tf)}
            
            masked_dict = self.apply_mask_to_features_dict(features_dict, masks_dict)
            
            # Convert back to tensor format
            if features.dim() == 4:
                masked_features = torch.stack([masked_dict[f"tf_{i}"] for i in range(n_tf)], dim=1)
            else:
                masked_features = torch.stack([masked_dict[f"tf_{i}"].squeeze(0) for i in range(n_tf)], dim=0)
            
            return masked_features