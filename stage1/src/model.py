#!/usr/bin/env python3
"""
Stage 1 Multi-TF Self-Supervised Reconstruction Model
TFå›ºæœ‰CNN + å…±æœ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ + Bottleneck + TFåˆ¥ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# T5è»¢ç§»å­¦ç¿’ç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .lm_adapter import T5TimeSeriesAdapter
    T5_AVAILABLE = True
except ImportError:
    T5TimeSeriesAdapter = None
    T5_AVAILABLE = False

# ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .masking import MaskingStrategy
# ğŸ”¥ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‰ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆ10å€é«˜é€Ÿï¼‰
from .masking_vectorized import VectorizedMaskingStrategy
from .model_vectorized import VectorizedStage1Model


class CrossScaleFusion(nn.Module):
    """Coarseâ†’Fine Cross-Attention for multi-scale fusion"""
    
    def __init__(self, d_model: int, num_heads: int = 8):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        
        self.query_proj = nn.Linear(d_model, d_model)
        self.key_proj = nn.Linear(d_model, d_model)
        self.value_proj = nn.Linear(d_model, d_model)
        self.attn = nn.MultiheadAttention(d_model, num_heads=num_heads, batch_first=True)
        
    def forward(self, coarse_features: torch.Tensor, fine_features: torch.Tensor, 
                coarse_mask: Optional[torch.Tensor] = None, 
                fine_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Cross-scale attention from coarse to fine timeframes
        
        Args:
            coarse_features: [batch, seq_coarse, d_model] - coarse TF features
            fine_features: [batch, seq_fine, d_model] - fine TF features  
            coarse_mask: [batch, seq_coarse] - padding mask for coarse
            fine_mask: [batch, seq_fine] - padding mask for fine
            
        Returns:
            fused_features: [batch, d_model] - CLS token representation
        """
        # Use CLS token from coarse as query
        q = self.query_proj(coarse_features[:, :1])  # [batch, 1, d_model]
        k = self.key_proj(fine_features)  # [batch, seq_fine, d_model] 
        v = self.value_proj(fine_features)  # [batch, seq_fine, d_model]
        
        # Cross-attention
        fused, _ = self.attn(
            q, k, v,
            key_padding_mask=fine_mask  # Only mask fine features
        )
        
        return fused.squeeze(1)  # [batch, d_model]

class TFSpecificStem(nn.Module):
    """TFå›ºæœ‰ã‚¹ãƒ†ãƒ : 1D depth-wise CNN"""
    
    def __init__(self, n_features: int, d_model: int, kernel_size: int = 3):
        super().__init__()
        self.n_features = n_features
        self.d_model = d_model
        
        # Depth-wise convolution
        self.depthwise_conv = nn.Conv1d(
            n_features, n_features, 
            kernel_size=kernel_size, 
            padding=kernel_size//2, 
            groups=n_features
        )
        
        # Point-wise convolution (projection)
        self.pointwise_conv = nn.Conv1d(n_features, d_model, kernel_size=1)
        
        # Normalization & activation
        self.norm = nn.LayerNorm(d_model)
        self.activation = nn.GELU()
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch, seq_len, n_features]
        Returns:
            out: [batch, seq_len, d_model]
        """
        # [batch, seq_len, features] -> [batch, features, seq_len]
        x = x.transpose(1, 2)
        
        # Depth-wise + Point-wise convolution
        x = self.depthwise_conv(x)
        x = self.pointwise_conv(x)
        
        # [batch, d_model, seq_len] -> [batch, seq_len, d_model]
        x = x.transpose(1, 2)
        
        # Normalization & activation
        x = self.norm(x)
        x = self.activation(x)
        
        return x

class CrossScaleAttention(nn.Module):
    """ã‚¯ãƒ­ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«æ³¨æ„æ©Ÿæ§‹"""
    
    def __init__(self, d_model: int, n_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads
        
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.d_head)
        
    def forward(self, tf_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            tf_features: [batch, n_tf, seq_len, d_model]
        Returns:
            attended: [batch, n_tf, seq_len, d_model]
        """
        batch_size, n_tf, seq_len, d_model = tf_features.shape
        
        # Reshape for multi-head attention
        # [batch, n_tf, seq_len, d_model] -> [batch, n_tf*seq_len, d_model]
        x = tf_features.view(batch_size, n_tf * seq_len, d_model)
        
        q = self.q_proj(x).view(batch_size, n_tf * seq_len, self.n_heads, self.d_head).transpose(1, 2)
        k = self.k_proj(x).view(batch_size, n_tf * seq_len, self.n_heads, self.d_head).transpose(1, 2)
        v = self.v_proj(x).view(batch_size, n_tf * seq_len, self.n_heads, self.d_head).transpose(1, 2)
        
        # Scaled dot-product attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        
        out = torch.matmul(attn, v)
        out = out.transpose(1, 2).contiguous().view(batch_size, n_tf * seq_len, d_model)
        out = self.out_proj(out)
        
        # Reshape back
        out = out.view(batch_size, n_tf, seq_len, d_model)
        
        return out

class TransformerEncoderLayer(nn.Module):
    """Transformer ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å±¤ï¼ˆMambaä»£æ›¿ï¼‰"""
    
    def __init__(self, d_model: int, n_heads: int = 8, d_ff: int = None, dropout: float = 0.1):
        super().__init__()
        d_ff = d_ff or 4 * d_model
        
        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.GELU()
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            x: [batch, seq_len, d_model]
            mask: [batch, seq_len] attention mask
        Returns:
            out: [batch, seq_len, d_model]
        """
        # Self-attention
        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=mask)
        x = self.norm1(x + self.dropout(attn_out))
        
        # Feed-forward
        ff_out = self.linear2(self.dropout(self.activation(self.linear1(x))))
        x = self.norm2(x + self.dropout(ff_out))
        
        return x

class SharedEncoder(nn.Module):
    """å…±æœ‰ã‚¯ãƒ­ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, config: dict):
        super().__init__()
        self.d_model = config['model']['encoder']['d_model']
        self.n_layers = config['model']['encoder']['n_layers']
        self.cross_attn_every = config['model']['encoder']['cross_attn_every']
        
        # Transformer layers
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(self.d_model)
            for _ in range(self.n_layers)
        ])
        
        # Cross-scale attention layers
        self.cross_attn_layers = nn.ModuleList([
            CrossScaleAttention(self.d_model)
            for _ in range(self.n_layers // self.cross_attn_every)
        ])
        
    def forward(self, tf_features: torch.Tensor, masks: Optional[torch.Tensor] = None, key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            tf_features: [batch, n_tf, seq_len, d_model] or [batch, seq_len, d_model] (async mode)
            masks: [batch, n_tf, seq_len] padding masks (legacy)
            key_padding_mask: [batch, seq_len] padding mask (async mode)
        Returns:
            encoded: [batch, n_tf, seq_len, d_model] or [batch, seq_len, d_model]
        """
        # async mode support: 3D input
        if tf_features.dim() == 3:
            return self._forward_single_tf(tf_features, key_padding_mask)
        
        # legacy mode: 4D input
        batch_size, n_tf, seq_len, d_model = tf_features.shape
        
        # ğŸ”¥ NaN-paddedã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”¨ã®attention_maskè‡ªå‹•ç”Ÿæˆ
        if masks is None:
            # NaNå€¤ã‚’æ¤œå‡ºã—ã¦padding maskã‚’ä½œæˆ (True=ãƒã‚¹ã‚¯å¯¾è±¡, False=æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿)
            masks = torch.isnan(tf_features).any(dim=-1)  # [batch, n_tf, seq_len]
        
        # Process each TF separately first
        for layer_idx, layer in enumerate(self.layers):
            # Reshape for processing
            x_flat = tf_features.view(batch_size * n_tf, seq_len, d_model)
            mask_flat = masks.view(batch_size * n_tf, seq_len) if masks is not None else None
            
            # Self-attention within each TF
            x_flat = layer(x_flat, mask_flat)
            
            # Reshape back
            tf_features = x_flat.view(batch_size, n_tf, seq_len, d_model)
            
            # Cross-scale attention every N layers
            if (layer_idx + 1) % self.cross_attn_every == 0:
                cross_idx = layer_idx // self.cross_attn_every
                tf_features = tf_features + self.cross_attn_layers[cross_idx](tf_features)
                
        return tf_features
    
    def _forward_single_tf(self, tf_features: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å˜ä¸€TFç”¨forward (async mode)
        
        Args:
            tf_features: [batch, seq_len, d_model]
            key_padding_mask: [batch, seq_len] padding mask (True=valid, False=padded)
        Returns:
            encoded: [batch, seq_len, d_model]
        """
        batch_size, seq_len, d_model = tf_features.shape
        
        # padding maskã®å‡¦ç†
        if key_padding_mask is not None:
            # key_padding_maskã¯é€šå¸¸True=valid, False=paddedã ãŒã€
            # attention maskã¨ã—ã¦ã¯True=masked, False=validãŒå¿…è¦
            attention_mask = key_padding_mask  # ãã®ã¾ã¾ä½¿ç”¨
        else:
            # NaNå€¤ã‚’æ¤œå‡ºã—ã¦padding maskã‚’ä½œæˆ
            attention_mask = torch.isnan(tf_features).any(dim=-1)  # [batch, seq_len]
        
        # Process through transformer layers
        encoded = tf_features
        for layer_idx, layer in enumerate(self.layers):
            # Self-attention within the TF
            encoded = layer(encoded, attention_mask)
            
            # Note: Cross-scale attention is not applied in single TF mode
            
        return encoded

class Bottleneck(nn.Module):
    """Bottleneck: å…¨åŸŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®"""
    
    def __init__(self, d_model: int, latent_len: int, stride: int = 4):
        super().__init__()
        # latent_lenã¯å‚è€ƒå€¤ã®ã¿ï¼ˆå‹•çš„è¨ˆç®—ã§å®Ÿéš›ã®å€¤ã‚’ä½¿ç”¨ï¼‰
        self.latent_len = latent_len  
        self.stride = stride
        
        # Strided convolution for compression
        self.compress = nn.Conv1d(d_model, d_model, kernel_size=stride, stride=stride)
        self.norm = nn.LayerNorm(d_model)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch, n_tf, seq_len, d_model]
        Returns:
            compressed: [batch, n_tf, latent_len, d_model]
        """
        batch_size, n_tf, seq_len, d_model = x.shape
        
        # Reshape and compress
        x = x.view(batch_size * n_tf, seq_len, d_model).transpose(1, 2)  # [batch*n_tf, d_model, seq_len]
        compressed = self.compress(x)  # [batch*n_tf, d_model, latent_len]
        compressed = compressed.transpose(1, 2)  # [batch*n_tf, latent_len, d_model]
        
        # å‹•çš„ã«latent_lenã‚’è¨ˆç®—
        latent_len = compressed.size(1)  # [batch*n_tf, latent_len, d_model]
        
        # Reshape back
        compressed = compressed.view(batch_size, n_tf, latent_len, d_model)
        compressed = self.norm(compressed)
        
        return compressed

class TFDecoder(nn.Module):
    """TFå€‹åˆ¥ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, d_model: int, seq_len: int, n_output_features: int = 4, n_layers: int = 4):
        super().__init__()
        self.seq_len = seq_len
        self.n_output_features = n_output_features
        
        # Upsampling layers
        self.upsample = nn.ConvTranspose1d(d_model, d_model, kernel_size=4, stride=4)
        
        # Decoder layers
        self.decoder_layers = nn.ModuleList([
            nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)
            for _ in range(n_layers)
        ])
        
        # Output projection
        self.output_proj = nn.Conv1d(d_model, n_output_features, kernel_size=1)
        
        self.norm = nn.LayerNorm(d_model)
        self.activation = nn.GELU()
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch, latent_len, d_model]
        Returns:
            decoded: [batch, seq_len, n_output_features]
        """
        batch_size, latent_len, d_model = x.shape
        
        # [batch, latent_len, d_model] -> [batch, d_model, latent_len]
        x = x.transpose(1, 2)
        
        # Upsample (latent_len=1å¯¾å¿œ)
        if latent_len == 1:
            # latent_len=1ã®å ´åˆ: repeatâ†’upsample
            x = x.repeat(1, 1, 16)  # [batch, d_model, 16] ã¸æ‹¡å¼µ
        x = self.upsample(x)
        
        # Decoder layers
        for layer in self.decoder_layers:
            x = layer(x)
            x = F.gelu(x)
            
        # Output projection
        x = self.output_proj(x)
        
        # [batch, n_output_features, seq_len] -> [batch, seq_len, n_output_features]
        x = x.transpose(1, 2)
        
        # Ensure correct length
        if x.size(1) != self.seq_len:
            x = F.interpolate(x.transpose(1, 2), size=self.seq_len, mode='linear', align_corners=False).transpose(1, 2)
            
        return x

class Stage1Model(nn.Module):
    """Stage 1 ãƒãƒ«ãƒTFè‡ªå·±æ•™å¸«ã‚ã‚Šå†æ§‹ç¯‰ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, config: dict):
        super().__init__()
        self.config = config
        
        # Model parameters
        self.n_tf = config['data']['n_timeframes']
        self.seq_len = config['data']['seq_len']
        self.n_features = config['data']['n_features']
        self.d_model = config['model']['tf_stem']['d_model']
        self.latent_len = config['model']['bottleneck']['latent_len']
        
        # Model v2: async mode support
        self.async_sampler = config.get('model', {}).get('async_sampler', False)
        self.timeframes = config.get('data', {}).get('timeframes', ['m1', 'm5', 'm15', 'm30', 'h1', 'h4'])
        
        # TF-specific stems (now using ModuleDict for variable TFs)
        if self.async_sampler:
            # Dict-based stems for variable timeframes
            self.tf_stems = nn.ModuleDict({
                tf: TFSpecificStem(self.n_features, self.d_model, config['model']['tf_stem']['kernel_size'])
                for tf in self.timeframes
            })
        else:
            # List-based stems for fixed timeframes
            self.tf_stems = nn.ModuleList([
                TFSpecificStem(self.n_features, self.d_model, config['model']['tf_stem']['kernel_size'])
                for _ in range(self.n_tf)
            ])
        
        # Shared encoder (T5è»¢ç§»å­¦ç¿’ã¾ãŸã¯ãƒãƒ‹ãƒ©ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼)
        use_pretrained_lm = config.get('transfer_learning', {}).get('use_pretrained_lm', False)
        
        if use_pretrained_lm:
            if not T5_AVAILABLE:
                raise ImportError(
                    "T5è»¢ç§»å­¦ç¿’ãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ãŒã€lm_adapterã¾ãŸã¯transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"
                    "pip install transformers>=4.42.0 ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚"
                )
            print("ğŸ¤— T5è»¢ç§»å­¦ç¿’ã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆå…±æœ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼‰")
            # ğŸ”¥ T5ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¯å¸¸ã«å…±æœ‰ï¼ˆasync_samplerãƒ¢ãƒ¼ãƒ‰ã§ã‚‚ï¼‰
            self.shared_encoder = T5TimeSeriesAdapter(config)
        else:
            print("ğŸ“¦ å¾“æ¥ã®SharedEncoderã‚’ä½¿ç”¨ã—ã¾ã™")
            if self.async_sampler:
                # TF-specific encoders for async mode
                self.encoders = nn.ModuleDict({
                    tf: SharedEncoder(config) for tf in self.timeframes
                })
            else:
                self.shared_encoder = SharedEncoder(config)
        
        # Cross-scale fusion for async mode
        if self.async_sampler:
            cross_pairs = config.get('model', {}).get('cross_pairs', [["h4", "m1"], ["h1", "m1"], ["m30", "m1"]])
            self.cross_fusion = nn.ModuleDict({
                f"{coarse}_{fine}": CrossScaleFusion(self.d_model)
                for coarse, fine in cross_pairs
            })
            self.cross_pairs = cross_pairs
        
        # Bottleneck
        self.bottleneck = Bottleneck(
            self.d_model, 
            self.latent_len, 
            config['model']['bottleneck']['stride']
        )
        
        # TF-specific decoders
        if self.async_sampler:
            # Dict-based decoders for variable timeframes
            self.tf_decoders = nn.ModuleDict({
                tf: TFDecoder(self.d_model, self.seq_len, n_output_features=4, n_layers=config['model']['decoder']['n_layers'])
                for tf in self.timeframes
            })
        else:
            # List-based decoders for fixed timeframes
            self.tf_decoders = nn.ModuleList([
                TFDecoder(self.d_model, self.seq_len, n_output_features=4, n_layers=config['model']['decoder']['n_layers'])
                for _ in range(self.n_tf)
            ])
        
        # ğŸ”¥ Learnable Mask Token Strategy
        # ğŸ”¥ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ä½¿ç”¨ï¼ˆ10å€é«˜é€Ÿï¼‰
        self.masking_strategy = VectorizedMaskingStrategy(config, n_features=self.n_features)
        
        # Positional encoding
        self.pos_encoding = self._create_positional_encoding()
        
    def _create_positional_encoding(self) -> nn.Embedding:
        """ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½œæˆ"""
        return nn.Embedding(self.seq_len, self.d_model)
        
    def forward(self, batch: Dict[str, torch.Tensor], eval_mask_ratio: Optional[float] = None) -> Dict[str, torch.Tensor]:
        """
        Model v2: Dict input support for variable-length multi-timeframe processing
        
        Args:
            batch: {
                'm1': [B, L_m1, F],
                'm5': [B, L_m5, F],
                ...
            } - Dict of timeframe tensors with variable lengths (NaN-padded)
            eval_mask_ratio: Override mask ratio for evaluation (fixes val_corr=0 issue)
            
        Returns:
            outputs: Dict[str, torch.Tensor] - Same keys as input, values are [B, L_tf, 4]
        """
        # ğŸ”¥ å…¥åŠ›å‹ãƒã‚§ãƒƒã‚¯: Tensorã‹Dictã‹ã‚’åˆ¤å®š
        if isinstance(batch, dict):
            # Dictå½¢å¼ -> async_samplerã®è¨­å®šã«å¾“ã†
            if self.async_sampler:
                return self._forward_async(batch, eval_mask_ratio)
            else:
                return self._forward_sync(batch, eval_mask_ratio)
        else:
            # Tensorå½¢å¼ -> å¾“æ¥ã®legacy forward
            return self._forward_legacy_tensor(batch, eval_mask_ratio)
    
    def _forward_async(self, batch: Dict[str, torch.Tensor], eval_mask_ratio: Optional[float] = None) -> Dict[str, torch.Tensor]:
        """Async mode forward pass with variable-length support"""
        encoded = {}
        padding_masks = {}
        training_masks = {}
        
        # 1-A. TFã”ã¨å‰å‡¦ç†ï¼ˆstem â†’ encoderï¼‰
        for tf, x in batch.items():
            # NaN detection for padding mask
            mask = torch.isnan(x[..., 0])  # [B, L] - use first feature to detect NaN
            x_clean = torch.nan_to_num(x, nan=0.0)  # NaN â†’ 0
            
            # Apply self-supervised masking BEFORE stem processing
            if self.training or eval_mask_ratio is not None:
                # Generate training masks for this TF
                mask_ratio = eval_mask_ratio if eval_mask_ratio is not None else 0.15
                tf_training_mask = self._generate_tf_masks(x_clean, mask_ratio)
                # Apply masking to input features (before stem)
                x_masked_input = self._apply_tf_masks(x_clean, tf_training_mask)
                training_masks[tf] = tf_training_mask
            else:
                x_masked_input = x_clean
                training_masks[tf] = torch.zeros_like(mask, dtype=torch.bool)
            
            # TF-specific stem processing (after masking)
            x_stem = self.tf_stems[tf](x_masked_input)  # [B, L, d_model]
            
            # TF-specific encoder ã¾ãŸã¯ å…±æœ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
            if hasattr(self, 'shared_encoder'):
                # T5ã¾ãŸã¯å…±æœ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨
                encoded_features = self.shared_encoder(x_stem, key_padding_mask=mask)
            else:
                # TFå›ºæœ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ï¼ˆéT5ãƒ¢ãƒ¼ãƒ‰ï¼‰
                encoded_features = self.encoders[tf](x_stem, key_padding_mask=mask)
            encoded[tf] = encoded_features  # [B, L, d_model]
            padding_masks[tf] = mask
        
        # 1-B. Cross-scale Fusion (coarseâ†’fine)
        fused_cls = {}
        if hasattr(self, 'cross_fusion') and self.cross_fusion:
            for coarse, fine in self.cross_pairs:
                if coarse in encoded and fine in encoded:
                    key = f"{coarse}_{fine}"
                    if key in self.cross_fusion:
                        fused_cls[fine] = self.cross_fusion[key](
                            encoded[coarse], encoded[fine],
                            padding_masks[coarse], padding_masks[fine]
                        )
        
        # 1-C. Bottleneck + Decoder  
        outputs = {}
        for tf, z in encoded.items():
            # Async modeç”¨ã®ä¿®æ­£ã•ã‚ŒãŸBottleneckå‡¦ç†
            # z: [B, L, d_model] â†’ mean-pooling â†’ [B, d_model] â†’ MLP â†’ [B, latent_len, d_model]
            
            valid_mask = ~padding_masks[tf]  # [B, L]
            if valid_mask.sum() > 0:
                # Mean pooling over valid positions
                z_pool = (z * valid_mask.unsqueeze(-1)).sum(dim=1) / valid_mask.sum(dim=1, keepdim=True)  # [B, d_model]
            else:
                z_pool = z.mean(dim=1)  # Fallback to regular mean
            
            # Async modeç”¨ã®ç°¡ç•¥åŒ–bottleneck: ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆMLP
            batch_size = z_pool.size(0)
            # z_pool: [B, d_model] â†’ [B, 1, d_model] (å˜ä¸€latent)
            z_latent = z_pool.unsqueeze(1)  # [B, 1, d_model]
            
            # TF-specific decoder (latent_len=1ã¨ã—ã¦å‡¦ç†)
            outputs[tf] = self.tf_decoders[tf](z_latent)  # [B, seq_len, 4]
        
        return outputs
    
    def _forward_sync(self, batch: Dict[str, torch.Tensor], eval_mask_ratio: Optional[float] = None) -> Dict[str, torch.Tensor]:
        """Legacy sync mode forward pass - convert Dict to tensor format"""
        # Convert Dict format to legacy tensor format for backward compatibility
        timeframes = list(batch.keys())
        
        # Find maximum sequence length and batch size
        batch_size = list(batch.values())[0].shape[0]
        max_seq_len = max(x.shape[1] for x in batch.values())
        n_features = list(batch.values())[0].shape[2]
        n_tf = len(timeframes)
        
        # Pad all sequences to max length and stack
        features = torch.full((batch_size, n_tf, max_seq_len, n_features), 
                            float('nan'), device=list(batch.values())[0].device)
        
        for i, (tf, x) in enumerate(batch.items()):
            seq_len = x.shape[1]
            features[:, i, :seq_len, :] = x
        
        # Use legacy forward logic
        # ğŸ”¥ è‡ªå·±æ•™å¸«ã‚ã‚Šãƒã‚¹ã‚­ãƒ³ã‚°å‡¦ç†
        training_masks = None
        if self.training or eval_mask_ratio is not None:
            mask_ratio = eval_mask_ratio if eval_mask_ratio is not None else 0.15
            training_masks = torch.stack([
                self.masking_strategy.generate_masks(
                    features[b], 
                    seed=hash((id(features), b)) % 2147483647,
                    mask_ratio=mask_ratio
                ) 
                for b in range(batch_size)
            ], dim=0)
        
        # Apply masking
        if training_masks is not None:
            masked_features = torch.stack([
                self.masking_strategy.apply_mask_to_features(features[b], training_masks[b])
                for b in range(batch_size)
            ], dim=0)
        else:
            masked_features = features
            training_masks = torch.zeros(batch_size, n_tf, max_seq_len, device=features.device, dtype=torch.bool)
        
        # Process through model
        if hasattr(self, 'shared_encoder'):
            if isinstance(self.shared_encoder, T5TimeSeriesAdapter):
                encoded = self.shared_encoder(masked_features)
            else:
                tf_embeddings = []
                for i in range(n_tf):
                    stem_out = self.tf_stems[i](masked_features[:, i])
                    tf_embeddings.append(stem_out)
                tf_embeddings = torch.stack(tf_embeddings, dim=1)
                
                pos_ids = torch.arange(max_seq_len, device=masked_features.device).unsqueeze(0).expand(batch_size, -1)
                pos_emb = self.pos_encoding(pos_ids).unsqueeze(1)
                tf_embeddings = tf_embeddings + pos_emb
                
                encoded = self.shared_encoder(tf_embeddings)
        
        compressed = self.bottleneck(encoded)
        
        reconstructed = []
        for i in range(n_tf):
            decoded = self.tf_decoders[i](compressed[:, i])
            reconstructed.append(decoded)
        reconstructed = torch.stack(reconstructed, dim=1)
        
        # Convert back to Dict format
        outputs = {}
        for i, tf in enumerate(timeframes):
            seq_len = batch[tf].shape[1]
            outputs[tf] = reconstructed[:, i, :seq_len, :]
        
        return outputs
    
    def _forward_legacy_tensor(self, batch: torch.Tensor, eval_mask_ratio: Optional[float] = None) -> torch.Tensor:
        """Legacy tensor format forward pass"""
        # å¾“æ¥ã®ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ã§ã®å‡¦ç†
        features = batch  # [batch, n_tf, seq_len, n_features]
        batch_size, n_tf, seq_len, n_features = features.shape
        
        # ğŸ”¥ è‡ªå·±æ•™å¸«ã‚ã‚Šãƒã‚¹ã‚­ãƒ³ã‚°å‡¦ç†
        training_masks = None
        if self.training or eval_mask_ratio is not None:
            mask_ratio = eval_mask_ratio if eval_mask_ratio is not None else 0.15
            training_masks = torch.stack([
                self.masking_strategy.generate_masks(
                    features[b], 
                    seed=hash((id(features), b)) % 2147483647,
                    eval_mask_ratio_override=mask_ratio
                ) 
                for b in range(batch_size)
            ], dim=0)
        
        # Apply masking
        if training_masks is not None:
            masked_features = torch.stack([
                self.masking_strategy.apply_mask_to_features(features[b], training_masks[b])
                for b in range(batch_size)
            ], dim=0)
        else:
            masked_features = features
            training_masks = torch.zeros(batch_size, n_tf, seq_len, device=features.device, dtype=torch.bool)
        
        # Process through model (legacy logic)
        if hasattr(self, 'shared_encoder'):
            if isinstance(self.shared_encoder, T5TimeSeriesAdapter):
                encoded = self.shared_encoder(masked_features)
            else:
                tf_embeddings = []
                for i in range(n_tf):
                    stem_out = self.tf_stems[i](masked_features[:, i])
                    tf_embeddings.append(stem_out)
                tf_embeddings = torch.stack(tf_embeddings, dim=1)
                
                pos_ids = torch.arange(seq_len, device=masked_features.device).unsqueeze(0).expand(batch_size, -1)
                pos_emb = self.pos_encoding(pos_ids).unsqueeze(1)
                encoded = tf_embeddings + pos_emb
                encoded = self.shared_encoder(encoded.view(batch_size, -1, encoded.shape[-1]))
        else:
            # Fallback to individual processing
            tf_embeddings = []
            for i in range(n_tf):
                stem_out = self.tf_stems[i](masked_features[:, i])
                tf_embeddings.append(stem_out)
            encoded = torch.stack(tf_embeddings, dim=1)
        
        # Decode
        reconstructed = []
        for i in range(n_tf):
            if hasattr(self, 'tf_decoders') and i < len(self.tf_decoders):
                decoder_out = self.tf_decoders[i](encoded[:, i])
            else:
                decoder_out = encoded[:, i]
            reconstructed.append(decoder_out)
        
        reconstructed = torch.stack(reconstructed, dim=1)
        return reconstructed
    
    def _generate_tf_masks(self, x: torch.Tensor, mask_ratio: float = 0.15) -> torch.Tensor:
        """Generate self-supervised masks for a single TF"""
        batch_size, seq_len, _ = x.shape
        
        # Simple random masking for now
        masks = torch.rand(batch_size, seq_len, device=x.device) < mask_ratio
        return masks
    
    def _apply_tf_masks(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """Apply masks to features using learnable mask tokens"""
        # Apply mask (set masked positions to 0 or use learnable mask token)
        masked_x = x.clone()
        if hasattr(self.masking_strategy, 'mask_token'):
            # Use learnable mask token
            mask_token = self.masking_strategy.mask_token.expand_as(x)
            masked_x = torch.where(mask.unsqueeze(-1), mask_token, x)
        else:
            # Simple masking to zero
            masked_x = torch.where(mask.unsqueeze(-1), torch.zeros_like(x), x)
        
        return masked_x
        
    def get_model_info(self) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024 * 1024),  # float32 assumption
            'architecture': {
                'n_tf': self.n_tf,
                'seq_len': self.seq_len,
                'n_features': self.n_features,
                'd_model': self.d_model,
                'latent_len': f'dynamic({self.seq_len // self.bottleneck.stride})'
            }
        }

# ğŸ”¥ ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼é–¢æ•°: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‰ˆã‚’å„ªå…ˆä½¿ç”¨
def create_stage1_model(config: dict, use_vectorized: bool = True):
    """
    Stage1ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‰ˆã‚’å„ªå…ˆï¼‰
    
    Args:
        config: ãƒ¢ãƒ‡ãƒ«è¨­å®š
        use_vectorized: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‰ˆã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆTrue=10å€é«˜é€Ÿã€False=å¾“æ¥ç‰ˆï¼‰
        
    Returns:
        model: Stage1ãƒ¢ãƒ‡ãƒ«
    """
    if use_vectorized:
        print("âš¡ ãƒ™ã‚¯ãƒˆãƒ«åŒ–Stage1ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆ10å€é«˜é€Ÿï¼‰")
        return VectorizedStage1Model(config)
    else:
        print("ğŸ“¦ å¾“æ¥ã®Stage1ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
        return Stage1Model(config)