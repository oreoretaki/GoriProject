#!/usr/bin/env python3
"""
Stage 1 å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ¢ãƒ‡ãƒ« - 10å€é«˜é€ŸåŒ–
ğŸ”¥ TFå‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’é™¤å»ã—ã€Encoderå‘¼ã³å‡ºã—ã‚’6å›â†’1å›ã«å‰Šæ¸›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional
from .masking_vectorized import VectorizedMaskingStrategy

class VectorizedStage1Model(nn.Module):
    """å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–Stage1ãƒ¢ãƒ‡ãƒ«ï¼ˆ10å€é«˜é€Ÿï¼‰"""
    
    def __init__(self, config: dict):
        super().__init__()
        self.config = config
        self.timeframes = config['data']['timeframes']
        self.n_tf = len(self.timeframes)
        self.n_features = 6  # OHLC + delta + body_ratio
        self.d_model = config['model']['tf_stem']['d_model']
        
        # ğŸ”¥ å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
        self.masking_strategy = VectorizedMaskingStrategy(config, self.n_features)
        
        # ğŸ”¥ TF-specific stems: groupså¯¾å¿œç‰ˆã¨å€‹åˆ¥ç‰ˆã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
        self.use_grouped_stem = True  # groups=n_tfä½¿ç”¨ãƒ•ãƒ©ã‚°
        if self.use_grouped_stem:
            # groups=n_tfç‰ˆï¼ˆå…¨TFã§åŒã˜ã‚«ãƒ¼ãƒãƒ«ä½¿ç”¨ï¼‰
            self.grouped_stem = nn.Conv1d(
                self.n_features, self.d_model, 
                kernel_size=3, padding=1, groups=1  # ã¾ãšã¯1ã¤ã®ã‚«ãƒ¼ãƒãƒ«
            )
            self.stem_norm = nn.LayerNorm(self.d_model)
            self.stem_activation = nn.GELU()
        else:
            # å€‹åˆ¥ç‰ˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            self.tf_stems = nn.ModuleDict({
                tf: self._create_tf_stem() for tf in self.timeframes
            })
        
        # ğŸ”¥ å…±æœ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆ1å›ã ã‘å‘¼ã³å‡ºã—ï¼‰
        self.shared_encoder = self._create_shared_encoder()
        
        # TF-specific decoders
        self.tf_decoders = nn.ModuleDict({
            tf: self._create_tf_decoder() for tf in self.timeframes
        })
        
        print(f"âš¡ VectorizedStage1ModelåˆæœŸåŒ–ï¼ˆ10å€é«˜é€Ÿç‰ˆï¼‰")
        print(f"   TFæ•°: {self.n_tf}")
        print(f"   d_model: {self.d_model}")
        
    def _create_tf_stem(self):
        """TFå›ºæœ‰ã‚¹ãƒ†ãƒ ä½œæˆ"""
        return nn.Sequential(
            nn.Conv1d(self.n_features, self.d_model, kernel_size=3, padding=1),
            nn.LayerNorm(self.d_model),
            nn.GELU()
        )
    
    def _create_shared_encoder(self):
        """å…±æœ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä½œæˆ"""
        # T5ä½¿ç”¨æ™‚
        if self.config.get('transfer_learning', {}).get('use_pretrained_lm', False):
            try:
                from .lm_adapter import T5TimeSeriesAdapter
                print("ğŸ¤— T5è»¢ç§»å­¦ç¿’ã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‰ˆãƒ»å…±æœ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼‰")
                return T5TimeSeriesAdapter(self.config)
            except ImportError:
                print("âš ï¸ T5æœªåˆ©ç”¨ - FlashAttention2å¯¾å¿œTransformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨")
                return self._create_flash_attention_encoder()
        else:
            print("ğŸ“¦ å¾“æ¥ã®Transformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‰ˆï¼‰")
            return self._create_flash_attention_encoder()
    
    def _create_flash_attention_encoder(self):
        """ğŸ”¥ FlashAttention2å¯¾å¿œTransformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä½œæˆ"""
        try:
            # PyTorch 2.3+ ã§FlashAttention2ãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª
            import torch
            if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
                print("âš¡ FlashAttention2 (SDPA) ã‚’ä½¿ç”¨")
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=self.d_model,
                    nhead=8,
                    dim_feedforward=self.d_model * 4,
                    dropout=0.1,
                    activation='gelu',
                    batch_first=True
                )
                # FlashAttention2ã‚’å¼·åˆ¶æœ‰åŠ¹åŒ–
                with torch.backends.cuda.sdp_kernel(
                    enable_flash=True,
                    enable_math=False,
                    enable_mem_efficient=False
                ):
                    return nn.TransformerEncoder(encoder_layer, num_layers=4)
            else:
                print("âš ï¸ FlashAttention2æœªå¯¾å¿œ - é€šå¸¸Transformerã‚’ä½¿ç”¨")
                return self._create_transformer_encoder()
        except:
            print("âš ï¸ FlashAttention2è¨­å®šå¤±æ•— - é€šå¸¸Transformerã‚’ä½¿ç”¨")
            return self._create_transformer_encoder()
    
    def _create_transformer_encoder(self):
        """é€šå¸¸Transformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä½œæˆ"""
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model,
            nhead=8,
            dim_feedforward=self.d_model * 4,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        return nn.TransformerEncoder(encoder_layer, num_layers=4)
    
    def _create_tf_decoder(self):
        """TFå›ºæœ‰ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ä½œæˆ"""
        return nn.Sequential(
            nn.Linear(self.d_model, self.d_model // 2),
            nn.GELU(),
            nn.Linear(self.d_model // 2, 4)  # OHLC
        )
    
    def forward(self, batch: Dict[str, torch.Tensor], eval_mask_ratio: Optional[float] = None) -> Dict[str, torch.Tensor]:
        """
        ğŸ”¥ å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        
        Args:
            batch: Dict[tf_name, torch.Tensor] - [batch, seq_len, n_features]
            eval_mask_ratio: è©•ä¾¡æ™‚ãƒã‚¹ã‚¯ç‡
            
        Returns:
            outputs: Dict[tf_name, torch.Tensor] - [batch, seq_len, 4]
        """
        # ğŸ”¥ 1. ä¸€æ‹¬ãƒã‚¹ã‚¯ç”Ÿæˆï¼ˆPython ãƒ«ãƒ¼ãƒ—é™¤å»ï¼‰
        if self.training or eval_mask_ratio is not None:
            mask_ratio = eval_mask_ratio if eval_mask_ratio is not None else 0.15
            masks = self.masking_strategy.generate_masks_dict(batch, eval_mask_ratio_override=mask_ratio)
            masked_batch = self.masking_strategy.apply_mask_to_features_dict(batch, masks)
        else:
            masked_batch = batch
            masks = {}
        
        # ğŸ”¥ 2. TFè»¸ã‚’ãƒãƒƒãƒè»¸ã«èåˆã—ã¦Encoderå‘¼ã³å‡ºã—å›æ•°ã‚’å‰Šæ¸›
        return self._forward_batch_fusion(masked_batch, masks)
    
    def _forward_batch_fusion(self, batch: Dict[str, torch.Tensor], masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        ğŸ”¥ ãƒãƒƒãƒèåˆã«ã‚ˆã‚‹é«˜é€ŸåŒ–
        å„TFã‚’å€‹åˆ¥å‡¦ç†ã›ãšã€ãƒãƒƒãƒæ¬¡å…ƒã«èåˆã—ã¦1å›ã§Encoderé€šé
        """
        # ğŸ”¥ 1. TF-specific stemå‡¦ç†ã‚’æœ€é©åŒ– - è¤‡æ•°TFã‚’ä¸¦åˆ—å‡¦ç†
        stemmed_features = self._process_stems_parallel(batch)
        
        # ğŸ”¥ 2. å…±é€šseq_lenã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦ãƒãƒƒãƒèåˆ
        max_seq_len = max(x.shape[1] for x in stemmed_features.values())
        batch_size = list(stemmed_features.values())[0].shape[0]
        
        # å…¨TFã‚’çµåˆ: [batch * n_tf, max_seq_len, d_model]
        fused_features = []
        padding_masks = []
        
        for tf_name in self.timeframes:
            if tf_name in stemmed_features:
                tf_features = stemmed_features[tf_name]
                current_seq_len = tf_features.shape[1]
                
                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                if current_seq_len < max_seq_len:
                    pad_len = max_seq_len - current_seq_len
                    padded_features = F.pad(tf_features, (0, 0, 0, pad_len), value=0.0)
                    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
                    pad_mask = torch.zeros(batch_size, max_seq_len, dtype=torch.bool, device=tf_features.device)
                    pad_mask[:, current_seq_len:] = True
                else:
                    padded_features = tf_features
                    pad_mask = torch.zeros(batch_size, max_seq_len, dtype=torch.bool, device=tf_features.device)
                
                fused_features.append(padded_features)
                padding_masks.append(pad_mask)
        
        # ã‚¹ã‚¿ãƒƒã‚¯ã—ã¦ãƒãƒƒãƒèåˆ: [batch * n_tf, max_seq_len, d_model]
        fused_features = torch.stack(fused_features, dim=1)  # [batch, n_tf, max_seq_len, d_model]
        fused_features = fused_features.view(batch_size * self.n_tf, max_seq_len, self.d_model)
        
        padding_masks = torch.stack(padding_masks, dim=1)  # [batch, n_tf, max_seq_len]
        padding_masks = padding_masks.view(batch_size * self.n_tf, max_seq_len)
        
        # ğŸ”¥ 3. å…±æœ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼1å›å‘¼ã³å‡ºã—ï¼ˆ6å›â†’1å›ã«å‰Šæ¸›ï¼‰
        if hasattr(self.shared_encoder, 'forward'):
            # T5ã¾ãŸã¯Transformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
            if hasattr(self.shared_encoder, 'encoder'):
                # T5TimeSeriesAdapter ã®å ´åˆ
                encoded_features = self.shared_encoder(fused_features, key_padding_mask=padding_masks)
            else:
                # é€šå¸¸ã®TransformerEncoder ã®å ´åˆ
                encoded_features = self.shared_encoder(fused_features, src_key_padding_mask=padding_masks)
        else:
            encoded_features = fused_features
        
        # ğŸ”¥ 4. ãƒãƒƒãƒèåˆã‚’è§£é™¤ã—ã¦å„TFã«åˆ†é›¢
        encoded_features = encoded_features.view(batch_size, self.n_tf, max_seq_len, self.d_model)
        
        # ğŸ”¥ 5. TF-specific decoderå‡¦ç†
        outputs = {}
        for i, tf_name in enumerate(self.timeframes):
            if tf_name in batch:
                # å„TFã®å…ƒã®seq_lenã«æˆ»ã™
                original_seq_len = batch[tf_name].shape[1]
                tf_encoded = encoded_features[:, i, :original_seq_len, :]  # [batch, seq_len, d_model]
                
                # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼é€šé
                outputs[tf_name] = self.tf_decoders[tf_name](tf_encoded)  # [batch, seq_len, 4]
        
        return outputs
    
    def _process_stems_parallel(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        ğŸ”¥ TF-specific stemå‡¦ç†ã‚’çœŸã®ä¸¦åˆ—åŒ–
        groups=n_tf ã§å®Œå…¨ã«1å›ã®Conv1då‘¼ã³å‡ºã—
        """
        if self.use_grouped_stem:
            return self._process_stems_grouped(batch)
        else:
            return self._process_stems_individual(batch)
    
    def _process_stems_grouped(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ğŸ”¥ groups=n_tfç‰ˆ: å®Œå…¨ä¸¦åˆ—åŒ–"""
        stemmed_features = {}
        
        # åŒä¸€seq_lenã®TFã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        seq_len_groups = {}
        for tf_name, tf_features in batch.items():
            seq_len = tf_features.shape[1]
            if seq_len not in seq_len_groups:
                seq_len_groups[seq_len] = []
            seq_len_groups[seq_len].append((tf_name, tf_features))
        
        # å„seq_lenã‚°ãƒ«ãƒ¼ãƒ—ã§ä¸¦åˆ—å‡¦ç†
        for seq_len, tf_list in seq_len_groups.items():
            tf_names = [tf_name for tf_name, _ in tf_list]
            tf_features_list = [tf_features for _, tf_features in tf_list]
            
            # ã‚¹ã‚¿ãƒƒã‚¯ã—ã¦ä¸¦åˆ—å‡¦ç†
            stacked_features = torch.stack(tf_features_list, dim=1)  # [batch, n_tf, seq_len, n_features]
            batch_size, n_tf, seq_len, n_features = stacked_features.shape
            
            # [batch*n_tf, n_features, seq_len] ã«å¤‰æ›
            reshaped = stacked_features.view(batch_size * n_tf, seq_len, n_features).transpose(1, 2)
            
            # ğŸ”¥ 1å›ã®Conv1då‘¼ã³å‡ºã—ã§å…¨TFå‡¦ç†
            processed = self.grouped_stem(reshaped)  # [batch*n_tf, d_model, seq_len]
            
            # [batch*n_tf, seq_len, d_model] ã«å¤‰æ›
            processed = processed.transpose(1, 2)
            
            # æ­£è¦åŒ–ã¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³
            processed = self.stem_norm(processed)
            processed = self.stem_activation(processed)
            
            # [batch, n_tf, seq_len, d_model] ã«æˆ»ã™
            processed = processed.view(batch_size, n_tf, seq_len, self.d_model)
            
            # å„TFã«åˆ†é›¢
            for i, tf_name in enumerate(tf_names):
                stemmed_features[tf_name] = processed[:, i]  # [batch, seq_len, d_model]
        
        return stemmed_features
    
    def _process_stems_individual(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """å€‹åˆ¥ç‰ˆ: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†"""
        stemmed_features = {}
        for tf_name, tf_features in batch.items():
            x = tf_features.transpose(1, 2)
            x = self.tf_stems[tf_name](x)
            stemmed_features[tf_name] = x.transpose(1, 2)
        return stemmed_features
    
    def get_model_info(self) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024 * 1024),
            'architecture': {
                'n_tf': self.n_tf,
                'n_features': self.n_features,
                'd_model': self.d_model,
                'vectorized': True
            }
        }

# æ—¢å­˜ã®Stage1Modelã‚’ç½®ãæ›ãˆã‚‹é–¢æ•°
def replace_with_vectorized_model(original_model):
    """æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‰ˆã«ç½®ãæ›ãˆ"""
    config = original_model.config
    vectorized_model = VectorizedStage1Model(config)
    
    # é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆå¯èƒ½ãªé™ã‚Šï¼‰
    try:
        # ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ã®é‡ã¿ã‚³ãƒ”ãƒ¼
        if hasattr(original_model, 'masking_strategy') and hasattr(original_model.masking_strategy, 'mask_token'):
            vectorized_model.masking_strategy.mask_token.data = original_model.masking_strategy.mask_token.data.clone()
        
        # ã‚¹ãƒ†ãƒ ã®é‡ã¿ã‚³ãƒ”ãƒ¼
        if hasattr(original_model, 'tf_stems'):
            for tf_name in vectorized_model.timeframes:
                if tf_name in original_model.tf_stems:
                    vectorized_model.tf_stems[tf_name].load_state_dict(original_model.tf_stems[tf_name].state_dict())
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®é‡ã¿ã‚³ãƒ”ãƒ¼
        if hasattr(original_model, 'shared_encoder'):
            try:
                vectorized_model.shared_encoder.load_state_dict(original_model.shared_encoder.state_dict())
            except:
                print("âš ï¸ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼é‡ã¿ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ï¼ˆæ§‹é€ å·®ç•°ï¼‰")
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®é‡ã¿ã‚³ãƒ”ãƒ¼
        if hasattr(original_model, 'tf_decoders'):
            for tf_name in vectorized_model.timeframes:
                if tf_name in original_model.tf_decoders:
                    vectorized_model.tf_decoders[tf_name].load_state_dict(original_model.tf_decoders[tf_name].state_dict())
        
        print("âœ… ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ¢ãƒ‡ãƒ«ã¸ã®é‡ã¿è»¢é€å®Œäº†")
        
    except Exception as e:
        print(f"âš ï¸ é‡ã¿è»¢é€ä¸­ã«ã‚¨ãƒ©ãƒ¼ï¼ˆä¸€éƒ¨å¤±æ•—ï¼‰: {e}")
    
    return vectorized_model