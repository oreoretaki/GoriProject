#!/usr/bin/env python3
"""
Stage 1 æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
é«˜é€ŸåŒ–æœ€é©åŒ–ä»˜ãDataLoaderå®Ÿè£…
"""

import torch
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
from typing import Dict, Tuple, Optional
import os
from pathlib import Path

from .window_sampler import MultiTFWindowSampler
from .feature_engineering import FeatureEngineer
from .normalization import TFNormalizer

class Stage1Dataset(Dataset):
    """Stage 1 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, data_dir: str, config: dict, split: str = "train"):
        self.data_dir = Path(data_dir)
        self.config = config
        self.split = split
        
        # configã«data_dirã‚’è¨­å®š
        self.config['data']['data_dir'] = str(self.data_dir)
        
        print(f"ğŸ”„ Stage 1 DatasetåˆæœŸåŒ– ({split})")
        print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {data_dir}")
        
        # TFãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.tf_data = self._load_tf_data()
        
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        self.feature_engineer = FeatureEngineer(config)
        
        # æ­£è¦åŒ–
        self.normalizer = TFNormalizer(
            config=config,
            cache_stats=True
        )
        
        # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆtrainå°‚ç”¨çµ±è¨ˆå„ªå…ˆï¼‰
        try:
            if split == "train":
                # trainã®å ´åˆã€ã¾ãštrainå°‚ç”¨çµ±è¨ˆã‚’è©¦è¡Œ
                try:
                    self.normalizer.load_stats(split="train")
                    print(f"   ğŸ“Š trainå°‚ç”¨çµ±è¨ˆã‚’ãƒ­ãƒ¼ãƒ‰")
                except FileNotFoundError:
                    print(f"   ğŸ“Š trainå°‚ç”¨çµ±è¨ˆã‚’æ–°è¦è¨ˆç®—ä¸­...")
                    self.normalizer.fit(self.tf_data)
                    self.normalizer.save_stats(split="train")
            else:
                # valã®å ´åˆã€trainå°‚ç”¨çµ±è¨ˆã‚’èª­ã¿è¾¼ã¿
                try:
                    self.normalizer.load_stats(split="train")
                    print(f"   ğŸ“Š trainå°‚ç”¨çµ±è¨ˆã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆvalç”¨ï¼‰")
                except FileNotFoundError:
                    print(f"   âš ï¸ trainå°‚ç”¨çµ±è¨ˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨æœŸé–“çµ±è¨ˆã‚’ä½¿ç”¨")
                    self.normalizer.load_stats()
        except FileNotFoundError:
            print(f"   ğŸ“Š æ­£è¦åŒ–çµ±è¨ˆã‚’æ–°è¦è¨ˆç®—ä¸­...")
            self.normalizer.fit(self.tf_data)
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–+ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
        cache_dir = self.data_dir / "cache"
        self.window_sampler = MultiTFWindowSampler(
            tf_data=self.tf_data,
            seq_len=config['data']['seq_len'],
            split=split,
            val_split=config['validation']['val_split'],
            min_coverage=0.8,
            cache_dir=str(cache_dir),
            val_gap_days=config['validation'].get('val_gap_days', 1.0)
        )
        
        # æ³¨æ„ï¼šãƒã‚¹ã‚­ãƒ³ã‚°ã¯ãƒ¢ãƒ‡ãƒ«å†…ã§å®Ÿè¡Œï¼ˆdata_loaderå´ã§ã¯ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼‰
        
    def _load_tf_data(self) -> Dict[str, pd.DataFrame]:
        """TFãƒ‡ãƒ¼ã‚¿ã‚’é«˜é€Ÿèª­ã¿è¾¼ã¿"""
        tf_data = {}
        timeframes = self.config['data']['timeframes']
        
        print(f"   æ™‚é–“è¶³: {timeframes}")
        print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {self.config['data']['seq_len']}")
        
        for tf in timeframes:
            file_path = self.data_dir / f"simple_gap_aware_{tf}.parquet"
            
            if file_path.exists():
                # Parqueté«˜é€Ÿèª­ã¿è¾¼ã¿
                df = pd.read_parquet(file_path)
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿®æ­£: M1ä»¥å¤–ã¯timestampåˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ä½¿ç”¨
                if tf == 'm1':
                    df.index = pd.to_datetime(df.index)
                else:
                    # timestampåˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š
                    if 'timestamp' in df.columns:
                        df.index = pd.to_datetime(df['timestamp'])
                        df = df.drop('timestamp', axis=1)  # timestampåˆ—ã‚’å‰Šé™¤
                    else:
                        df.index = pd.to_datetime(df.index)
                
                # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€ (UTCã«çµ±ä¸€)
                if df.index.tz is None:
                    df.index = df.index.tz_localize('UTC')
                elif str(df.index.tz) != 'UTC':
                    df.index = df.index.tz_convert('UTC')
                
                tf_data[tf] = df
                print(f"   {tf.upper()}: {len(df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰, æœŸé–“: {df.index[0]} - {df.index[-1]}")
            else:
                raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
                
        return tf_data
    
    def __len__(self) -> int:
        return len(self.window_sampler)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒƒãƒå–å¾—"""
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ‡ãƒ¼ã‚¿å–å¾—
        window_data = self.window_sampler[idx]
        
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        features, targets = self.feature_engineer.process_window(window_data)
        
        # æ­£è¦åŒ–ï¼ˆfeaturesç”¨ï¼‰
        features = self.normalizer.normalize(features)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”¨ã®æ­£è¦åŒ–ï¼ˆOHLCç”¨ï¼‰
        targets = self.normalizer.normalize_targets(targets)
        
        # æ—¢ã«ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ï¼ˆBF16äº’æ›ï¼‰
        features_tensor = features.to(torch.float32)
        targets_tensor = targets.to(torch.float32)
        
        # ğŸ”¥ ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆãƒã‚¹ã‚­ãƒ³ã‚°ã¯ãƒ¢ãƒ‡ãƒ«å†…ã§å®Ÿè¡Œï¼‰
        return {
            'features': features_tensor,  # ç”Ÿã®ç‰¹å¾´é‡ï¼ˆãƒã‚¹ã‚¯ãªã—ï¼‰
            'targets': targets_tensor
        }

def create_stage1_dataloaders(data_dir: str, config: dict) -> Tuple[DataLoader, DataLoader]:
    """æœ€é©åŒ–ã•ã‚ŒãŸDataLoaderä½œæˆ"""
    
    # DataLoaderè¨­å®šå–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä»˜ãï¼‰
    dataloader_config = config.get('dataloader', {})
    batch_size = config['training']['batch_size']
    
    # æœ€é©åŒ–è¨­å®š
    dataloader_kwargs = {
        'batch_size': batch_size,
        'num_workers': dataloader_config.get('num_workers', 8),
        'pin_memory': dataloader_config.get('pin_memory', True),
        'persistent_workers': dataloader_config.get('persistent_workers', True),
        'prefetch_factor': dataloader_config.get('prefetch_factor', 4),
        'drop_last': True,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºçµ±ä¸€
    }
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    train_dataset = Stage1Dataset(data_dir, config, split="train")
    val_dataset = Stage1Dataset(data_dir, config, split="val")
    
    # DataLoaderä½œæˆ
    train_loader = DataLoader(
        train_dataset, 
        shuffle=True,
        **dataloader_kwargs
    )
    
    val_loader = DataLoader(
        val_dataset,
        shuffle=False,
        **dataloader_kwargs
    )
    
    print(f"ğŸ“Š DataLoaderä½œæˆå®Œäº†")
    print(f"   è¨“ç·´: {len(train_loader)}ãƒãƒƒãƒ ({len(train_dataset)}ã‚µãƒ³ãƒ—ãƒ«)")
    print(f"   æ¤œè¨¼: {len(val_loader)}ãƒãƒƒãƒ ({len(val_dataset)}ã‚µãƒ³ãƒ—ãƒ«)")
    print(f"   æœ€é©åŒ–: num_workers={dataloader_kwargs['num_workers']}, prefetch={dataloader_kwargs['prefetch_factor']}")
    
    return train_loader, val_loader