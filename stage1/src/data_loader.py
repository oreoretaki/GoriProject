#!/usr/bin/env python3
"""
Stage 1 æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
é«˜é€ŸåŒ–æœ€é©åŒ–ä»˜ãDataLoaderå®Ÿè£…
"""

import torch
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
from typing import Dict, Tuple, Optional, List
import os
from pathlib import Path
from collections import defaultdict

from .window_sampler import MultiTFWindowSampler
from .feature_engineering import FeatureEngineer
from .normalization import TFNormalizer


def collate_multiscale(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
    """
    å¯å¤‰é•·ãƒãƒƒãƒã®ãŸã‚ã® collate é–¢æ•°ï¼ˆModel v2å¯¾å¿œï¼‰
    å„TFã§ç•°ãªã‚‹é•·ã•ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦çµ±ä¸€
    
    Args:
        batch: List[Dict[str, Dict[str, torch.Tensor]]] - [{'features': {...}, 'targets': {...}}]
    
    Returns:
        Dict with 'features' and 'targets', each containing Dict[tf_name: torch.Tensor]
    """
    # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰æ§‹é€ ã‚’ç¢ºèª
    if not batch:
        return {}
        
    sample = batch[0]
    
    # async_samplerãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    if isinstance(sample.get('features'), dict):
        # Model v2: Dictå½¢å¼
        result = {'features': {}, 'targets': {}}
        
        # featuresã‚’å‡¦ç†
        if 'features' in sample:
            tf_features = defaultdict(list)
            for item in batch:
                for tf_name, tf_tensor in item['features'].items():
                    tf_features[tf_name].append(tf_tensor)
            
            for tf_name, tensors in tf_features.items():
                # pad_sequence ã‚’ä½¿ã£ã¦å¯å¤‰é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                padded_tensor = torch.nn.utils.rnn.pad_sequence(
                    tensors, 
                    batch_first=True, 
                    padding_value=float('nan')
                )
                # dtypeçµ±ä¸€ï¼ˆAMPäº’æ›æ€§ã®ãŸã‚ï¼‰- NaNã‚’ä¿æŒã—ã¦ãƒã‚¹ã‚¯è¨ˆç®—ã§é™¤å¤–
                result['features'][tf_name] = padded_tensor.to(torch.float32)
        
        # targetsã‚’å‡¦ç†
        if 'targets' in sample:
            tf_targets = defaultdict(list)
            for item in batch:
                for tf_name, tf_tensor in item['targets'].items():
                    tf_targets[tf_name].append(tf_tensor)
            
            for tf_name, tensors in tf_targets.items():
                # pad_sequence ã‚’ä½¿ã£ã¦å¯å¤‰é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                padded_tensor = torch.nn.utils.rnn.pad_sequence(
                    tensors, 
                    batch_first=True, 
                    padding_value=float('nan')
                )
                # dtypeçµ±ä¸€ï¼ˆAMPäº’æ›æ€§ã®ãŸã‚ï¼‰- NaNã‚’ä¿æŒã—ã¦ãƒã‚¹ã‚¯è¨ˆç®—ã§é™¤å¤–
                result['targets'][tf_name] = padded_tensor.to(torch.float32)
        
        return result
    else:
        # Legacy: tensorå½¢å¼ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        features_list = []
        targets_list = []
        
        for item in batch:
            features_list.append(item['features'])
            targets_list.append(item['targets'])
        
        return {
            'features': torch.stack(features_list, dim=0),
            'targets': torch.stack(targets_list, dim=0)
        }


class Stage1Dataset(Dataset):
    """Stage 1 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, data_dir: str, config: dict, split: str = "train"):
        self.data_dir = Path(data_dir)
        self.config = config
        self.split = split
        
        # configã«data_dirã‚’è¨­å®š
        self.config['data']['data_dir'] = str(self.data_dir)
        
        print(f"ğŸ”„ Stage 1 DatasetåˆæœŸåŒ– ({split})")
        print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {data_dir}")
        
        # TFãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.tf_data = self._load_tf_data()
        
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        self.feature_engineer = FeatureEngineer(config)
        
        # æ­£è¦åŒ–
        self.normalizer = TFNormalizer(
            config=config,
            cache_stats=True
        )
        
        # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆtrainå°‚ç”¨çµ±è¨ˆå„ªå…ˆï¼‰
        try:
            if split == "train":
                # trainã®å ´åˆã€ã¾ãštrainå°‚ç”¨çµ±è¨ˆã‚’è©¦è¡Œ
                try:
                    self.normalizer.load_stats(split="train")
                    print(f"   ğŸ“Š trainå°‚ç”¨çµ±è¨ˆã‚’ãƒ­ãƒ¼ãƒ‰")
                except FileNotFoundError:
                    print(f"   ğŸ“Š trainå°‚ç”¨çµ±è¨ˆã‚’æ–°è¦è¨ˆç®—ä¸­...")
                    self.normalizer.fit(self.tf_data)
                    self.normalizer.save_stats(split="train")
            else:
                # valã®å ´åˆã€trainå°‚ç”¨çµ±è¨ˆã‚’èª­ã¿è¾¼ã¿
                try:
                    self.normalizer.load_stats(split="train")
                    print(f"   ğŸ“Š trainå°‚ç”¨çµ±è¨ˆã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆvalç”¨ï¼‰")
                except FileNotFoundError:
                    print(f"   âš ï¸ trainå°‚ç”¨çµ±è¨ˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨æœŸé–“çµ±è¨ˆã‚’ä½¿ç”¨")
                    self.normalizer.load_stats()
        except FileNotFoundError:
            print(f"   ğŸ“Š æ­£è¦åŒ–çµ±è¨ˆã‚’æ–°è¦è¨ˆç®—ä¸­...")
            self.normalizer.fit(self.tf_data)
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–+ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
        seq_len = config['data']['seq_len']
        cache_dir = self.data_dir / f"cache_seq{seq_len}"  # seq_lenåˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ†é›¢
        async_sampler = config.get('model', {}).get('async_sampler', False)
        
        # ğŸ”¥ æ¤œè¨¼æ™‚ã¯å°‚ç”¨sampling_probsã‚’ä½¿ç”¨ï¼ˆDrop-inå®Œå…¨ç„¡åŠ¹åŒ–ï¼‰
        if split == "val" and 'sampling_probs_val' in config['data']:
            sampling_probs = config['data']['sampling_probs_val']
            print("   ğŸ”§ æ¤œè¨¼ç”¨sampling_probsé©ç”¨ï¼ˆDrop-inç„¡åŠ¹åŒ–ï¼‰")
        else:
            sampling_probs = config['data'].get('sampling_probs')
        
        self.window_sampler = MultiTFWindowSampler(
            tf_data=self.tf_data,
            seq_len=config['data']['seq_len'],
            split=split,
            val_split=config['validation']['val_split'],
            min_coverage=0.8,
            cache_dir=str(cache_dir),
            val_gap_days=config['validation'].get('val_gap_days', 1.0),
            async_sampler=async_sampler,
            sampling_probs=sampling_probs  # ğŸ”¥ splitåˆ¥Drop-in Sampling
        )
        
        # æ³¨æ„ï¼šãƒã‚¹ã‚­ãƒ³ã‚°ã¯ãƒ¢ãƒ‡ãƒ«å†…ã§å®Ÿè¡Œï¼ˆdata_loaderå´ã§ã¯ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼‰
        
    def _load_tf_data(self) -> Dict[str, pd.DataFrame]:
        """TFãƒ‡ãƒ¼ã‚¿ã‚’é«˜é€Ÿèª­ã¿è¾¼ã¿"""
        tf_data = {}
        timeframes = self.config['data']['timeframes']
        
        print(f"   æ™‚é–“è¶³: {timeframes}")
        print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {self.config['data']['seq_len']}")
        
        for tf in timeframes:
            file_path = self.data_dir / f"simple_gap_aware_{tf}.parquet"
            
            if file_path.exists():
                # Parqueté«˜é€Ÿèª­ã¿è¾¼ã¿
                df = pd.read_parquet(file_path)
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿®æ­£: M1ä»¥å¤–ã¯timestampåˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ä½¿ç”¨
                if tf == 'm1':
                    df.index = pd.to_datetime(df.index)
                else:
                    # timestampåˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š
                    if 'timestamp' in df.columns:
                        df.index = pd.to_datetime(df['timestamp'])
                        df = df.drop('timestamp', axis=1)  # timestampåˆ—ã‚’å‰Šé™¤
                    else:
                        df.index = pd.to_datetime(df.index)
                
                # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€ (UTCã«çµ±ä¸€)
                if df.index.tz is None:
                    df.index = df.index.tz_localize('UTC')
                elif str(df.index.tz) != 'UTC':
                    df.index = df.index.tz_convert('UTC')
                
                tf_data[tf] = df
                print(f"   {tf.upper()}: {len(df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰, æœŸé–“: {df.index[0]} - {df.index[-1]}")
            else:
                raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
                
        return tf_data
    
    def __len__(self) -> int:
        return len(self.window_sampler)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒƒãƒå–å¾—ï¼ˆModel v2 Dictå¯¾å¿œï¼‰"""
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆDict[tf_name, pd.DataFrame] å½¢å¼ï¼‰
        window_data = self.window_sampler[idx]
        
        # éåŒæœŸãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ã§å‡¦ç†ã‚’åˆ†å²
        async_sampler = self.config.get('model', {}).get('async_sampler', False)
        
        if async_sampler:
            # Model v2: Dictå½¢å¼ã§å„TFã‚’å€‹åˆ¥å‡¦ç†
            tf_features = {}
            tf_targets = {}
            
            for tf_name, tf_window_df in window_data.items():
                # å„TFã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
                tf_feat, tf_targ = self.feature_engineer.process_single_tf_window(tf_name, tf_window_df)
                
                # æ­£è¦åŒ–
                tf_feat_norm = self.normalizer.normalize_single_tf(tf_feat, tf_name)
                tf_targ_norm = self.normalizer.normalize_targets_single_tf(tf_targ, tf_name)
                
                # numpy -> torch tensorå¤‰æ› + NaNå¯¾ç­–
                tf_feat_tensor = torch.tensor(tf_feat_norm, dtype=torch.float32)
                tf_targ_tensor = torch.tensor(tf_targ_norm, dtype=torch.float32)
                
                # ğŸ”¥ NaNä¿æŒ: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ä½ç½®ã®ãƒã‚¹ã‚¯è¨ˆç®—ã§ä½¿ç”¨
                tf_features[tf_name] = tf_feat_tensor
                tf_targets[tf_name] = tf_targ_tensor
            
            return {
                'features': tf_features,  # Dict[tf_name, torch.Tensor]
                'targets': tf_targets     # Dict[tf_name, torch.Tensor]
            }
        else:
            # Legacy: tensorå½¢å¼ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
            features, targets = self.feature_engineer.process_window(window_data)
            
            # æ­£è¦åŒ–ï¼ˆfeaturesç”¨ï¼‰
            features = self.normalizer.normalize(features)
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”¨ã®æ­£è¦åŒ–ï¼ˆOHLCç”¨ï¼‰
            targets = self.normalizer.normalize_targets(targets)
            
            # æ—¢ã«ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ï¼ˆBF16äº’æ›ï¼‰
            features_tensor = features.to(torch.float32)
            targets_tensor = targets.to(torch.float32)
            
            return {
                'features': features_tensor,  # ç”Ÿã®ç‰¹å¾´é‡ï¼ˆãƒã‚¹ã‚¯ãªã—ï¼‰
                'targets': targets_tensor
            }

def create_stage1_dataloaders(data_dir: str, config: dict) -> Tuple[DataLoader, DataLoader]:
    """æœ€é©åŒ–ã•ã‚ŒãŸDataLoaderä½œæˆ"""
    
    # DataLoaderè¨­å®šå–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä»˜ãï¼‰
    dataloader_config = config.get('dataloader', {})
    batch_size = config['training']['batch_size']
    
    # éåŒæœŸã‚µãƒ³ãƒ—ãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã®ç¢ºèª
    async_sampler = config.get('model', {}).get('async_sampler', False)
    
    # æœ€é©åŒ–è¨­å®š
    num_workers = dataloader_config.get('num_workers', 8)
    dataloader_kwargs = {
        'batch_size': batch_size,
        'num_workers': num_workers,
        'pin_memory': dataloader_config.get('pin_memory', True),
        'persistent_workers': dataloader_config.get('persistent_workers', True),
        'drop_last': not async_sampler,  # asyncæ™‚ã¯Falseï¼ˆå¯å¤‰é•·å¯¾å¿œï¼‰ã€syncæ™‚ã¯True
    }
    
    # ğŸ”¥ prefetch_factorã¯num_workers > 0ã®å ´åˆã®ã¿è¿½åŠ 
    if num_workers > 0:
        dataloader_kwargs['prefetch_factor'] = dataloader_config.get('prefetch_factor', 4)
    
    # éåŒæœŸãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€collate_fnã‚’è¿½åŠ 
    if async_sampler:
        dataloader_kwargs['collate_fn'] = collate_multiscale
        print("ğŸ”„ éåŒæœŸãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    train_dataset = Stage1Dataset(data_dir, config, split="train")
    val_dataset = Stage1Dataset(data_dir, config, split="val")
    
    # DataLoaderä½œæˆ
    train_loader = DataLoader(
        train_dataset, 
        shuffle=True,
        **dataloader_kwargs
    )
    
    val_loader = DataLoader(
        val_dataset,
        shuffle=False,
        **dataloader_kwargs
    )
    
    print(f"ğŸ“Š DataLoaderä½œæˆå®Œäº†")
    print(f"   è¨“ç·´: {len(train_loader)}ãƒãƒƒãƒ ({len(train_dataset)}ã‚µãƒ³ãƒ—ãƒ«)")
    print(f"   æ¤œè¨¼: {len(val_loader)}ãƒãƒƒãƒ ({len(val_dataset)}ã‚µãƒ³ãƒ—ãƒ«)")
    prefetch_info = f"prefetch={dataloader_kwargs.get('prefetch_factor', 'disabled')}" if num_workers > 0 else "prefetch=disabled"
    print(f"   æœ€é©åŒ–: num_workers={dataloader_kwargs['num_workers']}, {prefetch_info}")
    
    return train_loader, val_loader