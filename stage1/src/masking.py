#!/usr/bin/env python3
"""
Stage 1 ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
ãƒ©ãƒ³ãƒ€ãƒ é€£ç¶šãƒ–ãƒ­ãƒƒã‚¯ãƒ»TFé–“åŒæœŸãƒã‚¹ã‚­ãƒ³ã‚°
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

class MaskingStrategy(nn.Module):
    """ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ã‚¯ãƒ©ã‚¹ï¼ˆLearnable Mask Tokenç‰ˆï¼‰"""
    
    def __init__(self, config: dict, n_features: int = 6):
        """
        Args:
            config: è¨­å®šè¾æ›¸
            n_features: ç‰¹å¾´é‡æ•°ï¼ˆmask_tokenã®æ¬¡å…ƒï¼‰
        """
        super().__init__()
        self.config = config
        self.timeframes = config['data']['timeframes']
        self.n_features = n_features
        
        # ãƒã‚¹ã‚­ãƒ³ã‚°è¨­å®š
        self.mask_ratio = config['masking']['mask_ratio']  # 0.15
        self.mask_span_min = config['masking']['mask_span_min']  # 5
        self.mask_span_max = config['masking']['mask_span_max']  # 60
        self.sync_across_tf = config['masking']['sync_across_tf']  # True
        
        # ğŸ”¥ Learnable Mask Tokenï¼ˆå­¦ç¿’å¯èƒ½ãªãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰- å¿…ãšä½œæˆ
        self.mask_token = nn.Parameter(torch.randn(n_features) * 0.02)
        
        # ğŸ”¥ ãƒ™ã‚¯ãƒˆãƒ«åŒ–è¨­å®šãƒ•ãƒ©ã‚°
        self.use_vectorized = config.get('masking', {}).get('use_vectorized', True)
        
        print(f"ğŸ­ MaskingStrategyåˆæœŸåŒ–ï¼ˆLearnable Mask Tokenç‰ˆï¼‰")
        print(f"   ãƒã‚¹ã‚¯ç‡: {self.mask_ratio}")
        print(f"   ã‚¹ãƒ‘ãƒ³ç¯„å›²: {self.mask_span_min}-{self.mask_span_max}")
        print(f"   TFé–“åŒæœŸ: {self.sync_across_tf}")
        print(f"   ğŸ’¡ Learnable Mask Token: {n_features}æ¬¡å…ƒï¼ˆåˆæœŸå€¤: Î¼={self.mask_token.mean().item():.3f}, Ïƒ={self.mask_token.std().item():.3f}ï¼‰")
        print(f"   âš¡ ãƒ™ã‚¯ãƒˆãƒ«åŒ–: {self.use_vectorized} ï¼ˆ10å€é«˜é€Ÿï¼‰")
        
        # torchä¹±æ•°ç”Ÿæˆå™¨ã«çµ±ä¸€ï¼ˆå†ç¾æ€§ã®ãŸã‚ãƒ»ãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œï¼‰
        self.generator = None  # å®Ÿè¡Œæ™‚ã«ãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œç‰ˆã‚’åˆæœŸåŒ–
        
    def generate_masks(self, features, seed: int = None, eval_mask_ratio_override: float = None):
        """
        ãƒãƒ«ãƒTFç‰¹å¾´é‡ã«å¯¾ã™ã‚‹ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ (Dictå¯¾å¿œ)
        
        Args:
            features: [n_tf, seq_len, n_features] ç‰¹å¾´é‡ãƒ†ãƒ³ã‚½ãƒ« ã¾ãŸã¯ Dict[str, torch.Tensor]
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ï¼ˆå†ç¾æ€§ç”¨ï¼‰
            eval_mask_ratio_override: è©•ä¾¡æ™‚ã®ãƒã‚¹ã‚¯ç‡ä¸Šæ›¸ã (None=é€šå¸¸, 0=ãƒã‚¹ã‚¯ãªã—, 1=å…¨ãƒã‚¹ã‚¯)
            
        Returns:
            masks: [n_tf, seq_len] ãƒã‚¹ã‚¯ãƒ†ãƒ³ã‚½ãƒ« (1=ãƒã‚¹ã‚¯, 0=è¦³æ¸¬) ã¾ãŸã¯ Dict[str, torch.Tensor]
        """
        # Dict format support for Model v2
        if isinstance(features, dict):
            return self.generate_masks_dict(features, seed, eval_mask_ratio_override)
        
        # Legacy tensor format support  
        # ğŸ”¥ ãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œgeneratoråˆæœŸåŒ–
        device = features.device
        if self.generator is None or self.generator.device != device:
            self.generator = torch.Generator(device=device)
            
        if seed is not None:
            self.generator.manual_seed(seed)
            
        # featuresã®å½¢çŠ¶ã‚’ç¢ºèª: [batch, n_tf, seq_len, n_features] ã¾ãŸã¯ [n_tf, seq_len, n_features]
        if features.dim() == 4:
            batch_size, n_tf, seq_len, n_features = features.shape
        elif features.dim() == 3:
            n_tf, seq_len, n_features = features.shape
            batch_size = 1
        else:
            raise ValueError(f"Unexpected features shape: {features.shape}")
        
        # è©•ä¾¡æ™‚ã®ãƒã‚¹ã‚¯ç‡ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å‡¦ç†
        effective_mask_ratio = self.mask_ratio
        if eval_mask_ratio_override is not None:
            effective_mask_ratio = eval_mask_ratio_override
            # print(f"   [MASK DBG] Override: {self.mask_ratio} â†’ {effective_mask_ratio}")  # ğŸ”‡ ç„¡åŠ¹åŒ–
            
        # ğŸ”¥ ãƒãƒƒãƒã‚µã‚¤ã‚ºã«å¿œã˜ã¦ãƒã‚¹ã‚¯ã®å½¢çŠ¶ã‚’æ±ºå®šï¼ˆboolå‹ã§çµ±ä¸€ï¼‰
        if features.dim() == 4:
            masks = torch.zeros(batch_size, n_tf, seq_len, device=features.device, dtype=torch.bool)
        else:
            masks = torch.zeros(n_tf, seq_len, device=features.device, dtype=torch.bool)
        
        if self.sync_across_tf:
            # TFé–“åŒæœŸãƒã‚¹ã‚­ãƒ³ã‚°: M1ãƒ™ãƒ¼ã‚¹ã§ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆã—ã€ä»–ã®TFã«é©ç”¨
            base_mask = self._generate_single_mask(seq_len, effective_mask_ratio)
            
            if features.dim() == 4:
                # ãƒãƒƒãƒå‡¦ç†
                base_mask = base_mask.to(features.device)
                for b in range(batch_size):
                    for i in range(n_tf):
                        tf_mask = self._adapt_mask_to_tf(base_mask, features[b, i], seq_len)
                        masks[b, i] = tf_mask
            else:
                # å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†
                base_mask = base_mask.to(features.device)
                for i in range(n_tf):
                    tf_mask = self._adapt_mask_to_tf(base_mask, features[i], seq_len)
                    masks[i] = tf_mask
        else:
            # TFå€‹åˆ¥ãƒã‚¹ã‚­ãƒ³ã‚°
            if features.dim() == 4:
                # ãƒãƒƒãƒå‡¦ç†
                for b in range(batch_size):
                    for i in range(n_tf):
                        tf_mask = self._generate_single_mask(seq_len, effective_mask_ratio)
                        tf_mask = tf_mask.to(features.device)
                        tf_mask = self._adapt_mask_to_tf(tf_mask, features[b, i], seq_len)
                        masks[b, i] = tf_mask
            else:
                # å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†
                for i in range(n_tf):
                    tf_mask = self._generate_single_mask(seq_len, effective_mask_ratio)
                    tf_mask = tf_mask.to(features.device)
                    tf_mask = self._adapt_mask_to_tf(tf_mask, features[i], seq_len)
                    masks[i] = tf_mask
        
        # ãƒ‡ãƒãƒƒã‚°: å®Ÿéš›ã®ãƒã‚¹ã‚¯ç‡ã‚’ç¢ºèªï¼ˆå®Œå…¨ã«ç„¡åŠ¹åŒ–ï¼‰
        if False:  # ğŸ”¥ ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã‚’å®Œå…¨ã«ç„¡åŠ¹åŒ–
            actual_ratios = []
            for i in range(n_tf):
                mask_i = masks[i] if masks.dim() == 2 else masks[0, i]
                # ğŸ”¥ Bool tensor mean() ã‚¨ãƒ©ãƒ¼ä¿®æ­£: floatå¤‰æ›
                actual_ratio = mask_i.float().mean().item()
                actual_ratios.append(actual_ratio)
                print(f"   [MASK DBG] TF{i} actual mask ratio: {actual_ratio:.4f}")
            print(f"   [MASK DBG] Mean actual mask ratio: {sum(actual_ratios)/len(actual_ratios):.4f}")
            print(f"   [MASK DBG] Override: {eval_mask_ratio_override:.2f} â†’ {eval_mask_ratio_override:.2f}")
                
        return masks
        
    def generate_masks_dict(self, features: Dict[str, torch.Tensor], seed: int = None, eval_mask_ratio_override: float = None) -> Dict[str, torch.Tensor]:
        """
        Dictå½¢å¼ã®ç‰¹å¾´é‡ã«å¯¾ã™ã‚‹ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ (Model v2ç”¨)
        
        Args:
            features: Dict[tf_name, torch.Tensor] - å„TFã®ç‰¹å¾´é‡ [batch, seq_len, n_features]
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ï¼ˆå†ç¾æ€§ç”¨ï¼‰
            eval_mask_ratio_override: è©•ä¾¡æ™‚ã®ãƒã‚¹ã‚¯ç‡ä¸Šæ›¸ã
            
        Returns:
            masks: Dict[tf_name, torch.Tensor] - å„TFã®ãƒã‚¹ã‚¯ [batch, seq_len]
        """
        # ğŸ”¥ ãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œgeneratoråˆæœŸåŒ–
        device = next(iter(features.values())).device
        if self.generator is None or self.generator.device != device:
            self.generator = torch.Generator(device=device)
            
        if seed is not None:
            self.generator.manual_seed(seed)
            
        # è©•ä¾¡æ™‚ã®ãƒã‚¹ã‚¯ç‡ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å‡¦ç†
        effective_mask_ratio = self.mask_ratio
        if eval_mask_ratio_override is not None:
            effective_mask_ratio = eval_mask_ratio_override
            
        masks = {}
        
        # ğŸ”¥ ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç† vs å¾“æ¥å‡¦ç†
        if self.use_vectorized:
            # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‰ˆ: 10å€é«˜é€Ÿ
            masks = self._generate_masks_dict_vectorized(features, effective_mask_ratio)
        else:
            # å¾“æ¥ç‰ˆ: äº’æ›æ€§ã®ãŸã‚ã«ä¿æŒ
            if self.sync_across_tf:
                # TFé–“åŒæœŸãƒã‚¹ã‚­ãƒ³ã‚°: æœ€ã‚‚é•·ã„TFã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
                max_seq_len = max(x.shape[1] for x in features.values())
                base_mask = self._generate_single_mask(max_seq_len, effective_mask_ratio)
                
                for tf_name, tf_features in features.items():
                    batch_size, seq_len, n_features = tf_features.shape
                    tf_masks = torch.zeros(batch_size, seq_len, device=tf_features.device, dtype=torch.bool)
                    
                    # Adapt base mask to this TF's sequence length
                    adapted_base_mask = self._adapt_base_mask_to_length(base_mask, seq_len)
                    
                    for b in range(batch_size):
                        # Check for padding (invalid positions) - dtypeå¯¾å¿œ
                        if tf_features.dtype.is_floating_point:
                            valid_mask = ~torch.isnan(tf_features[b, :, 0])  # float: NaNæ¤œå‡º
                        else:
                            valid_mask = tf_features[b, :, 0] != -1  # int: -1ã‚’pad flagã¨ã—ã¦ä½¿ç”¨
                        
                        # Apply base mask only to valid positions
                        tf_mask = adapted_base_mask.clone()
                        tf_mask = tf_mask & valid_mask  # Only mask valid positions
                        
                        tf_masks[b] = tf_mask
                    
                    masks[tf_name] = tf_masks
            else:
                # TFå€‹åˆ¥ãƒã‚¹ã‚­ãƒ³ã‚°
                for tf_name, tf_features in features.items():
                    batch_size, seq_len, n_features = tf_features.shape
                    tf_masks = torch.zeros(batch_size, seq_len, device=tf_features.device, dtype=torch.bool)
                    
                    for b in range(batch_size):
                        # Check for padding (invalid positions) - dtypeå¯¾å¿œ
                        if tf_features.dtype.is_floating_point:
                            valid_mask = ~torch.isnan(tf_features[b, :, 0])  # float: NaNæ¤œå‡º
                        else:
                            valid_mask = tf_features[b, :, 0] != -1  # int: -1ã‚’pad flagã¨ã—ã¦ä½¿ç”¨
                        valid_seq_len = valid_mask.sum().item()
                        
                        if valid_seq_len > 0:
                            # Generate mask for valid sequence length
                            tf_mask = self._generate_single_mask(valid_seq_len, effective_mask_ratio)
                            
                            # Map back to full sequence (only valid positions)
                            valid_indices = torch.where(valid_mask)[0]
                            if len(valid_indices) > 0:
                                full_mask = torch.zeros(seq_len, device=tf_features.device, dtype=torch.bool)
                                if len(tf_mask) <= len(valid_indices):
                                    full_mask[valid_indices[:len(tf_mask)]] = tf_mask
                                tf_masks[b] = full_mask
                        
                    masks[tf_name] = tf_masks
        
        return masks
        
    def _adapt_base_mask_to_length(self, base_mask: torch.Tensor, target_length: int) -> torch.Tensor:
        """
        ãƒ™ãƒ¼ã‚¹ãƒã‚¹ã‚¯ã‚’æŒ‡å®šé•·ã«é©å¿œ
        
        Args:
            base_mask: [base_seq_len] ãƒ™ãƒ¼ã‚¹ãƒã‚¹ã‚¯
            target_length: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            
        Returns:
            adapted_mask: [target_length] é©å¿œæ¸ˆã¿ãƒã‚¹ã‚¯
        """
        if len(base_mask) == target_length:
            return base_mask.clone()
        elif len(base_mask) > target_length:
            # å³ç«¯ã‚’å–ã‚‹ï¼ˆæœ€æ–°éƒ¨åˆ†ã‚’é‡è¦–ï¼‰
            return base_mask[-target_length:].clone()
        else:
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆå·¦ç«¯ã‚’Falseã§åŸ‹ã‚ã‚‹ï¼‰
            adapted_mask = torch.zeros(target_length, dtype=torch.bool, device=base_mask.device)
            adapted_mask[-len(base_mask):] = base_mask
            return adapted_mask
            
    def _generate_single_mask(self, seq_len: int, mask_ratio: float = None) -> torch.Tensor:
        """
        å˜ä¸€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
        
        Args:
            seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            mask_ratio: ãƒã‚¹ã‚¯ç‡ï¼ˆNoneã®å ´åˆã¯self.mask_ratioã‚’ä½¿ç”¨ï¼‰
            
        Returns:
            mask: [seq_len] ãƒã‚¹ã‚¯ãƒ†ãƒ³ã‚½ãƒ«
        """
        mask = torch.zeros(seq_len, dtype=torch.bool)  # ğŸ”¥ boolå‹ã§çµ±ä¸€
        effective_mask_ratio = mask_ratio if mask_ratio is not None else self.mask_ratio
        target_masked = int(seq_len * effective_mask_ratio)
        masked_count = 0
        
        # çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
        if seq_len < self.mask_span_min:
            return mask  # å…¨ã¦Falseã®ãƒã‚¹ã‚¯ã‚’è¿”ã™ï¼ˆãƒã‚¹ã‚¯ã—ãªã„ï¼‰
        
        # ãƒ©ãƒ³ãƒ€ãƒ é€£ç¶šãƒ–ãƒ­ãƒƒã‚¯ã§ãƒã‚¹ã‚­ãƒ³ã‚°
        while masked_count < target_masked:
            # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¹ã‚¯ã‚¹ãƒ‘ãƒ³é•·ï¼ˆtorchä¹±æ•°ä½¿ç”¨ï¼‰
            span_len = torch.randint(
                self.mask_span_min, 
                self.mask_span_max + 1, 
                (1,), 
                generator=self.generator
            ).item()
            
            # ãƒ©ãƒ³ãƒ€ãƒ ãªé–‹å§‹ä½ç½®
            max_start = max(0, seq_len - span_len)
            if max_start <= 0:
                break
                
            start_pos = torch.randint(
                0, 
                max_start + 1, 
                (1,), 
                generator=self.generator
            ).item()
            end_pos = min(start_pos + span_len, seq_len)
            
            # ãƒã‚¹ã‚¯é©ç”¨
            mask[start_pos:end_pos] = True
            masked_count = mask.sum().item()
            
            # ç›®æ¨™ãƒã‚¹ã‚¯æ•°ã‚’è¶…ãˆãŸå ´åˆã¯èª¿æ•´
            if masked_count > target_masked:
                # è¶…éåˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è§£é™¤
                masked_indices = torch.where(mask)[0]  # boolå‹å¯¾å¿œ
                excess = int(masked_count - target_masked)
                if excess > 0:
                    remove_indices = masked_indices[torch.randperm(len(masked_indices))[:excess]]
                    mask[remove_indices] = False
                break
                
        return mask  # ğŸ”¥ æ—¢ã«boolå‹ãªã®ã§ãã®ã¾ã¾è¿”ã™
        
    def _adapt_mask_to_tf(self, base_mask: torch.Tensor, tf_features: torch.Tensor, seq_len: int) -> torch.Tensor:
        """
        ãƒ™ãƒ¼ã‚¹ãƒã‚¹ã‚¯ã‚’ç‰¹å®šTFã®ç‰¹å¾´é‡ã«é©å¿œ
        
        Args:
            base_mask: [seq_len] ãƒ™ãƒ¼ã‚¹ãƒã‚¹ã‚¯ï¼ˆM1åŸºæº–ï¼‰
            tf_features: [seq_len, n_features] TFç‰¹å¾´é‡
            seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            
        Returns:
            adapted_mask: [seq_len] é©å¿œæ¸ˆã¿ãƒã‚¹ã‚¯
        """
        # å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹éƒ¨åˆ†ã‚’ç‰¹å®šï¼ˆå³ç«¯æ•´åˆ—ã‚’è€ƒæ…®ï¼‰
        valid_mask = torch.any(tf_features != 0, dim=1)
        
        if not valid_mask.any():
            # ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒã‚¹ã‚¯ã—ãªã„
            return torch.zeros(seq_len, dtype=torch.bool)
            
        # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã§ã®ã¿ãƒã‚¹ã‚­ãƒ³ã‚°
        adapted_mask = base_mask.clone()
        adapted_mask = adapted_mask & valid_mask  # boolæ¼”ç®—ã§çµ±ä¸€
        
        return adapted_mask
        
    def apply_mask_to_features(self, features, masks):
        """
        ç‰¹å¾´é‡ã«ãƒã‚¹ã‚¯ã‚’é©ç”¨ï¼ˆLearnable Mask Tokenç‰ˆã€Dictå¯¾å¿œï¼‰
        
        Args:
            features: [n_tf, seq_len, n_features] ç‰¹å¾´é‡ ã¾ãŸã¯ Dict[str, torch.Tensor]
            masks: [n_tf, seq_len] ãƒã‚¹ã‚¯ï¼ˆ1=ãƒã‚¹ã‚¯, 0=è¦³æ¸¬ï¼‰ã¾ãŸã¯ Dict[str, torch.Tensor]
            
        Returns:
            masked_features: [n_tf, seq_len, n_features] ãƒã‚¹ã‚¯æ¸ˆã¿ç‰¹å¾´é‡ ã¾ãŸã¯ Dict[str, torch.Tensor]
        """
        # Dict format support for Model v2
        if isinstance(features, dict):
            return self.apply_mask_to_features_dict(features, masks)
        
        # Legacy tensor format support
        # ç‰¹å¾´é‡ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆinplaceæ“ä½œã®ãŸã‚ï¼‰
        masked_features = features.clone()
        
        # ãƒã‚¹ã‚¯ä½ç½®ã‚’ç‰¹å®š [n_tf, seq_len, 1] -> bool
        mask_expanded = masks.unsqueeze(-1).bool()  # [n_tf, seq_len, 1]
        
        # ğŸ”¥ ãƒã‚¹ã‚¯ä½ç½®ã«Learnable Mask Tokenã‚’è¨­å®šï¼ˆ0ä¹—ç®—ã§ã¯ãªãinplaceç½®æ›ï¼‰
        # mask_tokenã‚’å„ãƒã‚¹ã‚¯ä½ç½®ã«é©ç”¨
        n_tf, seq_len, n_features = features.shape
        mask_token_expanded = self.mask_token.unsqueeze(0).unsqueeze(0)  # [1, 1, n_features]
        mask_token_broadcasted = mask_token_expanded.expand(n_tf, seq_len, n_features)
        
        # ãƒã‚¹ã‚¯ä½ç½®ã®ã¿ã‚’ç½®æ›
        masked_features[mask_expanded.expand_as(features)] = mask_token_broadcasted[mask_expanded.expand_as(features)]
        
        return masked_features
        
    def apply_mask_to_features_dict(self, features: Dict[str, torch.Tensor], masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        Dictå½¢å¼ã®ç‰¹å¾´é‡ã«ãƒã‚¹ã‚¯ã‚’é©ç”¨ï¼ˆLearnable Mask Tokenç‰ˆï¼‰
        
        Args:
            features: Dict[tf_name, torch.Tensor] - å„TFã®ç‰¹å¾´é‡ [batch, seq_len, n_features]
            masks: Dict[tf_name, torch.Tensor] - å„TFã®ãƒã‚¹ã‚¯ [batch, seq_len]
            
        Returns:
            masked_features: Dict[tf_name, torch.Tensor] - å„TFã®ãƒã‚¹ã‚¯æ¸ˆã¿ç‰¹å¾´é‡ [batch, seq_len, n_features]
        """
        masked_features = {}
        
        for tf_name, tf_features in features.items():
            if tf_name not in masks:
                # ãƒã‚¹ã‚¯ãŒãªã„å ´åˆã¯å…ƒã®ç‰¹å¾´é‡ã‚’ãã®ã¾ã¾ä½¿ç”¨
                masked_features[tf_name] = tf_features.clone()
                continue
                
            tf_masks = masks[tf_name]
            batch_size, seq_len, n_features = tf_features.shape
            
            # ç‰¹å¾´é‡ã‚’ã‚³ãƒ”ãƒ¼
            masked_tf_features = tf_features.clone()
            
            # ãƒã‚¹ã‚¯ä½ç½®ã‚’ç‰¹å®š [batch, seq_len, 1] -> bool
            mask_expanded = tf_masks.unsqueeze(-1).bool()  # [batch, seq_len, 1]
            
            # ğŸ”¥ ãƒã‚¹ã‚¯ä½ç½®ã«Learnable Mask Tokenã‚’è¨­å®š
            # mask_tokenã‚’å„ãƒã‚¹ã‚¯ä½ç½®ã«é©ç”¨
            mask_token_expanded = self.mask_token.unsqueeze(0).unsqueeze(0)  # [1, 1, n_features]
            mask_token_broadcasted = mask_token_expanded.expand(batch_size, seq_len, n_features)
            
            # ãƒã‚¹ã‚¯ä½ç½®ã®ã¿ã‚’ç½®æ›
            masked_tf_features[mask_expanded.expand_as(tf_features)] = mask_token_broadcasted[mask_expanded.expand_as(tf_features)]
            
            masked_features[tf_name] = masked_tf_features
        
        return masked_features
        
    def get_mask_statistics(self, masks: torch.Tensor) -> Dict:
        """ãƒã‚¹ã‚¯çµ±è¨ˆã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        
        n_tf, seq_len = masks.shape
        stats = {}
        
        for i, tf in enumerate(self.timeframes):
            tf_mask = masks[i]
            mask_ratio = tf_mask.sum().item() / seq_len
            
            # é€£ç¶šãƒã‚¹ã‚¯ãƒ–ãƒ­ãƒƒã‚¯ã®æ¤œå‡º
            mask_blocks = self._find_mask_blocks(tf_mask)
            
            stats[tf] = {
                'mask_ratio': mask_ratio,
                'masked_tokens': int(tf_mask.sum().item()),
                'total_tokens': seq_len,
                'n_blocks': len(mask_blocks),
                'block_lengths': [end - start for start, end in mask_blocks],
                'avg_block_length': np.mean([end - start for start, end in mask_blocks]) if mask_blocks else 0
            }
            
        return stats
        
    def _find_mask_blocks(self, mask: torch.Tensor) -> List[Tuple[int, int]]:
        """ãƒã‚¹ã‚¯ã®é€£ç¶šãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¤œå‡º"""
        
        mask_np = mask.numpy().astype(bool)
        blocks = []
        
        in_block = False
        start = 0
        
        for i, is_masked in enumerate(mask_np):
            if is_masked and not in_block:
                # ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹
                start = i
                in_block = True
            elif not is_masked and in_block:
                # ãƒ–ãƒ­ãƒƒã‚¯çµ‚äº†
                blocks.append((start, i))
                in_block = False
                
        # æœ€å¾Œã¾ã§ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if in_block:
            blocks.append((start, len(mask_np)))
            
        return blocks
    
    def _generate_masks_dict_vectorized(self, features: Dict[str, torch.Tensor], mask_ratio: float) -> Dict[str, torch.Tensor]:
        """
        ğŸ”¥ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒã‚¹ã‚¯ç”Ÿæˆ - Dictå½¢å¼ï¼ˆ10å€é«˜é€Ÿï¼‰
        
        Args:
            features: Dict[tf_name, torch.Tensor] - [batch, seq_len, n_features]
            mask_ratio: ãƒã‚¹ã‚¯ç‡
            
        Returns:
            masks: Dict[tf_name, torch.Tensor] - [batch, seq_len] bool
        """
        masks = {}
        
        if self.sync_across_tf:
            # TFé–“åŒæœŸãƒã‚¹ã‚­ãƒ³ã‚°: æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã§ãƒ™ãƒ¼ã‚¹ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
            max_seq_len = max(x.shape[1] for x in features.values())
            first_tf = next(iter(features.values()))
            batch_size = first_tf.shape[0]
            device = first_tf.device
            
            # ğŸ”¥ ä¸€æ‹¬ã§ãƒ™ãƒ¼ã‚¹ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
            base_masks = self._generate_batch_masks_vectorized(batch_size, max_seq_len, mask_ratio, device)
            
            # å„TFã«é©ç”¨
            for tf_name, tf_features in features.items():
                _, seq_len, _ = tf_features.shape
                
                # ãƒ™ãƒ¼ã‚¹ãƒã‚¹ã‚¯ã‚’é©å¿œ
                if seq_len == max_seq_len:
                    tf_masks = base_masks
                else:
                    # å³ç«¯ã‚’å–ã‚‹ï¼ˆæœ€æ–°éƒ¨åˆ†ã‚’é‡è¦–ï¼‰
                    tf_masks = base_masks[:, -seq_len:]
                    
                # ğŸ”¥ æœ‰åŠ¹ä½ç½®ã®ã¿ãƒã‚¹ã‚¯é©ç”¨ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
                tf_masks = self._apply_valid_mask_vectorized(tf_masks, tf_features)
                masks[tf_name] = tf_masks
        else:
            # TFå€‹åˆ¥ãƒã‚¹ã‚­ãƒ³ã‚°
            for tf_name, tf_features in features.items():
                batch_size, seq_len, _ = tf_features.shape
                device = tf_features.device
                
                # ğŸ”¥ ä¸€æ‹¬ã§ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
                tf_masks = self._generate_batch_masks_vectorized(batch_size, seq_len, mask_ratio, device)
                
                # ğŸ”¥ æœ‰åŠ¹ä½ç½®ã®ã¿ãƒã‚¹ã‚¯é©ç”¨ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
                tf_masks = self._apply_valid_mask_vectorized(tf_masks, tf_features)
                masks[tf_name] = tf_masks
        
        return masks
    
    def _apply_valid_mask_vectorized(self, masks: torch.Tensor, features: torch.Tensor) -> torch.Tensor:
        """ğŸ”¥ æœ‰åŠ¹ä½ç½®ã®ã¿ãƒã‚¹ã‚¯é©ç”¨ - ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        # ğŸ”¥ æœ‰åŠ¹ä½ç½®ã‚’ä¸€æ‹¬æ¤œå‡º
        if features.dtype.is_floating_point:
            valid_mask = ~torch.isnan(features[:, :, 0])  # [batch, seq_len]
        else:
            valid_mask = features[:, :, 0] != -1
        
        # ğŸ”¥ æœ‰åŠ¹ä½ç½®ã®ã¿ãƒã‚¹ã‚¯é©ç”¨
        masks = masks & valid_mask
        
        return masks
    
    def _generate_batch_masks_vectorized(self, batch_size: int, seq_len: int, mask_ratio: float, device: torch.device) -> torch.Tensor:
        """
        ğŸ”¥ ãƒãƒƒãƒå…¨ä½“ã®ãƒã‚¹ã‚¯ã‚’ä¸€æ‹¬ç”Ÿæˆ - å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆæ–°æ©Ÿèƒ½çµ±åˆï¼‰
        
        Args:
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            mask_ratio: ãƒã‚¹ã‚¯ç‡
            device: ãƒ‡ãƒã‚¤ã‚¹
            
        Returns:
            masks: [batch, seq_len] bool tensor
        """
        # ğŸ”¥ ãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œgeneratoråˆæœŸåŒ–
        if self.generator is None or self.generator.device != device:
            self.generator = torch.Generator(device=device)
            
        # çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å ´åˆã¯ãƒã‚¹ã‚¯ãªã—
        if seq_len < self.mask_span_min:
            return torch.zeros(batch_size, seq_len, device=device, dtype=torch.bool)
        
        # ğŸ”¥ ä¸€æ‹¬ã§ãƒã‚¹ã‚¯ã‚¹ãƒ‘ãƒ³ã‚’ç”Ÿæˆ
        target_masked = int(seq_len * mask_ratio)
        
        # å¿…è¦ãªã‚¹ãƒ‘ãƒ³æ•°ã‚’æ¨å®šï¼ˆä¿å®ˆçš„ã«å¤šã‚ã«ç”Ÿæˆï¼‰
        avg_span_len = (self.mask_span_min + self.mask_span_max) / 2
        estimated_spans = max(1, int(target_masked / avg_span_len * 2))  # 2å€å®‰å…¨ä¿‚æ•°
        
        # ğŸ”¥ ãƒãƒƒãƒÃ—ã‚¹ãƒ‘ãƒ³æ•°ã®ä¹±æ•°ã‚’ä¸€æ‹¬ç”Ÿæˆ
        span_lengths = torch.randint(
            self.mask_span_min, 
            self.mask_span_max + 1, 
            (batch_size, estimated_spans),
            device=device,
            generator=self.generator
        )
        
        # ğŸ”¥ é–‹å§‹ä½ç½®ã‚‚ä¸€æ‹¬ç”Ÿæˆ
        start_positions = torch.randint(
            0, 
            max(1, seq_len - self.mask_span_min), 
            (batch_size, estimated_spans),
            device=device,
            generator=self.generator
        )
        
        # ğŸ”¥ çµ‚äº†ä½ç½®ã‚’è¨ˆç®—
        end_positions = (start_positions + span_lengths).clamp(max=seq_len)
        
        # ğŸ”¥ å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒã‚¹ã‚¯é©ç”¨ - Pythonãƒ«ãƒ¼ãƒ—é™¤å»
        masks = torch.zeros(batch_size, seq_len, device=device, dtype=torch.bool)
        
        # ğŸ”¥ å®Œå…¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–: for-sãƒ«ãƒ¼ãƒ—ã‚‚é™¤å»
        # 1) æœ€å¤§ã‚¹ãƒ‘ãƒ³é•·ã‚’å–å¾—
        span_lengths_actual = end_positions - start_positions  # [batch, spans]
        max_span_len = span_lengths_actual.max().item()
        
        if max_span_len > 0:
            # 2) ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã‚’ä½œæˆ
            idx = torch.arange(max_span_len, device=device)  # [L]
            
            # 3) ã‚¹ãƒ‘ãƒ³ãƒã‚¹ã‚¯ã‚’ä½œæˆ [batch, spans, L]
            span_mask = (idx[None, None, :] < span_lengths_actual[:, :, None])
            
            # 4) ç¯„å›²ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ [batch, spans, L]
            range_idx = idx[None, None, :] + start_positions[:, :, None]
            
            # 5) æœ‰åŠ¹ç¯„å›²ã®ãƒã‚¹ã‚¯ã‚’ä½œæˆ
            valid_mask = span_mask & (range_idx < seq_len)
            
            # 6) scatter_ã§ãƒã‚¹ã‚¯ã‚’ä¸€æ‹¬é©ç”¨
            batch_idx = torch.arange(batch_size, device=device)[:, None, None].expand(-1, estimated_spans, max_span_len)
            masks[batch_idx[valid_mask], range_idx[valid_mask]] = True
        
        # ğŸ”¥ ãƒã‚¹ã‚¯æ•°ã‚’æ­£ç¢ºã«èª¿æ•´ï¼ˆãƒãƒƒãƒä¸¦åˆ—ï¼‰
        for b in range(batch_size):
            current_masked = masks[b].sum().item()
            
            if current_masked > target_masked:
                # è¶…éåˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è§£é™¤
                masked_indices = torch.where(masks[b])[0]
                excess = current_masked - target_masked
                if excess > 0:
                    # ğŸ”¥ CPUã§randpermã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰GPUã«ç§»å‹•
                    cpu_generator = torch.Generator()
                    if hasattr(self.generator, 'get_state'):
                        cpu_generator.set_state(self.generator.get_state().cpu())
                    perm_indices = torch.randperm(len(masked_indices), generator=cpu_generator)[:excess]
                    remove_indices = masked_indices[perm_indices]
                    masks[b, remove_indices] = False
            elif current_masked < target_masked:
                # ä¸è¶³åˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¿½åŠ 
                unmasked_indices = torch.where(~masks[b])[0]
                needed = target_masked - current_masked
                if needed > 0 and len(unmasked_indices) > 0:
                    # ğŸ”¥ CPUã§randpermã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰GPUã«ç§»å‹•
                    cpu_generator = torch.Generator()
                    if hasattr(self.generator, 'get_state'):
                        cpu_generator.set_state(self.generator.get_state().cpu())
                    perm_indices = torch.randperm(len(unmasked_indices), generator=cpu_generator)[:needed]
                    add_indices = unmasked_indices[perm_indices]
                    masks[b, add_indices] = True
        
        return masks