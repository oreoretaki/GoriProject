#!/usr/bin/env python3
"""
Stage 1 ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
ãƒ©ãƒ³ãƒ€ãƒ é€£ç¶šãƒ–ãƒ­ãƒƒã‚¯ãƒ»TFé–“åŒæœŸãƒã‚¹ã‚­ãƒ³ã‚°
"""

import numpy as np
import torch
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

class MaskingStrategy:
    """ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: dict):
        """
        Args:
            config: è¨­å®šè¾æ›¸
        """
        self.config = config
        self.timeframes = config['data']['timeframes']
        
        # ãƒã‚¹ã‚­ãƒ³ã‚°è¨­å®š
        self.mask_ratio = config['masking']['mask_ratio']  # 0.15
        self.mask_span_min = config['masking']['mask_span_min']  # 5
        self.mask_span_max = config['masking']['mask_span_max']  # 60
        self.sync_across_tf = config['masking']['sync_across_tf']  # True
        
        print(f"ğŸ­ MaskingStrategyåˆæœŸåŒ–")
        print(f"   ãƒã‚¹ã‚¯ç‡: {self.mask_ratio}")
        print(f"   ã‚¹ãƒ‘ãƒ³ç¯„å›²: {self.mask_span_min}-{self.mask_span_max}")
        print(f"   TFé–“åŒæœŸ: {self.sync_across_tf}")
        
        # ä¹±æ•°ç”Ÿæˆå™¨ï¼ˆå†ç¾æ€§ã®ãŸã‚ï¼‰
        self.rng = np.random.RandomState()
        
    def generate_masks(self, features: torch.Tensor, seed: int = None) -> torch.Tensor:
        """
        ãƒãƒ«ãƒTFç‰¹å¾´é‡ã«å¯¾ã™ã‚‹ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
        
        Args:
            features: [n_tf, seq_len, n_features] ç‰¹å¾´é‡ãƒ†ãƒ³ã‚½ãƒ«
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ï¼ˆå†ç¾æ€§ç”¨ï¼‰
            
        Returns:
            masks: [n_tf, seq_len] ãƒã‚¹ã‚¯ãƒ†ãƒ³ã‚½ãƒ« (1=ãƒã‚¹ã‚¯, 0=è¦³æ¸¬)
        """
        if seed is not None:
            self.rng.seed(seed)
            
        n_tf, seq_len, n_features = features.shape
        masks = torch.zeros(n_tf, seq_len)
        
        if self.sync_across_tf:
            # TFé–“åŒæœŸãƒã‚¹ã‚­ãƒ³ã‚°: M1ãƒ™ãƒ¼ã‚¹ã§ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆã—ã€ä»–ã®TFã«é©ç”¨
            base_mask = self._generate_single_mask(seq_len)
            
            for i in range(n_tf):
                # å„TFã®å®Ÿéš›ã®é•·ã•ã«å¿œã˜ã¦ãƒã‚¹ã‚¯ã‚’èª¿æ•´
                tf_mask = self._adapt_mask_to_tf(base_mask, features[i], seq_len)
                masks[i] = tf_mask
        else:
            # TFå€‹åˆ¥ãƒã‚¹ã‚­ãƒ³ã‚°
            for i in range(n_tf):
                tf_mask = self._generate_single_mask(seq_len)
                # å„TFã®å®Ÿéš›ã®é•·ã•ã«å¿œã˜ã¦èª¿æ•´
                tf_mask = self._adapt_mask_to_tf(tf_mask, features[i], seq_len)
                masks[i] = tf_mask
                
        return masks
        
    def _generate_single_mask(self, seq_len: int) -> torch.Tensor:
        """
        å˜ä¸€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
        
        Args:
            seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            
        Returns:
            mask: [seq_len] ãƒã‚¹ã‚¯ãƒ†ãƒ³ã‚½ãƒ«
        """
        mask = torch.zeros(seq_len)
        target_masked = int(seq_len * self.mask_ratio)
        masked_count = 0
        
        # ãƒ©ãƒ³ãƒ€ãƒ é€£ç¶šãƒ–ãƒ­ãƒƒã‚¯ã§ãƒã‚¹ã‚­ãƒ³ã‚°
        while masked_count < target_masked:
            # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¹ã‚¯ã‚¹ãƒ‘ãƒ³é•·
            span_len = self.rng.randint(self.mask_span_min, self.mask_span_max + 1)
            
            # ãƒ©ãƒ³ãƒ€ãƒ ãªé–‹å§‹ä½ç½®
            max_start = max(0, seq_len - span_len)
            if max_start <= 0:
                break
                
            start_pos = self.rng.randint(0, max_start + 1)
            end_pos = min(start_pos + span_len, seq_len)
            
            # ãƒã‚¹ã‚¯é©ç”¨
            mask[start_pos:end_pos] = 1.0
            masked_count = mask.sum().item()
            
            # ç›®æ¨™ãƒã‚¹ã‚¯æ•°ã‚’è¶…ãˆãŸå ´åˆã¯èª¿æ•´
            if masked_count > target_masked:
                # è¶…éåˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è§£é™¤
                masked_indices = torch.where(mask == 1.0)[0]
                excess = int(masked_count - target_masked)
                if excess > 0:
                    remove_indices = masked_indices[torch.randperm(len(masked_indices))[:excess]]
                    mask[remove_indices] = 0.0
                break
                
        return mask
        
    def _adapt_mask_to_tf(self, base_mask: torch.Tensor, tf_features: torch.Tensor, seq_len: int) -> torch.Tensor:
        """
        ãƒ™ãƒ¼ã‚¹ãƒã‚¹ã‚¯ã‚’ç‰¹å®šTFã®ç‰¹å¾´é‡ã«é©å¿œ
        
        Args:
            base_mask: [seq_len] ãƒ™ãƒ¼ã‚¹ãƒã‚¹ã‚¯ï¼ˆM1åŸºæº–ï¼‰
            tf_features: [seq_len, n_features] TFç‰¹å¾´é‡
            seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
            
        Returns:
            adapted_mask: [seq_len] é©å¿œæ¸ˆã¿ãƒã‚¹ã‚¯
        """
        # å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹éƒ¨åˆ†ã‚’ç‰¹å®šï¼ˆå³ç«¯æ•´åˆ—ã‚’è€ƒæ…®ï¼‰
        valid_mask = torch.any(tf_features != 0, dim=1)
        
        if not valid_mask.any():
            # ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒã‚¹ã‚¯ã—ãªã„
            return torch.zeros(seq_len)
            
        # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã§ã®ã¿ãƒã‚¹ã‚­ãƒ³ã‚°
        adapted_mask = base_mask.clone()
        adapted_mask = adapted_mask * valid_mask.to(torch.float32)
        
        return adapted_mask
        
    def apply_mask_to_features(self, features: torch.Tensor, masks: torch.Tensor) -> torch.Tensor:
        """
        ç‰¹å¾´é‡ã«ãƒã‚¹ã‚¯ã‚’é©ç”¨
        
        Args:
            features: [n_tf, seq_len, n_features] ç‰¹å¾´é‡
            masks: [n_tf, seq_len] ãƒã‚¹ã‚¯
            
        Returns:
            masked_features: [n_tf, seq_len, n_features] ãƒã‚¹ã‚¯æ¸ˆã¿ç‰¹å¾´é‡
        """
        # ãƒã‚¹ã‚¯éƒ¨åˆ†ã‚’0ã«è¨­å®š
        mask_expanded = masks.unsqueeze(-1)  # [n_tf, seq_len, 1]
        masked_features = features * (1 - mask_expanded)
        
        return masked_features
        
    def get_mask_statistics(self, masks: torch.Tensor) -> Dict:
        """ãƒã‚¹ã‚¯çµ±è¨ˆã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        
        n_tf, seq_len = masks.shape
        stats = {}
        
        for i, tf in enumerate(self.timeframes):
            tf_mask = masks[i]
            mask_ratio = tf_mask.sum().item() / seq_len
            
            # é€£ç¶šãƒã‚¹ã‚¯ãƒ–ãƒ­ãƒƒã‚¯ã®æ¤œå‡º
            mask_blocks = self._find_mask_blocks(tf_mask)
            
            stats[tf] = {
                'mask_ratio': mask_ratio,
                'masked_tokens': int(tf_mask.sum().item()),
                'total_tokens': seq_len,
                'n_blocks': len(mask_blocks),
                'block_lengths': [end - start for start, end in mask_blocks],
                'avg_block_length': np.mean([end - start for start, end in mask_blocks]) if mask_blocks else 0
            }
            
        return stats
        
    def _find_mask_blocks(self, mask: torch.Tensor) -> List[Tuple[int, int]]:
        """ãƒã‚¹ã‚¯ã®é€£ç¶šãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¤œå‡º"""
        
        mask_np = mask.numpy().astype(bool)
        blocks = []
        
        in_block = False
        start = 0
        
        for i, is_masked in enumerate(mask_np):
            if is_masked and not in_block:
                # ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹
                start = i
                in_block = True
            elif not is_masked and in_block:
                # ãƒ–ãƒ­ãƒƒã‚¯çµ‚äº†
                blocks.append((start, i))
                in_block = False
                
        # æœ€å¾Œã¾ã§ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if in_block:
            blocks.append((start, len(mask_np)))
            
        return blocks