# T5転移学習実験ログ

## 📅 実験実行記録

### 実験1: 問題発見・修正フェーズ（2025-07-06）

#### ❌ 初回実験の問題（修正前）
```
実験    初期化/凍結    最終 val_loss    最終 val_corr    備考
Baseline (ランダム)    ―    204.46    -0.022    最も低損失だが相関は負値 
T5-freeze 3 epoch    T5 (凍結→段階解凍)    277.93    -0.084    Baseline より悪化 
T5-nofreeze    T5 (即時解凍)    3333.36    -0.001    損失が桁違いに悪化 
```

**根本原因**:
```
⚠️ google/t5-smallをダウンロードできません。ランダム初期化T5を作成します。
```

- ❌ モデルID誤り: `google/t5-small` → 正: `t5-small`
- ❌ 認証・ネットワーク問題: 401 Client Error
- ❌ 「巨大ランダムT5 vs 軽量SharedEncoder」の不公平比較

#### ✅ 修正内容

1. **モデルID修正**: `google/t5-small` → `t5-small`
2. **認証・プロキシ対応**: 汎用CLI化、環境変数サポート
3. **エラー停止**: ランダム初期化フォールバック禁止
4. **パッチ埋め込み改善**: LayerNorm + √d_model スケーリング
5. **設定最適化**: seq_len: 64→128, patch_len: 16→32

#### ✅ ダウンロード成功（2025-07-06）
```
✅ Model 't5-small' ready @ ~/.cache/huggingface
   d_model: 512
   params (full): 60.5 M
   params (encoder): 35.3 M
   ✅ 正しいPyTorch重み（フルモデル）ロード成功
```

#### 🔍 パラメータ数検証問題・解決（2025-07-06）

**問題**: 初期ログで35,330,816パラメータと表示（60M期待値より少ない）
```
⚠️ ログに出た35,330,816は「量子化ONNX版」や一部ウェイトしかロードしていない場合に起こる数字で、完全ロード出来ていません。
```

**解決**: download_t5.pyにフルモデル検証機能を追加
- T5ForConditionalGenerationでフルモデル（60M）をロード・検証
- T5EncoderModelでエンコーダー部分（35M）を個別確認
- 59M < full_params < 62M の範囲チェック
- ONNX版誤検出の防止機能

### 実験2: 真の転移学習検証フェーズ（実行中）

#### 🔄 現在実行中
- **fast_dev_run**: 事前学習済みT5重みでの動作確認
- **予想**: 前回の偽実験とは全く異なる結果

#### 📊 期待される結果

**仮説1: T5転移学習優位**
```
実験    val_corr_初期    val_corr_最終    収束速度    備考
Baseline    -0.2~-0.1    -0.02    遅い    ランダム初期化から学習
T5-freeze    0.0~+0.1    +0.05    速い    事前学習知識活用
T5-nofreeze    -0.05~0.0    +0.03    中程度    過学習リスク
```

**仮説2: 時系列での制約**
- テキスト事前学習の時系列適応限界
- パッチ化による時系列連続性の破壊
- ドメインギャップの影響

#### 🔍 検証ポイント

1. **初期性能**: エポック0-1でのT5優位性
2. **収束速度**: T5による学習加速効果
3. **最終性能**: ベースライン超越可能性
4. **学習安定性**: 勾配ノルム・AMP安定性
5. **段階的解凍効果**: 凍結→解凍の最適タイミング

## 📚 参考文献

- **論文**: "Random Initialization Can't Catch Up" (NeurIPS 2023)
- **検証対象**: 時系列データでの転移学習効果
- **手法**: T5エンコーダー + パッチ埋め込み + 段階的解凍

## 🎯 実験目標

**論文の時系列での再現**: テキスト領域の知見が時系列に適用可能かを検証し、Stage-1アーキテクチャでの最適な初期化戦略を確立する。